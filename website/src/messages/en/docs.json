{
  "meta": {
    "title": "Documentation - PlanToCode",
    "description": "Learn how to plan and ship code changes with PlanToCode: file discovery, implementation plans, terminal sessions, model guardrails, and voice."
  },
  "docs": {
    "meta": {
      "title": "Documentation - PlanToCode",
      "description": "Learn how to plan and ship code changes with PlanToCode: file discovery, implementation plans, terminal sessions, model guardrails, and voice."
    }
  },
  "architecture": {
    "meta": {
      "title": "PlanToCode architecture overview",
      "description": "Desktop, orchestration, and persistence layers that power implementation plans, workflows, and terminal sessions."
    },
    "category": "Architecture",
    "date": "2025-09-19",
    "description": "How the desktop shell, background workflows, and shared services are organised.",
    "frontend": {
      "heading": "Frontend surface",
      "providers": "Shared providers handle notifications, runtime configuration, and plan state. The Implementation Plans panel keeps plan metadata, manages modal visibility, and requests token estimates or prompt content as needed.",
      "ui": "The desktop UI is built with React components. Implementation plan content is displayed through a Monaco-based viewer that virtualises large plans, detects languages, and supports copy actions so reviewers can examine plan text without performance issues. Terminal sessions render inside a buffered view that attaches to PTY output and shows connection status updates."
    },
    "intro": "PlanToCode is a Tauri desktop application with a React front end. The UI renders implementation plans, terminals, and configuration controls, while the Rust backend exposes commands for workflows, token estimation, and persistent terminal sessions. This overview summarises how those pieces fit together.",
    "visuals": {
      "systemMap": {
        "title": "System map snapshot",
        "description": "This diagram depicts the PlanToCode system architecture as four interconnected layers arranged vertically. Top Layer - Desktop Frontend: A React/Next.js box containing components (Plan Viewer, Terminal Panel, Session Manager) connected via labeled arrows \"invoke()\" and \"listen()\" to the Tauri IPC bridge. Second Layer - Rust Backend: WorkflowOrchestrator (scheduling multi-stage jobs), TerminalSessionManager (PTY lifecycle), and job processors (FileDiscovery, PlanGeneration, TextImprovement, DeepResearch). Third Layer - Persistence: SQLite tables for sessions, background_jobs, and terminal_sessions with read/write arrows. Fourth Layer - External Services: Server routes under /api/llm/* and /api/auth with provider icons (OpenAI, Anthropic, Google, OpenRouter). Data flows run down through the layers; streaming responses and job events flow back up to the UI.",
        "imageSrc": "/images/architecture/system-map.svg",
        "imageAlt": "Diagram showing PlanToCode system map",
        "caption": "Four-layer architecture with data flowing down and events streaming back up."
      }
    },
    "metaDescription": "Desktop, orchestration, and persistence layers that power implementation plans, workflows, and terminal sessions.",
    "metaTitle": "PlanToCode architecture overview",
    "ogDescription": "Learn how the React front end, Tauri commands, and background services cooperate inside the desktop app.",
    "ogTitle": "PlanToCode architecture overview",
    "persistence": {
      "database": "Terminal output and session metadata are stored in SQLite via the terminal sessions repository. Each record includes identifiers, timestamps, working directories, environment variables, and the accumulated log so that restarts can recover prior output. The same repository emits events when session state changes.",
      "heading": "Persistence and configuration",
      "modelConfig": "Model defaults live in the application configuration table. Each task defines a default model, a list of allowed alternatives, token budgets, and optional copy-button presets. The React layer reads these settings to populate the model selector and guardrails."
    },
    "readTime": "7 min",
    "tauriCommands": {
      "commands": "The Rust side of the application exposes commands for workflows, terminal sessions, and model tooling. The workflow commands start background jobs through the Workflow Orchestrator, validating inputs and emitting progress events as the file discovery pipeline runs. Token estimation commands calculate prompt sizes for the currently selected model.",
      "heading": "Tauri commands and services",
      "terminal": "Terminal commands manage PTY processes, track remote clients, and verify whether supported CLI binaries are available before launching a session. Health checks combine PTY status with database records to report whether a session is still alive."
    },
    "ipcBridge": {
      "heading": "IPC bridge and event streaming",
      "description": "The desktop UI calls Rust commands through the Tauri IPC bridge for workflows, terminal sessions, token estimation, and configuration updates.",
      "details": "Commands are invoked from React and results stream back via event listeners so the UI can update job progress, terminal output, and session state in real time."
    },
    "workflowOrchestrator": {
      "heading": "Workflow orchestrator",
      "description": "Multi-stage workflows (file discovery, research, plan generation) are defined in JSON and executed by the Workflow Orchestrator.",
      "details": "Each stage maps to a Rust processor; intermediate results are persisted to SQLite and forwarded to the next stage with progress events emitted to the UI."
    },
    "title": "PlanToCode Architecture",
    "voicePipeline": {
      "description": "Voice transcription is implemented as a React hook that coordinates media permissions, microphone selection, and transcription requests. The hook integrates with the plan terminal and prompt editors, inserting recognised text directly into the active component and surfacing notifications if transcription fails.",
      "heading": "Voice transcription pipeline"
    },
    "server": {
      "heading": "Server layer",
      "description": "The server handles provider configuration (API keys from env/config and provider/model mappings), routes proxy requests by provider prefix, enforces rate limits, and records usage/cost for billing. It also serves system prompt defaults and runtime model configuration to clients."
    },
    "dataFlows": {
      "heading": "Data flows",
      "description": "Tasks, plans, jobs, and sessions flow between components: (1) Task refinement: React UI → TextImprovementPopover → Tauri command → WorkflowOrchestrator → text_improvement prompt → SQLite → React provider replaces text. (2) File discovery: Implementation Plans panel → Tauri command → 4 sequential jobs → progress events → SQLite → UI display. (3) Implementation plans: File discovery → Generate Plan → Tauri command → LLM streaming → SQLite → Monaco viewer → review/approve → export. (4) Terminal execution: PTY session → command execution → output streaming → SQLite → voice transcription injection → attention indicators."
    },
    "stateSync": {
      "heading": "State synchronization",
      "description": "React providers treat SQLite as the source of truth for plans, jobs, and terminal sessions, rehydrating state on startup.",
      "details": "Tauri events update in-memory state as jobs run, while repositories ensure plan history and session output survive restarts."
    },
    "llmRouting": {
      "heading": "LLM routing and provider normalization",
      "description": "Model requests are sent to the server proxy, which routes by provider prefix and normalizes payloads and streaming responses.",
      "details": "The proxy tracks usage and cost per user or API key, enforces rate limits, and routes specific providers through OpenRouter when configured (for example, Anthropic streaming or DeepSeek mappings)."
    }
  },
  "deepResearch": {
    "meta": {
      "title": "Deep research - PlanToCode",
      "description": "Technical documentation for the web search workflow: API integration, query optimization, result processing, and development workflow integration."
    },
    "apiIntegration": {
      "heading": "API Integration Details",
      "pipeline": {
        "description": "Research findings pass through a standardized processing pipeline that extracts meaningful information while preserving formatting and context. The pipeline handles various content types and synthesizes findings into actionable insights for development workflows.",
        "heading": "Content Processing Pipeline"
      },
      "providerConfig": {
        "description": "The system uses your configured LLM provider for web research. The LLM generates targeted research queries based on your task context and synthesizes findings from its training data and web search capabilities. Model selection and configuration are managed through the application settings.",
        "heading": "AI Research Configuration"
      }
    },
    "architecture": {
      "description": "The deep research system operates as a two-stage workflow: (1) WebSearchPromptsGeneration - AI analyzes your task and project context to generate targeted research queries, and (2) WebSearchExecution - the LLM executes research prompts in parallel and synthesizes findings. Each stage is designed for reliability, cost efficiency, and contextual relevance.",
      "heading": "Architecture Overview"
    },
    "bestPractices": {
      "examples": {
        "description": "Common integration patterns demonstrate how web search results enhance different development scenarios, from debugging specific errors to implementing new features with unfamiliar APIs.",
        "heading": "Integration Examples"
      },
      "heading": "Best Practices and Examples",
      "strategies": {
        "description": "To maximize the value of web search integration, follow these proven strategies for formulating queries, interpreting results, and integrating findings into your development workflow.",
        "heading": "Effective Search Strategies",
        "queryFormulation": {
          "constraints": "Include platform or environment constraints",
          "errors": "Combine library names with specific error messages",
          "heading": "Query Formulation",
          "practices": "Use \"best practices\" or \"recommended approach\" for pattern searches",
          "versions": "Include specific version numbers when relevant"
        },
        "resultEvaluation": {
          "crossReference": "Cross-reference solutions across multiple sources",
          "dates": "Check publication dates for time-sensitive information",
          "heading": "Result Evaluation",
          "official": "Prioritize official documentation over third-party sources",
          "verify": "Verify code examples in your development environment"
        }
      }
    },
    "category": "Technical Reference",
    "configuration": {
      "heading": "Configuration and Customization",
      "preferences": {
        "description": "Research behavior is configured through model selection and task settings. Choose your preferred AI model for research tasks, configure timeouts, and select which files to include for context.",
        "filters": "Model selection determines research quality and cost",
        "heading": "Research Settings",
        "limits": "Maximum 12 research prompts generated per task",
        "optionsHeading": "Configurable Options",
        "patterns": "Include relevant project files for better context",
        "sources": "Project directory and file selection for context",
        "triggers": "Start research manually via the workflow command"
      },
      "projectSettings": {
        "description": "Research configuration is session-aware. The system uses the current session's project directory and included files to provide context. Excluded paths (like node_modules, dist) are automatically filtered from the directory tree shown to the AI.",
        "heading": "Project-Specific Settings"
      }
    },
    "costs": {
      "heading": "Cost Considerations",
      "optimization": {
        "description": "Research costs are managed through intelligent prompt generation - the system limits research prompts to a maximum of 12 per task. Parallel execution minimizes wall-clock time. Each job tracks token usage and estimated costs in its metadata for full transparency.",
        "heading": "Cost Optimization"
      },
      "rateLimiting": {
        "cacheFirst": "Research results are stored per job so you can reuse them without rerunning",
        "description": "Deep research uses your configured LLM provider. Each research task generates multiple parallel LLM calls, so costs scale with the number of research prompts generated. The system tracks token usage and costs per job for transparency.",
        "guidelinesHeading": "Cost Management Tips",
        "heading": "Usage and Costs",
        "personal": "Token usage tracked per research job with detailed cost breakdown",
        "team": "Costs managed through your provider account (self-hosted) or PlanToCode billing (hosted)",
        "throttling": "Monitor job metadata for token counts and estimated costs"
      }
    },
    "cta": {
      "description": "The Deep Research and Web Search features are available in the PlanToCode desktop application. Download the build for your platform to start integrating web research into your development workflow.",
      "heading": "Ready to use Deep Research?",
      "links": {
        "architecture": "View System Architecture",
        "buildYourOwn": "Build Your Own Integration"
      }
    },
    "date": "2025-09-20",
    "description": "How PlanToCode performs web searches, processes results, and integrates findings into development workflows.",
    "devIntegration": {
      "caching": {
        "description": "Research results are stored in job metadata and can be accessed through the job details panel. Results persist for the session duration and can be referenced when creating implementation plans or making coding decisions.",
        "heading": "Result Storage"
      },
      "contextAware": {
        "description": "Research requests are automatically enhanced with context from your current session. The system includes your project's directory tree and selected file contents in the prompt generation phase, enabling the AI to formulate research queries that are specific to your codebase.",
        "heading": "Context-Aware Research"
      },
      "heading": "Development Workflow Integration",
      "resultIntegration": {
        "description": "Research findings can be used to inform implementation plans. When research tasks complete, findings are formatted as research_finding tags that can be incorporated into subsequent planning tasks, ensuring your implementation is guided by current best practices and accurate documentation.",
        "heading": "Result Integration"
      }
    },
    "intro": "The Deep Research feature enables PlanToCode to perform intelligent AI-powered research, gather relevant information, and integrate findings directly into development workflows. This system uses large language models to generate targeted research queries based on your project context, execute parallel research tasks, and synthesize actionable insights to enhance code generation and problem-solving capabilities.",
    "metaDescription": "Technical documentation for the web search workflow: API integration, query optimization, result processing, and development workflow integration.",
    "metaTitle": "Deep research - PlanToCode",
    "ogDescription": "Understand how web search operates within PlanToCode: from query generation to result processing and integration with development workflows.",
    "ogTitle": "Deep research - PlanToCode",
    "readTime": "8 min",
    "title": "Deep Research & Web Search",
    "troubleshooting": {
      "commonIssues": {
        "description": "Most research issues stem from LLM API connectivity, insufficient credits, or prompts that are too broad. The system provides clear error messages and job status tracking for troubleshooting.",
        "geographic": "Model Availability",
        "geographicSolution": "Some models may have regional restrictions from the provider",
        "heading": "Common Issues",
        "noResults": "No Research Prompts Generated",
        "noResultsSolution": "Provide more specific task descriptions or include relevant files for context",
        "rateLimit": "API Errors",
        "rateLimitSolution": "Check provider status and rate limit or credit balance"
      },
      "heading": "Troubleshooting and Support",
      "performance": {
        "description": "For optimal performance, provide clear and specific task descriptions. Include relevant project files to give the AI better context. The system executes research prompts in parallel to minimize total execution time.",
        "heading": "Performance Optimization"
      }
    },
    "workflow": {
      "execution": {
        "blogs": "Best practices and implementation patterns",
        "description": "Research prompts are executed in parallel by AI language models. Each prompt is processed independently, allowing the system to gather information on multiple aspects of your task simultaneously. Results are synthesized into structured findings with titles and actionable insights.",
        "documentation": "API documentation and technical specifications",
        "forums": "Error resolution and troubleshooting approaches",
        "github": "Code examples and implementation patterns",
        "heading": "Research Execution",
        "releases": "Version compatibility and migration guidance",
        "sourcesHeading": "Research Focus Areas"
      },
      "heading": "Research Workflow Stages",
      "processing": {
        "deduplication": "Findings consolidated across multiple research prompts",
        "description": "Research findings are structured into JSON format with titles and detailed findings. The system aggregates results from parallel research tasks, tracks success and failure counts, and provides a summary of the research outcomes. Results are stored in job metadata for easy access.",
        "extraction": "Key findings extracted and formatted for integration",
        "heading": "Result Processing & Synthesis",
        "scoring": "Results organized by research topic and relevance",
        "snippets": "Actionable insights and recommendations highlighted",
        "stepsHeading": "Processing Steps",
        "timestamp": "Research execution tracked with timing metrics"
      },
      "queryGeneration": {
        "api": "API documentation and library-specific research",
        "compatibility": "Version compatibility and migration paths",
        "description": "Research prompts are automatically generated by AI based on your task description, project context, and included files. The system analyzes your codebase structure via directory tree and file contents to formulate targeted research queries. Up to 12 focused research prompts are generated per task.",
        "errors": "Error resolution and debugging approaches",
        "heading": "Prompt Generation",
        "practices": "Best practices and recommended patterns",
        "security": "Security considerations and vulnerability awareness",
        "typesHeading": "Research Topics"
      }
    },
    "visuals": {
      "pipeline": {
        "title": "Deep Research Pipeline",
        "description": "The two-stage workflow: prompt generation and parallel research execution.",
        "imageSrc": "/images/docs/deep-research/workflow.svg",
        "imageAlt": "Deep research pipeline diagram showing prompt generation and execution stages",
        "caption": "Deep research workflow showing prompt generation and parallel execution stages"
      },
      "workflow": {
        "title": "Deep research workflow",
        "description": "The two-stage workflow: prompt generation and parallel research execution.",
        "imageSrc": "/images/docs/deep-research/workflow.svg",
        "caption": "Deep research workflow showing all processing stages"
      }
    }
  },
  "fileDiscovery": {
    "meta": {
      "title": "File discovery workflow - PlanToCode",
      "description": "Comprehensive technical guide to the 4-stage AI workflow that identifies and filters relevant files for task execution."
    },
    "apiUsage": {
      "heading": "API Usage Examples",
      "monitoring": "Monitoring Progress",
      "retrieving": "Retrieving Results",
      "starting": "Starting a Workflow"
    },
    "architecture": {
      "caching": "Intermediate results are persisted in SQLite job records for reuse and debugging.",
      "costTracking": "Cost tracking and timeout management for AI operations",
      "distributed": "The system uses a distributed job architecture where each stage runs as an independent background job, enabling cancellation, retry logic, and detailed progress tracking. Real-time events are published throughout execution to provide immediate feedback to the user interface.",
      "errorHandling": "Comprehensive error handling with automatic retry mechanisms",
      "eventDriven": "Event-driven progress reporting with WebSocket-like updates",
      "featuresHeading": "Key Architecture Features:",
      "gitIntegration": "Git integration with fallback to directory traversal",
      "heading": "Workflow Architecture",
      "overview": "The workflow operates as an orchestrated background job system with four distinct stages that execute sequentially. Each stage builds upon the previous stage's output, progressively refining the file selection based on task requirements."
    },
    "category": "Technical Guide",
    "configuration": {
      "exclusion": {
        "description": "Define directories and file patterns to exclude from the discovery process.",
        "heading": "Exclusion Patterns"
      },
      "heading": "Configuration Options",
      "retry": {
        "description": "Set maximum retry attempts for failed stages with exponential backoff.",
        "heading": "Retry Configuration"
      },
      "timeout": {
        "description": "Configure maximum execution time for the entire workflow or individual stages to prevent indefinite hanging.",
        "heading": "Timeout Management"
      },
      "workflowConfig": "Workflow Configuration"
    },
    "cta": {
      "description": "The file discovery workflow runs inside the desktop client alongside implementation planning and terminal sessions.",
      "heading": "Need the desktop app?",
      "links": {
        "architecture": "Learn about architecture",
        "buildYourOwn": "Build your own pipeline"
      }
    },
    "date": "2025-09-21",
    "description": "Comprehensive technical guide to the 4-stage AI workflow that identifies and filters relevant files for task execution.",
    "errorHandling": {
      "commonIssues": {
        "binaryDetection": "Binary file detection: Uses both extension-based and content-based binary detection",
        "gitNotFound": "Git repository not found: Falls back to directory traversal with standard exclusions",
        "heading": "Common Issues",
        "networkTimeout": "Network timeouts: Automatic retry with exponential backoff for transient failures",
        "tokenLimit": "Token limit exceeded: Implements intelligent batching and provides clear error messages"
      },
      "debugging": {
        "description": "The workflow provides comprehensive logging, performance metrics export, and detailed error context including stage information, retry attempts, and intermediate data for troubleshooting.",
        "heading": "Debugging Tools"
      },
      "errorCategories": {
        "billing": "Billing Errors: Insufficient credits or payment failures with actionable guidance",
        "heading": "Error Categories",
        "system": "System Errors: File system access, git command failures, or memory constraints",
        "validation": "Validation Errors: Invalid session ID, missing task description, or invalid project directory",
        "workflow": "Workflow Errors: Stage-specific failures with detailed context and retry suggestions"
      },
      "heading": "Error Handling & Troubleshooting"
    },
    "integration": {
      "desktop": {
        "description": "The workflow integrates seamlessly with the desktop application through Tauri commands, providing native file system access and event-driven updates via the WorkflowTracker class.",
        "heading": "Desktop Application"
      },
      "heading": "Integration Patterns",
      "implementationPlans": {
        "description": "Selected files are automatically fed into the Implementation Plans panel, ensuring that plan generation uses the same optimized file context without requiring re-execution of the discovery workflow.",
        "heading": "Implementation Plans Integration"
      },
      "sessionManagement": {
        "description": "Selected files and task history persist per session so follow-up actions can reuse the same context without rerunning discovery.",
        "heading": "Session Management"
      }
    },
    "intro": "PlanToCode identifies the right files before you plan or run commands. The 4-stage workflow narrows scope and keeps context tight.",
    "metaDescription": "Comprehensive technical guide to the 4-stage AI workflow that identifies and filters relevant files for task execution.",
    "metaTitle": "File discovery workflow - PlanToCode",
    "ogDescription": "Technical documentation for the multi-stage file discovery workflow architecture.",
    "ogTitle": "File discovery workflow - PlanToCode",
    "performance": {
      "costOptimization": {
        "description": "AI stages track actual costs from API responses, implement intelligent batching to minimize token usage, and provide cost estimates before execution to help manage expenses.",
        "heading": "Cost Optimization"
      },
      "heading": "Performance Considerations",
      "memory": {
        "description": "The workflow uses token-aware chunking, streaming responses, and cleanup of temporary data to manage memory. There is no fixed file batch size.",
        "heading": "Memory Management"
      },
      "monitoring": {
        "description": "Built-in performance tracking monitors execution times, memory usage, throughput metrics, and provides recommendations for optimization based on historical data analysis.",
        "heading": "Performance Monitoring"
      }
    },
    "readTime": "12 min",
    "stages": {
      "heading": "4-Stage Workflow Process",
      "stage1": {
        "description": "Uses AI to intelligently select the most relevant root directories from a list of candidate paths based on the task description. The LLM analyzes the primary project directory and candidate roots to determine which directories are most likely to contain files relevant to the task.",
        "heading": "Stage 1: Root Folder Selection",
        "technical": "Technical Details: Receives candidate root directories (up to depth 2) and the task description. The LLM evaluates each path against the task context and returns a filtered list of root directories that will be searched in subsequent stages.",
        "inputOutput": "Input/Output: Receives candidate_roots array and task_description. Returns root_directories array containing the AI-selected directories most relevant to the task."
      },
      "stage2": {
        "binaryDetection": "Binary Detection: Filters out files with binary extensions (.jpg, .png, .pdf, .exe, etc.) and uses content analysis to detect binary files by null bytes and non-printable character ratios.",
        "description": "Uses AI to generate intelligent regex pattern groups based on the task description and directory structure. Each pattern group can include path patterns (positive and negative) and content patterns. The processor then applies these patterns to filter files from each selected root directory.",
        "gitIntegration": "Git Integration: Finds the git repository root for each selected directory and uses git_utils to get all non-ignored files, respecting .gitignore rules while including both tracked and untracked files.",
        "heading": "Stage 2: Regex File Filter",
        "technical": "Technical Details: Generates a directory tree for each root, calls the LLM to produce patternGroups with path_pattern, content_pattern, and negative_path_pattern fields. Uses fancy-regex for lookahead/lookbehind support. Processes roots in parallel with configurable concurrency."
      },
      "stage3": {
        "aiProcessing": "AI Processing: Uses large language models to evaluate file content against task requirements, with intelligent chunking based on actual file sizes and token estimates to manage context windows efficiently.",
        "description": "Employs AI models to analyze file content and assess relevance to the specific task description. This stage performs deep content analysis by reading file contents and having the LLM identify which files are most relevant to the task.",
        "heading": "Stage 3: AI File Relevance Assessment",
        "technical": "Technical Details: Estimates tokens per file using file-type-aware heuristics (code ~3 chars/token, structured data ~5 chars/token). Creates content-aware chunks to stay under the 90k token threshold. Processes chunks in parallel with streaming to avoid timeouts. Validates all LLM-suggested paths against the filesystem."
      },
      "stage4": {
        "description": "Discovers additional relevant files by providing the LLM with the previously identified files and their contents, along with the directory tree. The AI analyzes imports, dependencies, and project structure to find related files that enhance the context for the task.",
        "heading": "Stage 4: Extended Path Finder",
        "relationship": "Relationship Analysis: Reads content of all previously identified files and provides it to the LLM alongside the directory tree (scoped to selected roots if available). The AI identifies additional files based on imports, references, and structural relationships.",
        "technical": "Technical Details: Generates a combined directory tree for selected root directories. Reads content of all initial_paths files. Uses streaming LLM calls to avoid Cloudflare timeouts. Validates discovered paths against the filesystem and normalizes to relative paths within the project."
      }
    },
    "stateManagement": {
      "eventDriven": {
        "description": "The system publishes real-time events for workflow status changes, stage completions, and error conditions. These events enable responsive user interfaces and integration with external monitoring systems.",
        "heading": "Event-Driven Updates"
      },
      "heading": "Workflow State Management",
      "intermediateData": {
        "description": "Each stage stores its output in a structured intermediate data format, including directory tree content, regex patterns, filtered file lists results. This data is accessible for debugging and can be used to resume workflows from specific stages.",
        "heading": "Intermediate Data Storage"
      },
      "transitions": {
        "description": "The workflow progresses through clearly defined states: Created → Running → Paused (optional) → Completed/Failed/Canceled. Each state transition publishes events that can be monitored for real-time updates.",
        "heading": "State Transitions"
      }
    },
    "visuals": {
      "pipeline": {
        "title": "File discovery pipeline",
        "description": "The 4-stage workflow: root folder selection, regex filtering, AI relevance assessment, and extended path discovery.",
        "imageSrc": "/images/docs/file-discovery/pipeline.svg",
        "caption": "File discovery pipeline showing all 4 stages",
        "imageAlt": "Diagram showing the 4-stage file discovery workflow: Root Folder Selection, Regex File Filter, AI File Relevance Assessment, and Extended Path Finder"
      }
    },
    "title": "File Discovery Workflow",
    "sqliteStorage": {
      "heading": "SQLite Storage",
      "description": "All workflow state, intermediate results, and job metadata are persisted in SQLite. Each stage stores its output in the background_jobs table, enabling workflow resumption and debugging. The job records include token usage, cost tracking, and system prompt templates for each AI stage."
    }
  },
  "hub": {
    "ctaDescription": "Download PlanToCode to access the implementation planner, model guardrails, terminal sessions, and transcription features described in this documentation.",
    "ctaHeading": "Ready to try these workflows?",
    "ctaLinks": {
      "overview": "Start with Overview",
      "runtime": "Runtime Walkthrough"
    },
    "description": "Learn how to plan and ship code changes with PlanToCode: file discovery, implementation plans, terminal sessions, model guardrails, and voice.",
    "exploreHeading": "Explore Documentation",
    "learnMore": "Learn More",
    "searchAriaLabel": "Search documentation",
    "searchPlaceholder": "Search documentation...",
    "searchShortcut": "⌘K",
    "title": "PlanToCode documentation"
  },
  "onThisPage": {
    "title": "On this page"
  },
  "sidebar": {
    "title": "Documentation"
  },
  "sections": {
    "architecture": {
      "title": "Architecture & Internals"
    },
    "inputs": {
      "title": "Inputs & Capture"
    },
    "planning": {
      "title": "Planning Pipeline"
    },
    "execution": {
      "title": "Execution & Automation"
    },
    "research": {
      "title": "Research & Models"
    },
    "platform": {
      "title": "Build & Deployment"
    }
  },
  "items": {
    "overview": {
      "title": "System Overview",
      "description": "Start here: what the system does, how the core loop works, and where each component lives."
    },
    "runtime-walkthrough": {
      "title": "Runtime Walkthrough",
      "description": "End-to-end timeline of what happens from task input to execution."
    },
    "architecture": {
      "title": "System Architecture",
      "description": "How the desktop shell, Rust services, server APIs, and persistence layers fit together."
    },
    "desktop-app": {
      "title": "Desktop App Internals",
      "description": "Tauri v2 shell, Rust command layer, PTY sessions, and UI state management."
    },
    "server-api": {
      "title": "Server API & LLM Proxy",
      "description": "Auth, provider routing, model configuration, and WebSocket endpoints."
    },
    "mobile-ios": {
      "title": "iOS Client Architecture",
      "description": "Swift workflows, Auth0 login flow, and device-link session management."
    },
    "background-jobs": {
      "title": "Background Jobs & Orchestration",
      "description": "Job records, workflow orchestration, processors, and event streaming."
    },
    "data-model": {
      "title": "Data Model & Storage",
      "description": "SQLite entities, relationships, and how state is rehydrated."
    },
    "decisions-tradeoffs": {
      "title": "Technical Decisions & Tradeoffs",
      "description": "Why Tauri, SQLite, and a dedicated LLM proxy were chosen and what they cost."
    },
    "build-your-own": {
      "title": "Build Your Own Pipeline",
      "description": "Conceptual guide for designing file discovery and plan generation workflows."
    },
    "meeting-ingestion": {
      "title": "Meeting & Recording Ingestion",
      "description": "How recordings become structured task inputs and artifacts."
    },
    "video-analysis": {
      "title": "Video Analysis",
      "description": "Frame sampling, prompts, and analysis artifacts from recordings."
    },
    "voice-transcription": {
      "title": "Voice Transcription",
      "description": "Recording lifecycle, project-aware settings, and device management."
    },
    "text-improvement": {
      "title": "Text Improvement",
      "description": "Selection popover, job queue, and integrations for prompt cleanup."
    },
    "file-discovery": {
      "title": "File Discovery Workflow",
      "description": "Background workflow that gathers relevant paths for each task."
    },
    "implementation-plans": {
      "title": "Implementation Plans",
      "description": "How plans stream into the Monaco viewer and stay linked to plan history."
    },
    "merge-instructions": {
      "title": "Merge Instructions",
      "description": "How multiple plan drafts are merged using XML-tagged source plans and user guidance."
    },
    "prompt-types": {
      "title": "Prompt Types & Templates",
      "description": "Catalog of prompt-driven job types and template assembly."
    },
    "terminal-sessions": {
      "title": "Terminal Sessions",
      "description": "Persistent PTY sessions, CLI detection, and recovery behaviour."
    },
    "copy-buttons": {
      "title": "Copy Buttons",
      "description": "Template handoff from plans into terminals and external tools."
    },
    "deep-research": {
      "title": "Deep Research & Web Search",
      "description": "Web search workflow, API integration, query optimization, and development workflow integration."
    },
    "provider-routing": {
      "title": "Provider Routing & Streaming",
      "description": "How provider requests are normalized, streamed, and tracked."
    },
    "model-configuration": {
      "title": "Model Configuration",
      "description": "Allowed models per task and token guardrails in the selector toggle."
    },
    "server-setup": {
      "title": "Dedicated Server Setup",
      "description": "Ansible-based infrastructure: base hardening, app deployment, and vault-managed secrets."
    },
    "tauri-v2": {
      "title": "Tauri v2 Development Guide",
      "description": "Project layout, commands, and capability-based permissions for Tauri v2."
    },
    "distribution-macos": {
      "title": "macOS Distribution",
      "description": "Signing, notarization, DMG packaging, and updater artifacts."
    },
    "distribution-windows": {
      "title": "Windows Distribution & Store",
      "description": "NSIS builds, MSIX packaging, and Microsoft Store submission."
    }
  },
  "implementationPlans": {
    "meta": {
      "title": "Implementation Plans - Review AI Changes",
      "description": "Guide to AI implementation planning. Generate, review, and approve file-by-file plans before execution. Prevent duplicates and wrong paths."
    },
    "category": "Product Guide",
    "context": {
      "audit": "Plan metadata persists with each job so you can review which inputs were used (task description, selected roots/files, model settings) and compare drafts later.",
      "heading": "Context and Metadata for Corporate Governance",
      "storage": "The panel stores which repository roots were selected during the file discovery workflow so that follow-up actions reuse the same scope. It also records plan-specific metadata, such as the project directory and any prepared prompt content, so downstream prompts can be generated or copied without recomputing the workflow.",
      "tokenEstimation": "Token estimation runs before prompts are copied. The panel calls the token estimation command with the project directory, selected files, and the currently chosen model, surfacing both system and user prompt totals so teams can stay under model limits."
    },
    "cta": {
      "claudeCodeLink": "See Claude plan mode workflow",
      "codexLink": "See Codex plan mode workflow",
      "cursorLink": "See Cursor plan mode workflow",
      "description": "Human-in-the-loop implementation plans are available inside the PlanToCode desktop application. Download the build for your platform to experience safe, governed AI-assisted development.",
      "heading": "Ready to adopt AI coding agents safely?",
      "links": {
        "architecture": "System Architecture",
        "decisions": "Decisions & Tradeoffs",
        "buildYourOwn": "Build Your Own Pipeline",
        "fileDiscovery": "File Discovery Workflow"
      }
    },
    "date": "2025-09-19",
    "description": "How PlanToCode enables confident adoption of AI coding agents through human-in-the-loop review, granular file-by-file plans, and clear handoff workflows.",
    "fileGranularity": {
      "created": "Created (with complete file paths and initial content structure)",
      "declaredFiles": "Each step in a plan explicitly declares which files will be:",
      "deleted": "Deleted (with justification and dependency analysis)",
      "heading": "File-by-File Granularity",
      "impact": "Reviewers can immediately identify if critical legacy code will be modified, if breaking changes are proposed, or if the plan touches files that require additional scrutiny.",
      "intro": "Implementation plans use a highly granular structure that breaks down development tasks on a file-by-file basis, with exact file paths corresponding to the project's repository structure. This granularity makes scope explicit before any code is touched.",
      "modified": "Modified (with specific line ranges and changes described)",
      "referenced": "Referenced (for context but not modified)",
      "transmission": "The file-by-file approach also enables precise transmission of approved plans to coding agents. Instead of vague instructions like \"update the authentication system,\" agents receive exact specifications: \"modify src/auth/session_manager.rs lines 45-67 to add token rotation, create src/auth/token_store.rs with the following structure...\""
    },
    "hitl": {
      "approve": "Approve:",
      "approveDesc": "When you are ready, you can hand the plan off to a coding agent or developer for execution.",
      "conclusion": "This workflow keeps execution aligned with the plan you reviewed and helps prevent surprise changes.",
      "edit": "Edit:",
      "editDesc": "You can directly modify steps, adjust approaches, add constraints, or remove risky operations using VS Code editing features.",
      "heading": "Human-in-the-Loop Governance",
      "intro": "PlanToCode keeps planning human-in-the-loop so you can review, edit, and decide when to hand off a plan for execution.",
      "reject": "Discard:",
      "rejectDesc": "If a draft isn't useful, you can delete it from the session list.",
      "requestChanges": "Request Changes:",
      "requestChangesDesc": "Generate alternative plans or merge drafts with custom instructions to converge on the approach you want.",
      "review": "Review:",
      "reviewDesc": "Plans open in Monaco editor where reviewers can examine every proposed change with full syntax highlighting and professional editing tools.",
      "workflow": "Plans are designed for a structured review workflow before any code modifications begin:"
    },
    "intro": "Review and approve every plan before execution. File-by-file granularity keeps scope explicit and changes aligned with your requirements.",
    "metaDescription": "Guide to AI implementation planning. Generate, review, and approve file-by-file plans before execution. Prevent duplicates and wrong paths.",
    "metaTitle": "Implementation Plans - Review AI Changes",
    "multiplePlans": {
      "description": "Plans can be merged, deleted, or reopened later. The panel keeps a list of selected plan identifiers, manages a dedicated modal for terminal output tied to a plan, and exposes navigation helpers so reviewers can page through earlier plans without closing the viewer. Terminal access, prompt copy controls, and merge instructions all share the same job identifier so plan history stays consistent.",
      "heading": "Working with multiple plans"
    },
    "ogDescription": "Understand how human-in-the-loop governance and file-by-file review workflows ensure safe AI development with complete control over code modifications.",
    "ogTitle": "Human-in-the-Loop Implementation Plans in PlanToCode",
    "plansOrigin": {
      "description": "Each plan corresponds to a background job in the current session. The panel subscribes to plan data, keeps track of which plan is currently open, and exposes navigation between earlier and newer jobs. This behaviour lives inside {code} and the surrounding panel component.",
      "heading": "Where the plans come from",
      "processor": "ImplementationPlanProcessor handles plan generation. It reads relevant files, optionally generates a directory tree based on selected root directories, and assembles a unified prompt for the LLM.",
      "storage": "Plan responses are stored in the background_jobs table with metadata including planTitle, summary, sessionName, and token usage. The raw LLM response is preserved for review and debugging.",
      "streaming": "Plans stream via the LlmTaskRunner with real-time progress events. Token warnings are logged for prompts exceeding 100k tokens but processing continues with full content."
    },
    "readTime": "6 min",
    "reviewingPlans": {
      "description": "Plan content is rendered through the shared {code}, which wraps Monaco Editor. The viewer automatically detects common languages, supports copy-to-clipboard actions, virtualises very large plans, and offers optional metrics such as character counts and syntax-aware highlighting.",
      "heading": "Reviewing plans with Monaco",
      "opening": "When a plan is opened, the panel resolves the active plan by job identifier, passes the content to Monaco, and lets reviewers move between neighbouring jobs without losing the currently open modal."
    },
    "visuals": {
      "structure": {
        "title": "Implementation plan structure",
        "description": "XML format for implementation plans with file-by-file granularity and metadata.",
        "imageSrc": "/images/docs/implementation-plans/structure.svg",
        "imageAlt": "Implementation plan XML structure diagram",
        "caption": "Plan structure showing steps, files, and dependency tracking"
      }
    },
    "title": "Implementation Plans",
    "planProcessor": {
      "heading": "Plan Generation Pipeline",
      "description": "The ImplementationPlanProcessor orchestrates plan generation by loading file contents, building context, and streaming results through the LLM task runner.",
      "inputs": "Session context, task description, selected relevant files, optional directory tree (configurable via include_project_structure flag), and web search flag for external research.",
      "prompt": "Uses prompt_utils::build_unified_prompt to combine task description, full file contents (no truncation), and directory tree into a model-specific format with estimated token counts.",
      "output": "Raw LLM response stored as JobResultData::Text. Metadata includes planTitle, summary, token usage, cache statistics, and actual cost.",
      "display": "Responses stream to the UI via progress events. Plans are rendered in a Monaco-based VirtualizedCodeViewer supporting syntax highlighting and copy actions."
    },
    "schema": {
      "heading": "Plan Data Structure",
      "description": "Implementation plans are stored as raw LLM responses with associated metadata. The response text is preserved exactly as generated, while structured metadata tracks the plan context and usage.",
      "fieldsHeading": "Metadata Fields",
      "fields": [
        "planTitle - Generated or user-provided title for the plan",
        "summary - Human-readable summary of the plan",
        "sessionName - Name of the session that generated the plan",
        "isStructured - True for implementation_plan jobs; false for merge outputs",
        "isStreaming - False for completed plans (true during generation)",
        "planData - Contains agent_instructions (optional) and steps array"
      ],
      "exampleHeading": "Metadata Example",
      "example": "{\n  \"planTitle\": \"Authentication System Refactor\",\n  \"summary\": \"Implementation plan generated\",\n  \"sessionName\": \"my-project\",\n  \"isStructured\": true,\n  \"isStreaming\": false,\n  \"planData\": {\n    \"agent_instructions\": null,\n    \"steps\": []\n  }\n}"
    }
  },
  "modelConfiguration": {
    "meta": {
      "title": "Model configuration and guardrails - PlanToCode",
      "description": "How PlanToCode lets you pick allowed models per task and keeps prompts within the active context window."
    },
    "category": "Product Guide",
    "date": "2025-09-20",
    "description": "Task-level model lists, selector controls, and token guardrails in the desktop client.",
    "intro": "PlanToCode treats model selection as a task-level decision. Each workflow ships with a default model and an allowed list, and the desktop client exposes these options through a toggle that prevents sending prompts that exceed the active context window.",
    "visuals": {
      "selector": {
        "title": "Model selector toggle",
        "description": "How the model selector shows allowed models with token guardrails.",
        "imageSrc": "/images/docs/model-configuration/selector.png",
        "imageAlt": "Model selector toggle with token estimates and guardrails",
        "caption": "Placeholder for the model selector view."
      }
    },
    "metaDescription": "How PlanToCode lets you pick allowed models per task and keeps prompts within the active context window.",
    "metaTitle": "Model configuration and guardrails - PlanToCode",
    "ogDescription": "Learn how task-level model settings, selector toggles, and token estimates work together.",
    "ogTitle": "Model configuration and guardrails - PlanToCode",
    "promptEstimation": {
      "description": "Token counts are calculated through the token estimation command. The panel submits the session id, task description, relevant files, and the selected model so the backend can return system, user, and total token values. These numbers feed directly into the selector guardrails and let teams spot over-limit prompts before copying them into another tool.",
      "heading": "Prompt estimation"
    },
    "readTime": "5 min",
    "selectorToggle": {
      "description": "The Implementation Plans panel renders allowed models with the {code}. The toggle displays each allowed model, tracks the active selection, and checks whether the estimated prompt plus planned output tokens fit within the model's advertised context window before allowing a switch.",
      "guardrails": "If a model cannot support the total token requirement, the toggle disables the button and surfaces a tooltip with the computed overage, keeping reviewers within safe limits before they send work to an agent.",
      "heading": "Selector toggle in the client"
    },
    "taskDefaults": {
      "description": "Default models and allowed alternatives are stored server-side in the application configuration. Each task type - such as implementation plans, merges, prompt generation, or voice transcription - defines a preferred model, a list of allowed options, and token limits that the desktop app reads at runtime.",
      "heading": "Task-driven defaults"
    },
    "title": "Model Configuration"
  },
  "terminalSessions": {
    "meta": {
      "title": "Terminal sessions - PlanToCode",
      "description": "Technical guide to PTY terminal implementation in PlanToCode. Learn how sessions persist, agent inactivity detection works, and recovery mechanisms."
    },
    "attentionDetection": {
      "conclusion": "This approach helps you track when agents have finished tasks or need guidance, without trying to guess why they stopped. Attention indicators clear automatically when new output is received.",
      "heading": "Agent attention detection",
      "intro": "The terminal monitors agent activity through a two-level inactivity detection system. When an agent stops producing output, the system progressively alerts you to check what has happened:",
      "level1": "Level 1 (30 seconds): \"Agent idle - may have completed task\" with yellow indicator",
      "level2": "Level 2 (2 minutes): \"Agent requires attention - check terminal\" with red indicator and desktop notification"
    },
    "category": "Product Guide",
    "date": "2025-09-22",
    "dependencyChecks": {
      "description": "Before launching commands, the terminal checks for the presence of supported CLI tools such as claude, cursor, codex, and gemini. The same command also reports the default shell so users know which environment will run. This prevents launching into a session that cannot find the required binary.",
      "heading": "Dependency checks"
    },
    "description": "Persistent PTY sessions, agent attention detection, and recovery behaviour in the Implementation Plans terminal.",
    "intro": "Run commands in a persistent PTY with health checks and logging. Voice transcription is available when you need it.",
    "visuals": {
      "sessionView": {
        "title": "Terminal session architecture",
        "description": "PTY process lifecycle, output routing, and persistence layers.",
        "imageSrc": "/images/docs/terminal-sessions/session-view.png",
        "imageAlt": "Terminal session architecture showing PTY, channels, and persistence",
        "caption": "Placeholder for terminal session architecture diagram."
      }
    },
    "lifecycle": {
      "description": "When a terminal opens, the UI component creates a PTY session and streams output through a buffered view. The component shows immediate connection status, forwards keystrokes to the PTY, and automatically retries if the session fails. Session metadata is stored in SQLite with timestamps, exit codes, working directories, and the full output log so that restarts can resume previous context.",
      "heading": "Session lifecycle"
    },
    "metaDescription": "Technical guide to PTY terminal implementation in PlanToCode. Learn how sessions persist, agent inactivity detection works, and recovery mechanisms.",
    "metaTitle": "Terminal sessions - PlanToCode",
    "ogDescription": "Understand session persistence, agent attention detection, and recovery in the plan terminal.",
    "ogTitle": "Terminal sessions - PlanToCode",
    "readTime": "6 min",
    "title": "Terminal Sessions",
    "voiceRecovery": {
      "heading": "Voice transcription and recovery",
      "recovery": "If a PTY session disconnects, the terminal surface displays recovery controls and retries the connection with exponential backoff. Health checks continue monitoring session state and provide automatic recovery actions when connection issues are detected.",
      "voice": "Inside the terminal modal, voice transcription can capture speech and paste it into the terminal input area. The recording hook looks up project-level transcription settings, tracks recording state, and inserts transcribed text when the recording stops."
    }
  },
  "copyButtons": {
    "meta": {
      "title": "Copy Buttons - PlanToCode",
      "description": "How template-driven copy buttons resolve placeholders against plans and hand off to terminals or clipboard for agent execution."
    },
    "category": "Execution",
    "date": "2025-09-23",
    "readTime": "10 min",
    "title": "Copy Buttons",
    "description": "Template-driven handoff from implementation plans to PTY terminals and external tools.",
    "intro": "Copy buttons resolve template placeholders against the active plan and then send the result to the clipboard (plan views) or the PTY (terminal modal).",
    "metaTitle": "Copy buttons - PlanToCode",
    "metaDescription": "How template-driven copy buttons resolve placeholders against plans and hand off to terminals or clipboard for agent execution.",
    "ogTitle": "Copy buttons - PlanToCode",
    "ogDescription": "Technical guide to copy button templates, placeholder resolution, and terminal handoff.",
    "visuals": {
      "templateFlow": {
        "title": "Template resolution flow",
        "description": "Templates resolve {{IMPLEMENTATION_PLAN}}, {{TASK_DESCRIPTION}}, and {{STEP_CONTENT}} before copying or sending to the terminal.",
        "imageSrc": "/images/docs/copy-buttons/templates.svg",
        "imageAlt": "Flow showing copy button template resolution",
        "caption": "Placeholder for a template resolution flow diagram."
      }
    },
    "templateConfiguration": {
      "heading": "Template Configuration Sources",
      "description": "Copy button templates follow a layered configuration model. Server defaults provide baseline templates, and project-level overrides customize the implementation_plan task for a given repo.",
      "serverDefaults": {
        "heading": "Server Defaults",
        "description": "Shared templates from /api/config/desktop-runtime-config. Includes button labels and template strings."
      },
      "projectOverrides": {
        "heading": "Project Overrides",
        "description": "Project overrides are stored in SQLite key_value_store under project_task_settings and merged with server defaults."
      },
      "taskSpecific": {
        "heading": "Task-Specific",
        "description": "Copy buttons are configured per task type (implementation_plan) and stored per project. There are no per-job overrides."
      }
    },
    "placeholderResolution": {
      "heading": "Placeholder Resolution",
      "description": "Templates use double-brace placeholders that are resolved against plan content and the current task description.",
      "placeholdersHeading": "Available Placeholders",
      "placeholders": [
        {
          "placeholder": "{{IMPLEMENTATION_PLAN}}",
          "description": "Full implementation plan content as generated by the LLM"
        },
        {
          "placeholder": "{{TASK_DESCRIPTION}}",
          "description": "The task description from the current session"
        },
        {
          "placeholder": "{{STEP_CONTENT}}",
          "description": "Content for the selected plan step (when a step is selected)"
        }
      ],
      "resolutionOrder": "Missing placeholders are replaced with empty strings. Step content is only available when a plan step is selected.",
      "exampleTemplate": "Example template:\n\n{{IMPLEMENTATION_PLAN}}\n\nUnderstand the implementation plan above thoroughly. Analyze the architecture, data flows, and sequence of events.\n\nTask: {{TASK_DESCRIPTION}}"
    },
    "processingPipeline": {
      "heading": "Template Processing Pipeline",
      "description": "When a button is clicked, placeholders are extracted, values are resolved, and the output is sent to clipboard or PTY depending on where the button is used.",
      "steps": [
        {
          "number": 1,
          "title": "Extract Placeholders",
          "description": "Regex scan for {{...}} patterns in the template string"
        },
        {
          "number": 2,
          "title": "Lookup Context",
          "description": "Resolve plan content and task description values for placeholders"
        },
        {
          "number": 3,
          "title": "Substitute Values",
          "description": "Replace placeholders with resolved values"
        },
        {
          "number": 4,
          "title": "Send Output",
          "description": "Copy to clipboard or write to the PTY input buffer"
        }
      ],
      "chunking": {
        "heading": "Large Plan Chunking",
        "description": "When sending to the PTY, the text is chunked into 4KB segments and a carriage return is appended."
      }
    },
    "terminalHandoff": {
      "heading": "PTY Terminal Handoff",
      "description": "In the plan terminal modal, copy buttons write the resolved template to the PTY input buffer as if typed by the user.",
      "detailsHeading": "Handoff Details",
      "details": [
        "Content sent via write_terminal_input_command to the PTY input buffer",
        "Chunked into 4KB segments for large plans",
        "Appends a carriage return after sending"
      ],
      "codeExample": "// Terminal handoff (PlanTerminalModal)\nconst textToSend = replacePlaceholders(button.content, {\n  IMPLEMENTATION_PLAN: planContent,\n  TASK_DESCRIPTION: taskDescription ?? \"\"\n});\nawait sendInChunks(sessionId, textToSend);"
    },
    "clipboardHandoff": {
      "heading": "Clipboard Handoff",
      "description": "In plan cards and plan modals, buttons copy the resolved template to the system clipboard using the browser clipboard API.",
      "crossPlatform": {
        "heading": "Cross-Platform API",
        "description": "Uses navigator.clipboard.writeText() inside the Tauri webview for clipboard access."
      },
      "feedback": {
        "heading": "User Feedback",
        "description": "Toast notification confirms the copy action."
      }
    },
    "defaultButtons": {
      "heading": "Default Copy Buttons",
      "description": "PlanToCode ships with several default copy buttons for implementation plans. These are templates you can edit in settings.",
      "buttonsHeading": "Built-in Buttons",
      "buttons": [
        {
          "id": "parallel-agents",
          "label": "Parallel Claude Coding Agents",
          "description": "Template that instructs Claude Code to launch parallel agents using the plan."
        },
        {
          "id": "investigate-results",
          "label": "Investigate Results",
          "description": "Template that asks the agent to review changes without launching new agents."
        },
        {
          "id": "task-only",
          "label": "Task",
          "description": "Copies only the task description."
        },
        {
          "id": "task-and-plan",
          "label": "Task + Plan",
          "description": "Combines task description and implementation plan for full context."
        },
        {
          "id": "plan-only",
          "label": "Plan",
          "description": "Copies only the implementation plan content."
        }
      ]
    },
    "customization": {
      "heading": "Customizing Copy Buttons",
      "description": "Copy buttons can be customized at multiple levels: global defaults, project-level overrides, and per-task configurations.",
      "globalDefaults": {
        "heading": "Global Defaults",
        "description": "Server-side configuration in /api/config/desktop-runtime-config defines the base set of copy buttons. These are loaded when the desktop app starts and cached for offline use."
      },
      "projectSettings": {
        "heading": "Project-Level Customization",
        "description": "Each project can override the default buttons through the Settings panel. Project-specific buttons are stored in key_value_store and merged with server defaults at runtime."
      },
      "taskSettings": {
        "heading": "Task-Level Configuration",
        "description": "Copy buttons are configured per task type (implementation_plan) and applied per project."
      },
      "editorDescription": "The copy button editor supports drag-and-drop ordering, inline label editing, and template content modification. Changes are persisted automatically."
    },
    "uiIntegration": {
      "heading": "UI Integration and Safety",
      "description": "Copy buttons appear in plan viewers and terminal headers. Clicking a button sends output immediately; there is no preview step by default.",
      "tokenEstimation": {
        "heading": "Token Estimation",
        "description": "Plan cards display token counts for the plan job; copy buttons do not compute per-template token estimates."
      },
      "previewModal": {
        "heading": "Full Preview Modal",
        "description": "There is no dedicated preview modal; open the plan content to inspect what will be copied."
      },
      "disabledState": {
        "heading": "Disabled State",
        "description": "Buttons are disabled when required context is missing (e.g., no active plan, missing session). Tooltips explain what context is needed to enable the button."
      }
    },
    "auditTrail": {
      "heading": "History and signoff",
      "description": "Copy button clicks are not stored in a dedicated table. Plan edits are stored in background_jobs.response and signoff state is recorded in background_jobs.metadata.userSignoff.",
      "schemaHeading": "Notes",
      "schema": "No copy_button_actions table exists in the current release.",
      "fieldsHeading": "Stored plan signals",
      "fields": [
        {
          "field": "background_jobs.response",
          "description": "Plan content after edits or merges"
        },
        {
          "field": "background_jobs.metadata.userSignoff",
          "description": "User signoff state and timestamp"
        }
      ],
      "retention": "No separate retention policy exists for copy button actions; job history retention is controlled in app settings."
    },
    "mobileIntegration": {
      "heading": "Mobile Integration",
      "description": "Copy buttons work in the iOS remote terminal actions bar. Resolved templates are sent to the linked desktop terminal.",
      "deviceLink": {
        "heading": "Device Link Support",
        "description": "When a mobile device is linked to a desktop session, copy buttons can target the desktop terminal directly. The resolved content is sent through the device link WebSocket connection."
      },
      "mobileButtons": {
        "heading": "Mobile-Specific Buttons",
        "description": "Mobile clients use the same copy button configuration stored in project task settings."
      }
    },
    "cta": {
      "heading": "Trace handoff to execution",
      "description": "Terminal sessions show where copy button output lands and how it is logged.",
      "terminalLink": "Terminal sessions",
      "plansLink": "Implementation plans"
    }
  },
  "textImprovement": {
    "meta": {
      "title": "Text improvement - PlanToCode",
      "description": "How the desktop workspace rewrites highlighted text, preserves formatting, and links the feature to voice and video inputs."
    },
    "category": "Product Guide",
    "cta": {
      "description": "Download PlanToCode to combine voice capture, video context, and inline rewriting before you generate implementation plans.",
      "heading": "Try text improvement in the desktop app",
      "links": {
        "architecture": "Architecture overview",
        "buildYourOwn": "Build your own"
      }
    },
    "date": "2025-09-21",
    "description": "How PlanToCode rewrites highlighted text without changing formatting and links the result back to your workspace.",
    "intro": "Refine text with AI context. Select text in any editor, trigger a background job, and get improved content that keeps your formatting intact.",
    "metaDescription": "How the desktop workspace rewrites highlighted text, preserves formatting, and links the feature to voice and video inputs.",
    "metaTitle": "Text improvement - PlanToCode",
    "ogDescription": "Understand the selection popover, job queue, model configuration, and integrations that power text improvement.",
    "ogTitle": "Text improvement - PlanToCode",
    "readTime": "7 min",
    "selectionPopover": {
      "component": "The popover itself is a minimal component rendered by {code}, which simply triggers the provider hook and shows a loading indicator while a rewrite is running. Because the provider registers global listeners, the popover appears in Monaco plan viewers, the plan terminal dictation field, and any task description inputs without extra wiring.",
      "heading": "Selection popover behaviour",
      "provider": "The {code} listens for selection events on standard inputs and Monaco editors. When you highlight non-empty text it positions a popover near the cursor, stores the selected range, and tracks whether the popover should be visible. Clicking the button kicks off the job and disables the control until the result returns. When the job completes the provider applies the improved text back into the same selection and flushes any pending saves to keep session state in sync."
    },
    "title": "Text Improvement",
    "triggerImprovement": {
      "action": "Pressing the popover button calls {code}. The action validates the selection, ensures a session identifier exists, and invokes the Rust command {code} via Tauri. The command builds a {code} containing the original text and queues a background job against the active session.",
      "backend": "On the backend, the {code} resolves the configured model for the {code} task, wraps the selection in XML tags, and runs the request through the {code} without streaming. When the model response returns it records token usage, cost, and the system prompt template before emitting the improved text back to the UI. The default configuration ships with Claude Sonnet 4.5 and Gemini 3 Pro as the approved models, capped at 4,096 tokens with a temperature of 0.7.",
      "heading": "What happens when you trigger an improvement",
      "metadata": "The background jobs sidebar records the original text in job metadata, so you can review what was sent alongside the rewritten copy. If the selection changes while a job is running, the provider skips replacing the text to avoid clobbering manual edits."
    },
    "videoCapture": {
      "dialog": "The video analysis dialog combines the current task description with an optional focus prompt wrapped in <description> and <video_attention_prompt> tags before sending the job. You can narrate while recording; the resulting summary can be pasted into the task description and refined with the improvement popover.",
      "features": "Video jobs include frame-rate controls, optional audio capture, and usage tracking. Results appear in the background jobs sidebar alongside text improvements.",
      "heading": "Video capture and prompt scaffolding"
    },
    "voiceIntegration": {
      "heading": "Voice transcription integration",
      "hook": "Voice recordings use the {code} hook. It loads per-project transcription defaults, requests microphone access, and inserts transcribed text at the cursor inside the task description or terminal dictation buffer. The inserted text can be highlighted and passed through the improvement popover.",
      "preferences": "Language, model, and temperature preferences persist at the project level, so teams get consistent transcription quality before refining the copy. Silence detection warns about bad audio levels, and a ten-minute cap prevents oversized recordings from blocking improvement jobs with large payloads."
    },
    "visuals": {
      "popoverFlow": {
        "title": "Text improvement flow",
        "description": "Selection popover triggers improvement job and returns enhanced text.",
        "imageSrc": "/images/docs/text-improvement/flow.svg",
        "imageAlt": "Text improvement flow diagram",
        "caption": "Selection popover to job pipeline overview."
      }
    },
    "processorDetails": {
      "heading": "Processor implementation details",
      "processor": "The {code} handles the text rewriting workflow on the Rust backend.",
      "stepsHeading": "Processing steps",
      "steps": [
        "Parse the incoming payload with original text and selection metadata",
        "Build the system prompt from the configured text_improvement template",
        "Submit the request to the LLM task runner without streaming",
        "Extract the improved text from the model response",
        "Record token usage, cost, and prompt template for billing",
        "Emit the result back to the UI via Tauri events"
      ]
    },
    "inlineRewriting": {
      "heading": "Inline rewriting behaviour",
      "description": "When the improved text returns, the provider automatically replaces the original selection. The rewriting preserves whitespace, line breaks, and any inline formatting present in the source. If the editor is Monaco-based, the change is applied as a single undo-able edit operation.",
      "contextsHeading": "Supported contexts",
      "contexts": [
        "Task description input fields",
        "Plan terminal dictation area",
        "Monaco plan viewers and editors",
        "Any standard HTML input or textarea"
      ]
    },
    "modelConfiguration": {
      "heading": "Model configuration",
      "description": "Text improvement uses the text_improvement task configuration from the desktop runtime config. You can override the default model and parameters in the settings panel.",
      "settingsHeading": "Configurable settings",
      "settings": [
        "Allowed models list (default: Claude Sonnet 4.5, Gemini 3 Pro)",
        "Maximum token limit (default: 4096)",
        "Temperature setting (default: 0.7)",
        "System prompt template override"
      ]
    },
    "keyFiles": {
      "heading": "Key implementation files",
      "items": [
        "desktop/src/contexts/TextImprovementProvider.tsx",
        "desktop/src/components/TextImprovementPopover.tsx",
        "desktop/src/actions/text-improvement/index.ts",
        "desktop/src-tauri/src/jobs/processors/text_improvement.rs",
        "server/src/config/task_model_config.rs"
      ]
    }
  },
  "voiceTranscription": {
    "meta": {
      "title": "Voice transcription - PlanToCode",
      "description": "How PlanToCode records audio, sends it to the configured transcription provider, and inserts text into task or terminal inputs."
    },
    "category": "Product Guide",
    "date": "2025-09-22",
    "description": "Recording lifecycle, device management, and transcription behavior for voice-driven prompts.",
    "deviceManagement": {
      "description": "The feature requests microphone permission, enumerates available audio inputs, and lets users choose the active device before recording. Changes take effect on the next recording.",
      "heading": "Device management",
      "monitoring": "Real-time audio level monitoring displays visual feedback during recording. The system warns when audio is silent so you can catch muted microphones before sending the recording."
    },
    "intro": "Voice transcription is available anywhere the desktop app exposes dictation controls, including the plan terminal and prompt editors. The feature records audio locally and sends a single recording to the transcription service when you stop, then inserts text into the active input field without blocking manual typing.",
    "metaDescription": "How PlanToCode records audio, sends it to the configured transcription provider, manages permissions, and inserts text into task or terminal inputs.",
    "metaTitle": "Voice transcription - PlanToCode",
    "ogDescription": "Learn how the recording hook manages devices, permissions, and streaming text.",
    "ogTitle": "Voice transcription - PlanToCode",
    "projectSettings": {
      "description": "When a recording session starts, the hook looks up the active project's transcription configuration so recordings follow the project's preferences.",
      "heading": "Project-aware settings",
      "storage": "Project transcription preferences are stored in SQLite key_value_store under project_task_settings and include the preferred model, language code, prompt, and temperature. Hosted uses managed providers; self-hosting can adjust the allowlist."
    },
    "readTime": "5 min",
    "recordingWorkflow": {
      "description": "The recording hook keeps a state machine with idle, recording, processing, and error states. It records audio into a single blob, enforces a ten-minute cap, and sends the recording on stop.",
      "heading": "Recording workflow",
      "statesHeading": "Recording states",
      "states": [
        "idle: No recording in progress, microphone permissions may or may not be granted",
        "recording: Capturing audio via MediaRecorder with live level monitoring",
        "processing: Uploading the recording to the transcription endpoint and awaiting a response",
        "error: Recording failed due to permission denial, device disconnection, or transcription API error"
      ]
    },
    "routingBehavior": {
      "heading": "Multi-destination routing",
      "description": "Transcribed text is routed based on the active UI context and inserted into the appropriate input.",
      "destinations": [
        "Task description editors: Cursor insertion with optional follow-up text_improvement",
        "Terminal dictation buffer: Command text inserted into PTY input",
        "Prompt editors: Direct insertion into active text inputs"
      ]
    },
    "pipeline": {
      "heading": "Transcription pipeline",
      "hook": "The {code} React hook manages the complete recording lifecycle. It initializes {code} for audio capture in WebM format with Opus codec, monitors audio levels, and handles device switching.",
      "command": "The desktop app invokes {code} to send audio data to the server endpoint {code}. The command validates minimum size (1KB), duration, temperature (0.0-1.0), and prompt length (max 1000 characters); the server enforces max file size (100MB).",
      "constraints": "Audio files must be between 1KB and 100MB. Supported formats: WAV, MP3, M4A, OGG, WebM, FLAC, AAC, and MP4. The transcription model must be specified explicitly and must be in the server allowlist (OpenAI models by default on hosted)."
    },
    "serverProcessing": {
      "heading": "Server-side processing",
      "endpoint": "The server exposes {code} which accepts multipart form data. It routes requests to OpenAI or Google based on the model's provider configuration, validates user credits, and calculates billing based on audio duration.",
      "parametersHeading": "Request parameters",
      "parameters": [
        "file: Audio file data (required) - WAV, MP3, M4A, OGG, WebM, FLAC, AAC, or MP4",
        "model: Transcription model ID (required) - from server allowlist (e.g., openai/gpt-4o-transcribe)",
        "durationMs: Recording duration in milliseconds (required for billing calculation)",
        "language: ISO 639-1 language code (optional) - improves accuracy for specific languages",
        "prompt: Context hint for transcription (optional, max 1000 characters) - helps with domain-specific vocabulary",
        "temperature: Sampling temperature 0.0-1.0 (optional, default 0.0) - lower values produce more deterministic output"
      ]
    },
    "dataFlow": {
      "heading": "Data flow",
      "description": "Audio data flows from the browser through the Tauri command layer to the server, which proxies requests to the appropriate transcription provider.",
      "stepsHeading": "Processing steps",
      "steps": [
        "Browser MediaRecorder captures audio in a single recording (WebM by default)",
        "useVoiceTranscription tracks duration and recording state",
        "On stop, the audio blob is converted to bytes and sent via transcribe_audio_command",
        "Tauri command validates size, duration, temperature, and prompt length",
        "Request sent to server /api/audio/transcriptions endpoint with auth token",
        "Server routes to the configured provider and returns transcribed text",
        "Transcribed text returned to desktop and inserted via callback"
      ]
    },
    "keyFiles": {
      "heading": "Key implementation files",
      "items": [
        "desktop/src/hooks/use-voice-recording/use-voice-transcription.ts",
        "desktop/src/actions/voice-transcription/transcribe.ts",
        "desktop/src-tauri/src/commands/audio_commands.rs",
        "server/src/handlers/proxy/specialized/transcription.rs",
        "server/src/clients/openai/transcription.rs",
        "server/src/clients/google_client.rs"
      ]
    },
    "examples": {
      "heading": "Usage examples",
      "description": "Common voice transcription workflows:",
      "items": [
        "Sprint planning: Dictate tasks, then run text_improvement and task_refinement",
        "Terminal commands: Dictation transcribed and typed directly into PTY for execution",
        "Bug reports: Verbal description captured, refined with text_improvement, then stored in task history",
        "Walkthrough notes: Narrate a screen recording and attach the video analysis summary to the task"
      ]
    },
    "cta": {
      "heading": "Continue exploring",
      "description": "Learn how transcribed text can be refined and how meeting recordings are processed into actionable tasks.",
      "links": {
        "textImprovement": "Text Improvement",
        "meetingIngestion": "Meeting Ingestion"
      }
    },
    "title": "Voice Transcription",
    "visuals": {
      "recordingFlow": {
        "title": "Voice transcription pipeline",
        "description": "Audio capture, provider transcription, and text insertion flow.",
        "imageSrc": "/images/docs/voice-transcription/pipeline.svg",
        "imageAlt": "Voice transcription pipeline diagram",
        "caption": "Audio flows from browser capture through Tauri commands to the configured transcription provider."
      }
    }
  },
  "overview": {
    "meta": {
      "title": "System overview - PlanToCode",
      "description": "Start here: what PlanToCode does, how the core loop works, and where each component lives in the repo."
    },
    "category": "Overview",
    "date": "2025-09-25",
    "readTime": "15 min",
    "title": "System Overview",
    "description": "A concise map of the system, the core loop, and the required dependencies.",
    "intro": "PlanToCode is a desktop workspace that plans and validates code changes before execution. It coordinates a local Rust job engine, a React UI, and a server proxy for LLM calls. The system follows an offline-first architecture where the desktop app operates independently using SQLite for local state, while the server handles authentication, LLM provider routing, and billing. Without LLM provider access (managed on hosted or self-hosted with your keys), the planning and analysis pipelines will not run.",
    "visuals": {
      "systemMap": {
        "title": "System map",
        "description": "Map of the desktop app, the Rust core, local SQLite storage, and the server proxy.",
        "imageSrc": "/images/docs/overview/system-map.svg",
        "imageAlt": "PlanToCode system map diagram",
        "caption": "Four-layer architecture with data flowing down and events streaming back up."
      }
    },
    "systemLayers": {
      "heading": "System layers",
      "description": "The system is organized into four distinct layers that communicate through well-defined interfaces:",
      "items": [
        "Presentation Layer: React UI with Monaco editors, terminal panels, and workflow controls (desktop/src/)",
        "Command Layer: Tauri commands that bridge React and Rust, handling IPC and state management (desktop/src-tauri/src/commands/)",
        "Processing Layer: Job processors, workflow orchestrator, and business logic in Rust (desktop/src-tauri/src/jobs/)",
        "Persistence Layer: SQLite repositories for local state and server PostgreSQL for auth/billing (desktop/src-tauri/src/db_utils/)"
      ]
    },
    "coreLoop": {
      "heading": "Core loop in practice",
      "description": "Every task flows through a well-defined lifecycle from capture to execution:",
      "steps": [
        "Capture the task from text, voice transcription (via useVoiceTranscription hook), or video recording analysis.",
        "Refine the task description and objectives with text_improvement jobs through TextImprovementProcessor.",
        "Run the file discovery workflow: RootFolderSelectionProcessor selects directories, RegexFileFilterProcessor applies patterns, FileRelevanceAssessmentProcessor scores contents, ExtendedPathFinderProcessor expands context.",
        "Generate implementation plans via ImplementationPlanProcessor, which streams XML-formatted plans to the Monaco viewer.",
        "Optionally merge multiple plan drafts with ImplementationPlanMergeProcessor using XML-tagged source plans.",
        "Execute or export the approved plan through PTY terminal sessions or copy-button templates for external agents.",
        "Persist every job, artifact, and terminal log to SQLite (background_jobs, terminal_sessions tables) for history and recovery."
      ]
    },
    "components": {
      "heading": "Major components",
      "description": "Each component has a specific responsibility and communicates through typed interfaces:",
      "items": [
        "Desktop UI (React) in desktop/src/ with Monaco plan views, terminal panels, and providers (SessionProvider, TextImprovementProvider).",
        "Rust core (Tauri v2) in desktop/src-tauri/ handling commands, jobs, and persistence with capability-based permissions.",
        "Local SQLite schema in desktop/src-tauri/migrations/consolidated_schema.sql with WAL mode for concurrent access.",
        "Server proxy (Actix-Web) in server/src/ for auth, provider routing, streaming responses, and billing via Stripe.",
        "Mobile iOS client in mobile/ios/Core/ with SwiftUI interface, Auth0 PKCE, and WebSocket device linking.",
        "Infrastructure automation in infrastructure/ansible/ for Hetzner (EU) and InterServer (US) dedicated servers."
      ]
    },
    "dependencies": {
      "heading": "Required dependencies",
      "description": "The system requires these external services and resources:",
      "items": [
        "External LLM providers (OpenAI, Anthropic, Google, X.AI, OpenRouter) for plan generation, transcription, and analysis.",
        "Auth0-based authentication with PKCE flow for desktop and mobile sessions.",
        "PostgreSQL 17 and Redis 7+ for server-side user accounts, billing state, and job queues (self-hosted deployments).",
        "Local filesystem access via git ls-files or directory traversal for file discovery workflows.",
        "Whisper-compatible transcription endpoint for voice input processing."
      ]
    },
    "codeMap": {
      "heading": "Where the behavior lives in the repo",
      "description": "Quick reference to key directories and files:",
      "items": [
        "Tauri commands: desktop/src-tauri/src/commands/ (35+ command modules: job_commands.rs, workflow_commands.rs, terminal_commands.rs, session_commands.rs, auth0_commands.rs)",
        "Workflow orchestration: desktop/src-tauri/src/jobs/workflow_orchestrator/ (definition_loader.rs, stage_scheduler.rs, event_emitter.rs, payload_builder.rs)",
        "Job processors: desktop/src-tauri/src/jobs/processors/ (implementation_plan_processor.rs, text_improvement_processor.rs, root_folder_selection_processor.rs)",
        "SQLite repositories: desktop/src-tauri/src/db_utils/ (background_job_repository/, session_repository.rs, terminal_repository.rs)",
        "Server routes: server/src/routes.rs (configure_routes, configure_public_auth_routes, configure_webhook_routes)",
        "LLM proxy handlers: server/src/handlers/proxy_handlers.rs and server/src/handlers/proxy/ (router.rs, providers/)",
        "Provider transformers: server/src/handlers/provider_transformers/ (openai.rs, google.rs, anthropic.rs, xai.rs)",
        "iOS workflows: mobile/ios/Core/Sources/Workflows/WorkflowManager.swift with MobileSessionManager and APIClient",
        "Infrastructure playbooks: infrastructure/ansible/site-base.yml (hardening, PostgreSQL, Redis) and site-app.yml (deployment)"
      ]
    },
    "keyAbstractions": {
      "heading": "Key abstractions",
      "description": "Understanding these core concepts helps navigate the codebase:",
      "items": [
        "Session: Project context stored in sessions table with task_description, included_files, and model preferences. Identified by UUID.",
        "Background Job: LLM-backed operation stored in background_jobs table with task_type, prompt, response, token tracking, and cost.",
        "Workflow: Multi-stage orchestrated process (e.g., file_finder_workflow) coordinated by WorkflowOrchestrator with IntermediateData passing between stages.",
        "Terminal Session: PTY process stored in terminal_sessions with output_log, status, and optional job_id linking for traceability.",
        "Provider: LLM service abstraction in server/src/handlers/proxy/providers/ with request transformation and response normalization."
      ]
    },
    "dataFlowSummary": {
      "heading": "Data flow summary",
      "description": "How data moves through the system for a typical planning task:",
      "items": [
        "User input enters through React components and flows to Tauri commands via @tauri-apps/api/core invoke().",
        "Commands create background_jobs records and dispatch to job processors via the job queue.",
        "Processors build prompts, send requests through the server LLM proxy, and stream responses via Tauri events.",
        "Responses are stored in SQLite and emitted to React providers that update the UI state.",
        "Terminal execution streams PTY output to the UI and persists logs for session recovery."
      ]
    }
  },
  "runtimeWalkthrough": {
    "meta": {
      "title": "Runtime walkthrough - PlanToCode",
      "description": "End-to-end timeline of a task from input to plan output, with job types and artifact flows."
    },
    "category": "Architecture",
    "date": "2025-09-25",
    "readTime": "12 min",
    "title": "Runtime Walkthrough",
    "description": "End-to-end runtime timeline from task input to plan output.",
    "intro": "This walkthrough traces a single task from initial capture through file discovery, plan generation, and terminal execution. Each stage maps to specific job types and produces artifacts stored in SQLite.",
    "visuals": {
      "timeline": {
        "title": "Runtime timeline",
        "description": "Visual timeline showing task input, workflow stages, and plan output.",
        "imageSrc": "/images/docs/runtime-walkthrough/timeline.svg",
        "imageAlt": "Runtime timeline diagram",
        "caption": "Task execution flows through six stages, with all artifacts persisted to SQLite."
      },
      "walkthroughVideo": {
        "title": "Runtime walkthrough video",
        "description": "Video demonstration of a complete task execution from input to plan output.",
        "videoSrc": "",
        "posterSrc": "",
        "caption": "Video walkthrough placeholder - record a demonstration of the full planning workflow."
      }
    },
    "timeline": {
      "heading": "High-level runtime sequence",
      "description": "A complete task execution follows this sequence of operations:",
      "steps": [
        "User enters or dictates a task description in the desktop UI via TaskDescriptionEditor component.",
        "Optional: text_improvement job refines the raw input through TextImprovementProcessor.",
        "User triggers file discovery workflow via the Implementation Plans panel start_file_finder_workflow command.",
        "WorkflowOrchestrator in desktop/src-tauri/src/jobs/workflow_orchestrator/ creates a workflow record and schedules stage 1.",
        "Stage 1 (root_folder_selection): RootFolderSelectionProcessor sends directory tree to LLM, stores selected roots in IntermediateData.selectedRoots.",
        "Stage 2 (regex_file_filter): RegexFileFilterProcessor generates patterns, runs git ls-files, stores matches in IntermediateData.locallyFilteredFiles.",
        "Stage 3 (file_relevance_assessment): FileRelevanceAssessmentProcessor chunks file contents, scores relevance, stores in IntermediateData.aiFilteredFiles.",
        "Stage 4 (extended_path_finder): ExtendedPathFinderProcessor expands context with imports and dependencies, stores in IntermediateData.verifiedPaths.",
        "UI receives workflow-completed event via event_emitter.rs, updates file selection display.",
        "User triggers plan generation with selected files via generate_implementation_plan command.",
        "ImplementationPlanProcessor in desktop/src-tauri/src/jobs/processors/implementation_plan_processor.rs streams XML plan content to Monaco viewer via job:stream-progress events.",
        "User reviews plan in VirtualizedCodeViewer component, can edit directly or request merge.",
        "Approved plan is copied to terminal via copy-button templates or exported for external agents.",
        "Terminal session in terminal_commands.rs captures PTY output, detects agent attention states.",
        "All artifacts persist in SQLite background_jobs and terminal_sessions tables for history and session recovery."
      ]
    },
    "jobTypes": {
      "heading": "Job types in the runtime",
      "description": "Each task_type maps to a specific processor and produces distinct artifacts:",
      "items": [
        "text_improvement: TextImprovementProcessor wraps text in XML, sends to LLM, returns refined text. Stored in background_jobs.response.",
        "root_folder_selection: RootFolderSelectionProcessor receives directory tree, returns JSON array of selected directories.",
        "regex_file_filter: RegexFileFilterProcessor generates patterns from task description, applies to git file list.",
        "file_relevance_assessment: FileRelevanceAssessmentProcessor loads file contents, chunks by token limit, scores relevance.",
        "extended_path_finder: ExtendedPathFinderProcessor analyzes imports/dependencies, expands context with related files.",
        "implementation_plan: ImplementationPlanProcessor streams XML-formatted plans with plan_step elements.",
        "implementation_plan_merge: ImplementationPlanMergeProcessor combines plans using source_plans XML tags and user instructions.",
        "video_analysis: Processes screen recordings via /api/llm/video/analyze with a framerate hint.",
        "web_search_prompts_generation: Generates research_task XML blocks for deep research workflow.",
        "web_search_execution: Executes research prompts in parallel, aggregates findings."
      ]
    },
    "inputCapture": {
      "heading": "Task input capture",
      "description": "Tasks enter the system through multiple input surfaces:",
      "text": "Task descriptions are typed or pasted into TaskDescriptionEditor which persists to sessions.task_description and creates history entries in task_description_history table with device_id for multi-device sync.",
      "voice": "Voice input uses useVoiceTranscription hook which records via MediaRecorder API, sends to /api/audio/transcriptions, and inserts at cursor position.",
      "video": "Video analysis uses VideoAnalysisDialog to capture screen recordings, upload to /api/llm/video/analyze, and extract UI state observations."
    },
    "workflowExecution": {
      "heading": "Workflow execution details",
      "description": "The WorkflowOrchestrator coordinates multi-stage workflows:",
      "scheduling": "workflow_lifecycle_manager.rs creates workflow records and stage_scheduler.rs dispatches stages sequentially based on workflow JSON definitions.",
      "data": "IntermediateData structures in workflow_types.rs pass outputs between stages: selectedRoots, rawRegexPatterns, locallyFilteredFiles, aiFilteredFiles, verifiedPaths.",
      "events": "event_emitter.rs publishes workflow-status and workflow-stage Tauri events consumed by WorkflowTracker in the React UI."
    },
    "persistence": {
      "heading": "State persistence",
      "description": "All artifacts are persisted for review and recovery:",
      "jobs": "background_job_repository/ stores job records with session_id, task_type, status, prompt, response, tokens_sent/received, actual_cost.",
      "sessions": "session_repository.rs manages sessions table with task_description, included_files, model_used, history versions.",
      "terminals": "terminal_repository.rs persists terminal_sessions with output_log, status, exit_code, working_directory for session recovery.",
      "rehydration": "On app restart, the Rust core rehydrates session state from SQLite, marks stale running jobs as failed, and restores terminal output logs."
    },
    "inputs": {
      "heading": "Task input capture",
      "capture": "Tasks enter the system through multiple input surfaces: typed text in TaskDescriptionEditor, voice dictation via useVoiceTranscription hook, or video analysis through VideoAnalysisDialog.",
      "artifacts": "Each input type updates SQLite state: task_description in sessions and task_description_history, voice transcription inserts text into the session or terminal input, and video analysis responses are stored in background_jobs."
    },
    "refinement": {
      "heading": "Input refinement",
      "jobs": "The text_improvement job type refines raw input through TextImprovementProcessor, wrapping text in XML and sending to the LLM for grammar, clarity, and structure improvements.",
      "storage": "Refined text is stored in background_jobs.response and can update sessions.task_description via the React provider."
    },
    "discovery": {
      "heading": "File discovery workflow",
      "workflow": "FileFinderWorkflow runs four sequential stages: root_folder_selection narrows directories, regex_file_filter applies patterns, file_relevance_assessment scores content, and extended_path_finder expands with dependencies.",
      "outputs": "Each stage stores results in IntermediateData structures passed between processors, with final file selections persisted to sessions.included_files."
    },
    "planGeneration": {
      "heading": "Plan generation",
      "jobs": "The implementation_plan job type uses ImplementationPlanProcessor to generate XML-formatted plans with plan_step elements containing file paths, operation types, and code changes.",
      "streaming": "Plan content streams to the UI via job:stream-progress Tauri events, displayed in the VirtualizedCodeViewer Monaco component with syntax highlighting."
    },
    "merge": {
      "heading": "Plan merging",
      "instructions": "The implementation_plan_merge job combines multiple plans using source_plans XML tags and user-provided merge instructions to resolve conflicts and consolidate changes.",
      "outputs": "Merged plans maintain traceability to source plans and include merged_from metadata in the final background_jobs record."
    },
    "review": {
      "heading": "Plan review",
      "editor": "Plans open in the Monaco-based VirtualizedCodeViewer for review. Users can edit plan text directly, request modifications, or approve for execution.",
      "audit": "Plan edits are persisted in background_jobs.response; signoff state is recorded in background_jobs.metadata.userSignoff."
    },
    "execution": {
      "heading": "Execution handoff",
      "terminal": "Approved plans are copied to the integrated terminal via copy-button templates, or exported for external agents like Claude Code, Cursor, or Codex.",
      "logging": "Terminal sessions in terminal_commands.rs capture PTY output, detect agent attention states, and log all execution activity to terminal_sessions table."
    },
    "state": {
      "heading": "State persistence",
      "jobs": "All job artifacts persist in background_jobs table with session_id, task_type, status, prompt, response, token counts, and cost tracking.",
      "rehydration": "On app restart, the Rust core rehydrates session state from SQLite, marks stale running jobs as failed, and restores terminal output logs."
    },
    "jobMap": {
      "heading": "Job type mapping",
      "items": [
        "text_improvement → TextImprovementProcessor → refined task descriptions",
        "root_folder_selection → RootFolderSelectionProcessor → selected directories",
        "regex_file_filter → RegexFileFilterProcessor → pattern-matched files",
        "file_relevance_assessment → FileRelevanceAssessmentProcessor → scored files",
        "extended_path_finder → ExtendedPathFinderProcessor → expanded context",
        "implementation_plan → ImplementationPlanProcessor → XML plan documents",
        "implementation_plan_merge → ImplementationPlanMergeProcessor → merged plans"
      ]
    },
    "cta": {
      "heading": "Explore the architecture",
      "description": "Understand how the components fit together in detail.",
      "links": {
        "architecture": "Architecture overview",
        "jobs": "Background jobs",
        "desktop": "Desktop app internals",
        "dataModel": "Data model",
        "plans": "Implementation plans"
      }
    }
  },
  "desktopApp": {
    "meta": {
      "title": "Desktop app internals - PlanToCode",
      "description": "How the Tauri desktop shell, Rust command layer, SQLite persistence, and PTY sessions work together."
    },
    "category": "Desktop",
    "date": "2025-09-25",
    "readTime": "14 min",
    "title": "Desktop App Internals",
    "description": "Tauri v2 shell, Rust command layer, PTY sessions, and UI state management.",
    "intro": "The desktop app is a Tauri v2 shell (version 2.9.1) running a React UI. Rust services expose commands for workflows, terminal sessions, and configuration while persisting state locally in SQLite. The capability-based permission model provides fine-grained security controls for filesystem access, HTTP requests, shell execution, and system notifications.",
    "visuals": {
      "shell": {
        "title": "Desktop shell overview",
        "description": "Screenshot showing the plan editor, terminal tabs, and job status sidebar.",
        "imageSrc": "/assets/images/demo-implementation-plans.jpg",
        "imageAlt": "PlanToCode desktop shell",
        "caption": "The desktop app showing the implementation plans panel and sidebar."
      }
    },
    "projectLayout": {
      "heading": "Project layout",
      "description": "The desktop application follows the standard Tauri v2 structure:",
      "items": [
        "desktop/src/: React UI components, hooks, providers, and desktop-specific adapters.",
        "desktop/src-tauri/: Rust core including commands, jobs, repositories, and services.",
        "desktop/src-tauri/src/lib.rs: Application entry point with plugin registration and AppState management.",
        "desktop/src-tauri/src/commands/: 35+ Tauri command handler modules organized by domain.",
        "desktop/src-tauri/src/jobs/: Background job processors, workflow orchestration, and queue management.",
        "desktop/src-tauri/capabilities/: JSON capability definitions for security permissions (default.json, desktop-default.json, plantocode-api.json).",
        "desktop/src-tauri/migrations/: SQLite schema migrations in consolidated_schema.sql."
      ]
    },
    "ui": {
      "heading": "React UI and surface area",
      "description": "The React UI renders the task description editor, plan viewer, and terminal panels:",
      "components": [
        "TaskDescriptionEditor: Multi-line input with voice transcription integration and text improvement popover.",
        "VirtualizedCodeViewer: Monaco-based plan display with syntax highlighting and copy actions.",
        "TerminalSurface: PTY output buffer with connection status, agent attention indicators, and voice input.",
        "SessionProvider: Global state management for active session, file selections, and model preferences.",
        "TextImprovementProvider: Selection listener and popover positioning for inline rewrites.",
        "WorkflowTracker: Real-time progress display for multi-stage workflows."
      ]
    },
    "commands": {
      "heading": "Tauri commands",
      "description": "Commands in desktop/src-tauri/src/commands/ expose Rust functionality to the React UI. Key modules include:",
      "modules": [
        "job_commands.rs: create_job, get_job, cancel_job, get_jobs_for_session, clear_job_history.",
        "workflow_commands.rs: start_file_finder_workflow, get_workflow_status, retry_workflow, pause_workflow, resume_workflow.",
        "terminal_commands.rs: start_terminal_session, attach_terminal_output, write_terminal_input, resize_terminal_session, get_terminal_metadata, graceful_exit_terminal.",
        "session_commands.rs: create_session, get_session, update_session, sync_task_description_history, sync_file_selection_history.",
        "auth0_commands.rs: initiate_login, complete_login, refresh_token, logout, get_user_info.",
        "implementation_plan_commands.rs: generate_implementation_plan, merge_implementation_plans, estimate_tokens.",
        "config_commands.rs: get_runtime_config, get_model_config, get_system_prompts, refresh_config_cache.",
        "settings_commands.rs: get_setting, set_setting, get_project_system_prompt, set_project_system_prompt."
      ]
    },
    "appState": {
      "heading": "AppState management",
      "description": "The Rust core manages application state through Tauri's state system:",
      "structure": "AppState struct in lib.rs holds: config_load_error (Option<String>), HTTP client (reqwest::Client), RuntimeConfig (server URL, onboarding status) behind Mutex, and Auth0State for authentication.",
      "config": "RuntimeConfig contains server_url, onboarding_complete flag, and is updated via set_runtime_config command. ConfigCache stores runtime AI configuration with per-project overrides.",
      "tokens": "TokenManager uses the OS keyring (via keyring crate) to securely store access_token, refresh_token, and jwt with automatic refresh before expiry."
    },
    "jobs": {
      "heading": "Job processors and workflows",
      "description": "Job processing architecture in desktop/src-tauri/src/jobs/:",
      "queue": "queue.rs manages the job queue with in-memory pending jobs and SQLite persistence. Jobs transition through statuses: idle, created, queued, acknowledged_by_worker, preparing, preparing_input, running, generating_stream, processing_stream, completed, failed, canceled.",
      "processors": "processors/ directory contains task-specific processors: ImplementationPlanProcessor (streaming plans), TextImprovementProcessor (inline rewrites), RootFolderSelectionProcessor, RegexFileFilterProcessor, FileRelevanceAssessmentProcessor, ExtendedPathFinderProcessor.",
      "orchestrator": "workflow_orchestrator/ coordinates multi-stage workflows: definition_loader.rs loads JSON workflow definitions, stage_scheduler.rs dispatches stages, payload_builder.rs constructs inputs, event_emitter.rs publishes progress events.",
      "streaming": "processors/generic_llm_stream_processor.rs handles streaming LLM responses, emitting job:stream-progress events and accumulating content in background_jobs.response."
    },
    "persistence": {
      "heading": "Local persistence",
      "description": "SQLite storage in desktop/src-tauri/migrations/consolidated_schema.sql:",
      "tables": [
        "sessions: id (UUID), name, project_directory, project_hash, task_description, included_files, force_excluded_files, model_used, history versions.",
        "background_jobs: id (UUID), session_id (FK), task_type, status, prompt, response, tokens_sent/received, cache_read/write_tokens, actual_cost, metadata (JSON), server_request_id.",
        "terminal_sessions: id, job_id (nullable FK), session_id, status, process_pid, output_log, working_directory, environment_vars, last_output_at.",
        "task_description_history: session_id (FK), description, device_id, sequence_number, version for multi-device sync.",
        "file_selection_history: session_id (FK), included_files, force_excluded_files, device_id, sequence_number.",
        "project_system_prompts: project_hash, task_type, system_prompt for per-project prompt overrides.",
        "key_value_store: key, value (JSON), updated_at for app settings.",
        "error_logs: timestamp, level, error_type, message, context, stack, metadata for client-side error tracking."
      ],
      "repositories": "Repositories in db_utils/ provide typed access: background_job_repository/ (modular with base.rs, worker.rs, metadata.rs, cleanup.rs), session_repository.rs, terminal_repository.rs, settings_repository.rs, error_log_repository.rs."
    },
    "terminal": {
      "heading": "Terminal sessions",
      "description": "PTY terminal implementation:",
      "commands": "terminal_commands.rs manages session lifecycle: create_terminal_session spawns PTY via portable-pty crate, send_terminal_input forwards keystrokes, resize_terminal adjusts dimensions, check_cli_availability verifies tool presence (claude, cursor, codex, gemini).",
      "persistence": "terminal_repository.rs persists sessions with output_log (accumulated terminal output), status (idle/running/completed/failed/agent_requires_attention), exit_code, working_directory. Sessions can be restored after app restart.",
      "attention": "Agent attention detection monitors last_output_at timestamps. Level 1 (30s idle): yellow indicator. Level 2 (2min idle): red indicator with desktop notification."
    },
    "inputStability": {
      "heading": "Task description stability",
      "description": "The task description editor includes safeguards to prevent cursor jumps:",
      "items": [
        "Remote updates are queued while the user is typing and flushed on idle or blur.",
        "Selection state is tracked and restored after React re-renders.",
        "Background writers call sessionActions.updateCurrentSessionFields to coordinate updates.",
        "Multi-device sync uses sequence_number and version fields to resolve conflicts."
      ]
    },
    "plugins": {
      "heading": "Tauri plugins",
      "description": "PlanToCode uses the Tauri v2 plugin ecosystem:",
      "list": [
        "tauri-plugin-http (2.5.2): HTTP client with CSP-aware fetch for API calls.",
        "tauri-plugin-dialog (2.4.2): Native file/folder picker and message dialogs.",
        "tauri-plugin-shell (2.3.3): Shell command execution for external CLI tools.",
        "tauri-plugin-store (2.4.1): Persistent key-value storage for app settings.",
        "tauri-plugin-notification (2.3.0): Desktop notifications for agent attention.",
        "tauri-plugin-updater (2.9.0): In-app updates with signature verification.",
        "tauri-plugin-single-instance (2.3.4): Single instance enforcement.",
        "tauri-plugin-process (2.3.1): Process restart capability."
      ]
    }
  },
  "serverApi": {
    "meta": {
      "title": "Server API and LLM proxy - PlanToCode",
      "description": "Auth, provider routing, model configuration, and WebSocket endpoints used by desktop and mobile clients."
    },
    "category": "Server",
    "date": "2025-09-25",
    "readTime": "12 min",
    "title": "Server API & LLM Proxy",
    "description": "Auth, provider routing, model configuration, billing, and WebSocket endpoints.",
    "intro": "The server is an Actix-Web service written in Rust that provides authentication, model configuration, LLM proxying, and billing. Desktop and mobile clients depend on it for secure provider routing and streaming responses. The server runs on dedicated infrastructure in two regions: Hetzner (EU) at api-eu.plantocode.com and InterServer (US) at api-us.plantocode.com.",
    "visuals": {
      "flow": {
        "title": "Server request flow",
        "description": "Diagram showing clients, API routes, and the LLM proxy.",
        "imageSrc": "/images/docs/provider-routing/routing-map.svg",
        "imageAlt": "Server request flow diagram",
        "caption": "Placeholder for the server request flow."
      }
    },
    "routeOrganization": {
      "heading": "Route organization",
      "description": "Routes are organized in server/src/routes.rs with three configuration functions:",
      "functions": [
        "configure_routes(): JWT-authenticated routes under /api scope. Includes auth, billing, config, providers, models, llm proxy, audio, system-prompts, consent, devices, notifications.",
        "configure_public_auth_routes(): Browser-based auth flow under /auth scope. Includes Auth0 initiate-login, callback, and logged-out routes.",
        "configure_webhook_routes(): Unauthenticated webhook endpoints under /webhooks scope. Currently handles Stripe webhooks."
      ]
    },
    "auth": {
      "heading": "Authentication endpoints",
      "description": "Authentication uses Auth0 with PKCE flow:",
      "routes": [
        "/auth/auth0/initiate-login (GET): Starts OAuth flow with code_challenge, redirects to Auth0.",
        "/auth/auth0/callback (GET): Handles Auth0 redirect, exchanges code for tokens.",
        "/api/auth/userinfo (GET): Returns authenticated user info from Auth0.",
        "/api/auth/logout (POST): Revokes tokens and clears session.",
        "/api/auth/account (DELETE): Account deletion with cascading cleanup.",
        "/api/auth0/refresh-app-token (POST): Refreshes access token using refresh token."
      ],
      "implementation": "Auth handlers in server/src/handlers/auth0_handlers.rs and server/src/handlers/auth/. JWT validation uses services/auth/jwt.rs with JWKS rotation. Revoked tokens tracked in revoked_token_repository.rs."
    },
    "llmProxy": {
      "heading": "LLM proxy and streaming",
      "description": "The LLM proxy normalizes requests across providers and streams responses:",
      "routes": [
        "/api/llm/chat/completions (POST): Main chat completion endpoint. Routes to OpenAI, Anthropic, Google, X.AI, or OpenRouter based on model ID.",
        "/api/llm/video/analyze (POST): Multipart video upload for video analysis (FPS hint). Requires google/* models with video capability.",
        "/api/llm/cancel (POST): Cancels in-flight streaming request by request_id.",
        "/api/llm/status/{request_id} (GET): Returns status of a request (active, completed, cancelled).",
        "/api/audio/transcriptions (POST): Whisper-compatible transcription. Multipart upload with audio file and parameters."
      ],
      "routing": "Router in server/src/handlers/proxy/router.rs selects provider based on model ID prefix (openai/, anthropic/, google/, xai/, openrouter/). Provider-specific handlers in server/src/handlers/proxy/providers/ transform requests and normalize responses.",
      "streaming": "Streaming responses use Server-Sent Events (SSE) via streaming/sse_adapter.rs. The proxy forwards chunks from providers, transforms them to a common format, and tracks token usage in real-time."
    },
    "providers": {
      "heading": "Provider routing",
      "description": "Provider handlers in server/src/handlers/proxy/providers/:",
      "handlers": [
        "openai.rs: OpenAI and OpenAI-compatible APIs (GPT-4, o1, o3).",
        "anthropic.rs: Anthropic Claude models with prompt caching support.",
        "google.rs: Google Gemini models including video analysis capability.",
        "xai.rs: X.AI Grok models.",
        "openrouter.rs: OpenRouter aggregation for model routing."
      ],
      "transformers": "Request/response transformers in server/src/handlers/provider_transformers/ normalize API differences. Each transformer handles: request body format, authentication headers, streaming chunk format, usage extraction, error normalization."
    },
    "config": {
      "heading": "Configuration endpoints",
      "description": "Configuration and model metadata endpoints:",
      "routes": [
        "/api/config/all-configurations (GET): Returns all application configurations including model settings per task type.",
        "/api/config/desktop-runtime-config (GET): Desktop-specific runtime configuration.",
        "/api/config/billing (GET/PUT): Billing configuration management.",
        "/api/providers (GET): List of available LLM providers with capabilities.",
        "/api/providers/with-counts (GET): Providers with model counts.",
        "/api/providers/by-capability/{capability} (GET): Filter providers by capability.",
        "/api/models (GET): All available models with pricing.",
        "/api/models/{id} (GET): Single model details.",
        "/api/models/by-provider/{provider_code} (GET): Models for a specific provider.",
        "/api/models/estimate-cost (POST): Cost estimation for a request.",
        "/api/models/estimate-tokens (POST): Token count estimation.",
        "/api/system-prompts/defaults (GET): Default system prompts by task type."
      ]
    },
    "billing": {
      "heading": "Billing endpoints",
      "description": "Credit-based billing system integrated with Stripe:",
      "routes": [
        "/api/billing/dashboard (GET): User billing dashboard data.",
        "/api/billing/usage-summary (GET): Detailed usage with cost breakdown.",
        "/api/billing/credits/balance (GET): Current credit balance.",
        "/api/billing/credits/details (GET): Credit details including grants and purchases.",
        "/api/billing/credits/unified-history (GET): Transaction history.",
        "/api/billing/checkout/credit-purchase (POST): Create Stripe checkout for credits.",
        "/api/billing/checkout/setup (POST): Create Stripe setup session for payment method.",
        "/api/billing/auto-top-off (GET/PUT): Auto top-off settings management."
      ],
      "implementation": "Billing handlers in server/src/handlers/billing/. Credit service in services/credit_service.rs. Stripe integration via services/stripe_service.rs with webhook handling in webhook_handlers.rs."
    },
    "devices": {
      "heading": "Device management",
      "description": "Device registration and push notifications:",
      "routes": [
        "/api/devices/register (POST): Register desktop device with device_id.",
        "/api/devices/mobile/register (POST): Register mobile device with platform info.",
        "/api/devices/{device_id}/heartbeat (POST): Device heartbeat for presence.",
        "/api/devices/{device_id}/push-token (POST): Save push notification token.",
        "/api/devices/{device_id}/connection-descriptor (GET): WebSocket connection info for device linking.",
        "/api/notifications/job-completed (POST): Send push notification for completed job.",
        "/api/notifications/job-progress (POST): Send progress notification."
      ]
    },
    "websockets": {
      "heading": "WebSocket endpoints",
      "description": "Real-time communication via WebSocket:",
      "endpoints": [
        "/ws/device-link: Relay for desktop-mobile device linking. Handles terminal output streaming, job status updates, and RPC commands between linked devices.",
        "/ws/events: General event stream for real-time updates."
      ],
      "implementation": "Device link relay in server/src/handlers/device_link_ws.rs. Sessions managed by services/relay_session_store.rs with heartbeat monitoring and reconnection support."
    },
    "serverStorage": {
      "heading": "Server-side persistence",
      "description": "PostgreSQL database with repositories in server/src/db/repositories/:",
      "repositories": [
        "user_repository.rs: User accounts linked to Auth0 sub.",
        "customer_billing_repository.rs: Stripe customer and credit state.",
        "credit_transaction_repository.rs: Credit transaction history.",
        "provider_repository.rs: LLM provider configuration.",
        "system_prompts_repository.rs: System prompt templates.",
        "consent_repository.rs: Legal consent tracking.",
        "audit_log_repository.rs: Audit trail for sensitive operations.",
        "revoked_token_repository.rs: JWT revocation list.",
        "api_key_repository.rs: API key management with secure hashing."
      ]
    }
  },
  "backgroundJobs": {
    "meta": {
      "title": "Background jobs - PlanToCode",
      "description": "Job queue architecture, processor types, state machine, and artifact storage for the desktop job engine."
    },
    "category": "Architecture",
    "date": "2025-09-25",
    "readTime": "14 min",
    "title": "Background Jobs",
    "description": "Job queue, processors, state machine, event streaming, and artifact storage.",
    "intro": "All LLM-backed work runs through the background job system in the desktop app. The job queue dispatches work to processors, streams progress events, and persists every prompt and response in SQLite for review and recovery. This architecture enables cancellation, retry, cost tracking, and real-time UI updates.",
    "visuals": {
      "stateMachine": {
        "title": "Job state machine",
        "description": "Diagram showing job status transitions from created through completion or failure.",
        "imageSrc": "/images/docs/background-jobs/state-machine.svg",
        "imageAlt": "Job state machine diagram",
        "caption": "Placeholder for job state machine diagram."
      }
    },
    "jobRecord": {
      "heading": "Job record structure",
      "description": "Each job creates a background_jobs row in SQLite with these fields:",
      "fields": [
        "id (TEXT PRIMARY KEY): UUID for the job.",
        "session_id (TEXT NOT NULL, FK): References sessions.id with CASCADE DELETE.",
        "task_type (TEXT DEFAULT 'unknown'): Processor identifier (e.g., implementation_plan, text_improvement, root_folder_selection).",
        "status (TEXT): Current state with CHECK constraint for valid values.",
        "prompt (TEXT NOT NULL): Full text sent to the LLM, stored for review and debugging.",
        "response (TEXT): LLM output or error message.",
        "error_message (TEXT): Detailed error information on failure.",
        "tokens_sent (INTEGER DEFAULT 0): Input token count from provider response.",
        "tokens_received (INTEGER DEFAULT 0): Output token count.",
        "cache_read_tokens (INTEGER DEFAULT 0): Tokens read from provider cache (Anthropic).",
        "cache_write_tokens (INTEGER DEFAULT 0): Tokens written to cache.",
        "model_used (TEXT): Model identifier used for the request.",
        "actual_cost (REAL): Computed cost based on token usage and model pricing.",
        "metadata (TEXT): JSON with task-specific data, workflow IDs, stage names.",
        "system_prompt_template (TEXT): Template identifier used for the system prompt.",
        "server_request_id (TEXT): Links to server-side usage tracking.",
        "created_at, updated_at, start_time, end_time (INTEGER): Timestamps.",
        "is_finalized (INTEGER DEFAULT 0): Whether final cost/usage has been recorded."
      ]
    },
    "statusValues": {
      "heading": "Status values and transitions",
      "description": "Jobs transition through well-defined statuses tracked in the database:",
      "statuses": [
        "idle: Initial state before processing starts.",
        "created: Job record created, not yet queued.",
        "queued: Added to job queue, waiting for processor.",
        "acknowledged_by_worker: Processor has picked up the job.",
        "preparing: Processor is gathering inputs (files, prompts).",
        "preparing_input: Building the LLM request payload.",
        "running: Request sent to LLM, awaiting response.",
        "generating_stream: Streaming response in progress.",
        "processing_stream: Processing streamed chunks.",
        "completed: Job finished successfully.",
        "completed_by_tag: Completed via stream end tag detection.",
        "failed: Job failed with error_message populated.",
        "canceled: User requested cancellation."
      ],
      "transitions": "Transitions are enforced in background_job_repository/worker.rs. Invalid transitions are rejected. Status changes emit job:status-changed Tauri events."
    },
    "orchestrator": {
      "heading": "Workflow orchestrator",
      "description": "Multi-stage workflows are managed by WorkflowOrchestrator in desktop/src-tauri/src/jobs/workflow_orchestrator/:",
      "modules": [
        "mod.rs: Main orchestrator struct and workflow execution entry point.",
        "definition_loader.rs: Loads workflow JSON definitions (e.g., file_finder_workflow.json) specifying stage order and processor types.",
        "stage_scheduler.rs: Schedules stages sequentially, waits for upstream completion.",
        "stage_job_manager.rs: Creates background_job records for each stage.",
        "payload_builder.rs: Constructs stage inputs from IntermediateData.",
        "data_extraction.rs: Extracts outputs from completed stage jobs.",
        "event_emitter.rs: Publishes workflow-status and workflow-stage Tauri events.",
        "state_updater.rs: Updates workflow state in memory and database.",
        "completion_handler.rs: Handles workflow completion and cleanup.",
        "failure_handler.rs: Manages stage failures and retry decisions.",
        "retry_handler.rs: Implements retry logic with exponential backoff."
      ],
      "dataFlow": "Workflows use WorkflowIntermediateData (defined in workflow_types.rs) to pass outputs between stages: directoryTreeContent, selectedRoots, rawRegexPatterns, locallyFilteredFiles, aiFilteredFiles, verifiedPaths, unverifiedPaths."
    },
    "processors": {
      "heading": "Job processors",
      "description": "Each task_type maps to a processor in desktop/src-tauri/src/jobs/processors/:",
      "implementations": [
        "implementation_plan_processor.rs: Loads selected file contents, builds structured prompt with directory tree, streams XML plan to UI. Uses generic_llm_stream_processor for streaming.",
        "text_improvement_processor.rs: Wraps selection in XML tags, sends non-streaming request, returns improved text. Runs via LlmTaskRunner.",
        "root_folder_selection_processor.rs: Sends directory tree to LLM, parses JSON array response of selected directories.",
        "RegexFileFilterProcessor (in processors/mod.rs): Generates regex patterns from task, applies to git file list, filters binaries.",
        "FileRelevanceAssessmentProcessor: Chunks file contents by token limit, scores relevance in batches, aggregates relevant paths.",
        "ExtendedPathFinderProcessor (path_finder_types.rs): Analyzes imports/dependencies, suggests related files, validates paths exist.",
        "web_search_prompts_generator_processor.rs: Generates research_task XML blocks for deep research.",
        "web_search_executor_processor.rs: Executes research prompts in parallel via server search API.",
        "generic_llm_stream_processor.rs: Reusable streaming processor that handles chunk accumulation, event emission, and response finalization."
      ]
    },
    "events": {
      "heading": "Event streaming",
      "description": "Job progress emits Tauri events consumed by the React UI:",
      "eventTypes": [
        "job:status-changed: Payload {jobId, status, error?}. Emitted on every status transition.",
        "job:stream-progress: Payload {jobId, content, tokensReceived}. Emitted for each streaming chunk.",
        "job:completed: Payload {jobId, response, tokensTotal, cost}. Emitted on successful completion.",
        "workflow-status: Payload {workflowId, status, currentStage?}. Workflow-level status updates.",
        "workflow-stage: Payload {workflowId, stageName, status}. Individual stage status."
      ],
      "reactConsumption": "React components subscribe via useEffect with listen() from @tauri-apps/api/event. WorkflowTracker aggregates workflow events. JobStatusIndicator displays real-time status."
    },
    "retry": {
      "heading": "Retry and cancellation",
      "description": "Job retry and cancellation mechanisms:",
      "retryLogic": "retry_handler.rs manages retry counts and delays. Retries use exponential backoff with configurable max attempts. Retry state stored in job.metadata.retryCount.",
      "cancellation": "Cancellation sets a flag checked between streaming chunks in generic_llm_stream_processor.rs. Server-side cancellation sends /api/llm/cancel with request_id.",
      "cleanup": "workflow_cleanup.rs handles cleanup of incomplete workflows. Stale jobs (running status after app restart) are marked failed."
    },
    "artifacts": {
      "heading": "Artifact storage",
      "description": "Job inputs and outputs are fully persisted for review:",
      "stored": [
        "prompt: Complete LLM prompt including system prompt and user content.",
        "response: Full LLM response text or streaming accumulation.",
        "metadata: JSON with task-specific data (original text for improvements, file lists, workflow context).",
        "system_prompt_template: Identifier linking to server-side prompt template version.",
        "Token counts and cost: Captured from provider response for billing and analysis."
      ],
      "access": "background_job_repository provides queries: get_jobs_for_session, get_job_by_id, get_jobs_by_task_type, get_recent_jobs. Job history displayed in BackgroundJobsSidebar component."
    },
    "costTracking": {
      "heading": "Cost tracking",
      "description": "Per-job cost tracking enables budget management:",
      "calculation": "Cost calculated using model pricing from server/src/models/model_pricing.rs. Formula: (tokens_sent * input_price + tokens_received * output_price) with cache adjustments.",
      "accumulation": "Session-level cost aggregated from background_jobs. UI displays cumulative cost in session header.",
      "serverSync": "server_request_id links desktop jobs to server-side usage records for billing reconciliation."
    },
    "cta": {
      "heading": "See the data model",
      "description": "Understand the SQLite schema that stores jobs, sessions, and terminal session logs.",
      "links": {
        "dataModel": "Data model",
        "runtime": "Runtime walkthrough"
      }
    }
  },
  "buildYourOwn": {
    "meta": {
      "title": "Build your own pipeline - PlanToCode",
      "description": "Conceptual guide for designing file discovery and plan generation workflows similar to PlanToCode."
    },
    "category": "Reference",
    "date": "2025-09-25",
    "readTime": "12 min",
    "title": "Build Your Own Pipeline",
    "description": "Conceptual guide for designing file discovery and plan generation workflows.",
    "intro": "This guide distills the key architectural patterns from PlanToCode into a conceptual blueprint. Whether you want to build a similar system or understand why certain design decisions were made, this document covers the foundational patterns you can reuse or adapt.",
    "visuals": {
      "pipelineMap": {
        "title": "Pipeline architecture map",
        "description": "Overview of the multi-stage pipeline from task input to plan output.",
        "imageSrc": "/images/docs/overview/system-map.svg",
        "imageAlt": "Pipeline architecture diagram",
        "caption": "Placeholder for pipeline architecture diagram."
      }
    },
    "keyPatterns": {
      "heading": "Key Architectural Patterns",
      "jobQueue": {
        "title": "Job Queue Pattern",
        "description": "All LLM-backed operations run as background jobs with status tracking, cancellation support, and retry logic. Jobs are persisted to SQLite so state survives app restarts.",
        "benefits": [
          "Decouples UI responsiveness from LLM latency",
          "Enables cancellation mid-stream",
          "Provides job history of all operations",
          "Supports retry with exponential backoff"
        ],
        "pitfalls": [
          "Job status management adds complexity",
          "Need careful handling of stale jobs on restart",
          "Stream accumulation can consume memory for large responses"
        ]
      },
      "workflowOrchestrator": {
        "title": "Workflow Orchestrator Pattern",
        "description": "Multi-stage workflows are coordinated by an orchestrator that schedules stages sequentially, passes intermediate data between them, and handles failures at any stage.",
        "components": [
          "Definition loader reads workflow JSON specs",
          "Stage scheduler dispatches stages in order",
          "Payload builder constructs inputs from prior outputs",
          "Event emitter publishes progress for UI updates"
        ]
      },
      "repositoryPattern": {
        "title": "Repository Pattern",
        "description": "All persistence goes through typed repositories that abstract SQLite operations. This provides a clean API, enables testing, and centralizes database access.",
        "benefits": [
          "Typed access prevents SQL injection",
          "Repositories can be mocked for testing",
          "Centralized query optimization",
          "Consistent error handling"
        ]
      }
    },
    "steps": {
      "step1": {
        "title": "1. Define your task model",
        "description": "Start by defining what constitutes a task in your system. PlanToCode uses sessions with task descriptions, file selections, and model preferences.",
        "details": "Store task metadata in a dedicated table with versioning for history tracking."
      },
      "step2": {
        "title": "2. Build the job queue",
        "description": "Create a job queue that persists jobs to storage, emits status events, and supports cancellation. Jobs should track prompts, responses, tokens, and cost.",
        "details": "Use a semaphore-based concurrency limiter to control parallel LLM requests."
      },
      "step3": {
        "title": "3. Implement processors",
        "description": "Each job type needs a processor that builds prompts, calls the LLM, and parses responses. Use streaming for long outputs.",
        "details": "Processors should be stateless and receive all context through job parameters."
      },
      "step4": {
        "title": "4. Create the workflow orchestrator",
        "description": "For multi-stage workflows, build an orchestrator that schedules stages, manages intermediate data, and handles failures.",
        "details": "Store workflow definitions as JSON for easy modification without code changes."
      },
      "step5": {
        "title": "5. Add the routing layer",
        "description": "Route LLM requests through a server proxy that normalizes payloads, manages API keys, and tracks usage.",
        "details": "Keep provider credentials on the server; never embed them in desktop clients."
      }
    },
    "architectureDecisions": {
      "heading": "Architecture Decisions",
      "decisions": [
        {
          "question": "Should you use a local database or server-side storage?",
          "recommendation": "Use local SQLite for job state and artifacts. This enables offline operation and fast queries. Sync to server only for billing and cross-device state."
        },
        {
          "question": "Streaming vs non-streaming responses?",
          "recommendation": "Use streaming for plan generation and any output shown progressively. Use non-streaming for short transformations like text improvement."
        },
        {
          "question": "How to handle LLM provider failures?",
          "recommendation": "Implement automatic retry with exponential backoff. Consider a fallback provider like OpenRouter for resilience."
        },
        {
          "question": "Where should file content be loaded?",
          "recommendation": "Load file content in the processor just before building the prompt. This ensures fresh content and avoids storing large blobs in job records."
        }
      ]
    },
    "customizeVsReuse": {
      "heading": "What to Customize vs Reuse",
      "customize": [
        "Prompt templates for your specific use case",
        "File discovery patterns for your project types",
        "Output format (XML, JSON, Markdown)",
        "Model selection per task type"
      ],
      "reuse": [
        "Job queue architecture with status tracking",
        "Workflow orchestrator pattern",
        "Repository pattern for persistence",
        "Streaming response handling",
        "Provider routing and normalization"
      ]
    },
    "commonPitfalls": {
      "heading": "Common Pitfalls to Avoid",
      "items": [
        {
          "pitfall": "Embedding API keys in the client",
          "solution": "Route all LLM requests through a server proxy that manages credentials securely."
        },
        {
          "pitfall": "Not persisting job state",
          "solution": "Store every job with full prompt and response for review and recovery."
        },
        {
          "pitfall": "Blocking UI on LLM calls",
          "solution": "Use background jobs with event-driven UI updates for responsive interfaces."
        },
        {
          "pitfall": "Ignoring token limits",
          "solution": "Estimate tokens before sending and chunk large inputs to stay within context windows."
        },
        {
          "pitfall": "No cancellation support",
          "solution": "Check cancellation flags between streaming chunks and propagate to server."
        }
      ]
    },
    "artifacts": {
      "heading": "Artifacts to Persist",
      "items": [
        "Full prompt sent to the LLM (for debugging and review)",
        "Complete response including streaming accumulation",
        "Token counts from provider response",
        "Computed cost based on model pricing",
        "System prompt template identifier for versioning",
        "Workflow intermediate data for multi-stage flows"
      ]
    },
    "implementationNotes": {
      "heading": "Implementation Notes",
      "items": [
        "Use SQLite with WAL mode for concurrent read/write access",
        "Implement graceful shutdown that marks running jobs as failed",
        "Add health checks for external dependencies before job processing",
        "Log all LLM errors with full context for debugging",
        "Consider caching file content with short TTL to avoid redundant reads"
      ]
    }
  },
  "decisionsTradeoffs": {
    "meta": {
      "title": "Technical decisions and tradeoffs - PlanToCode",
      "description": "Why Tauri, SQLite, and a dedicated LLM proxy were chosen, and what operational tradeoffs they create."
    },
    "category": "Architecture",
    "date": "2025-09-25",
    "readTime": "10 min",
    "title": "Technical Decisions & Tradeoffs",
    "description": "Why Tauri, SQLite, and a dedicated LLM proxy were chosen and what they cost.",
    "intro": "Every architecture involves tradeoffs. This document explains the major technology choices in PlanToCode, what benefits they provide, and what costs or limitations they introduce.",
    "visuals": {
      "tradeoffMatrix": {
        "title": "Tradeoff matrix",
        "description": "Visual comparison of technology choices with their benefits and costs.",
        "imageSrc": "/images/docs/overview/system-map.svg",
        "imageAlt": "Technology tradeoff matrix",
        "caption": "System architecture overview illustrating the technology stack decisions."
      }
    },
    "sections": {
      "tauri": {
        "title": "Tauri v2 for Desktop",
        "description": "Tauri provides a Rust backend with a web-based frontend, enabling cross-platform desktop apps with native performance and small binary sizes.",
        "benefits": [
          "Small binary size (~15MB vs 200MB+ for Electron)",
          "Native Rust performance for file operations and job processing",
          "Capability-based security model with fine-grained permissions",
          "Single codebase for macOS, Windows, and Linux",
          "Access to system APIs (PTY, keychain, notifications)"
        ],
        "tradeoffs": [
          "Smaller ecosystem than Electron",
          "Rust learning curve for backend development",
          "WebView rendering differences across platforms",
          "Less mature tooling for debugging IPC issues"
        ],
        "implementation": "PlanToCode uses Tauri 2.9.1 with ~35 command modules, capability-based permissions, and plugins for shell, dialog, and notifications."
      },
      "sqlite": {
        "title": "SQLite for Local Persistence",
        "description": "SQLite stores all local state including sessions, jobs, terminal output, and settings. This enables offline operation and fast queries.",
        "benefits": [
          "Zero-config embedded database",
          "Fast queries for local data",
          "Enables offline operation",
          "Single file backup and restore",
          "WAL mode for concurrent access"
        ],
        "tradeoffs": [
          "No built-in replication or sync",
          "Large terminal logs can grow the database",
          "Need manual schema migrations",
          "Single-writer limitation (mitigated by WAL)"
        ],
        "implementation": "Schema in consolidated_schema.sql with ~10 tables. Repositories provide typed access with rusqlite."
      },
      "llmProxy": {
        "title": "Dedicated LLM Proxy Server",
        "description": "All LLM requests route through a server proxy that manages API keys, normalizes requests, tracks usage, and handles billing.",
        "benefits": [
          "API keys never leave the server",
          "Single request format for all providers",
          "Centralized usage tracking and billing",
          "Provider failover without client updates",
          "Content filtering and rate limiting"
        ],
        "tradeoffs": [
          "Requires server infrastructure",
          "Adds network latency to requests",
          "Server becomes single point of failure",
          "Need to maintain provider integrations"
        ],
        "implementation": "Actix-Web server with handlers in server/src/handlers/proxy/. Transformers in provider_transformers/ normalize requests."
      },
      "websocket": {
        "title": "WebSocket Relay for Mobile",
        "description": "Desktop and mobile clients connect through a WebSocket relay for device linking, terminal streaming, and job synchronization.",
        "benefits": [
          "Real-time bidirectional communication",
          "No direct P2P networking required",
          "Works across NAT and firewalls",
          "Supports multiple linked devices"
        ],
        "tradeoffs": [
          "Requires persistent server connections",
          "Relay adds latency for large payloads",
          "Connection management complexity",
          "Need reconnection and heartbeat logic"
        ],
        "implementation": "device_link_ws.rs implements the relay with session tracking, heartbeats, and PTC1 binary framing for terminal output."
      }
    },
    "operational": {
      "heading": "Operational Consequences",
      "items": [
        "Tauri: Need separate builds for each platform. CI/CD must cross-compile or use platform-specific runners.",
        "SQLite: Database file grows with terminal output. May need periodic cleanup for long-running instances.",
        "LLM Proxy: Server downtime blocks all LLM operations. Need monitoring and redundancy for production.",
        "WebSocket: Reconnection logic adds complexity. Clients must handle connection drops gracefully."
      ]
    },
    "securityBoundaries": {
      "heading": "Security Boundaries",
      "description": "The architecture creates clear security boundaries that limit exposure:",
      "items": [
        "API keys stored in server vault, never sent to clients",
        "JWT tokens validated on every request with JWKS rotation",
        "Capability-based permissions limit filesystem access",
        "Content sent to LLMs requires explicit user approval",
        "Audit logs track all LLM requests with user context"
      ]
    },
    "whenToReconsider": {
      "heading": "When to Reconsider",
      "description": "These decisions may need revisiting if requirements change significantly:",
      "items": [
        "If browser-only access is required, consider a web-based alternative to Tauri",
        "If multi-device sync is critical, consider server-side job storage",
        "If provider lock-in is acceptable, direct API calls may reduce latency",
        "If mobile is primary, consider native apps instead of device linking"
      ]
    }
  },
  "dataModel": {
    "meta": {
      "title": "Data model and storage - PlanToCode",
      "description": "SQLite entities, relationships, and how state is rehydrated on app restart."
    },
    "category": "Architecture",
    "date": "2025-09-25",
    "readTime": "10 min",
    "title": "Data Model & Storage",
    "description": "SQLite entities, relationships, and how state is rehydrated.",
    "intro": "PlanToCode uses SQLite for all local state. This document describes the schema, entity relationships, and how state is restored when the app restarts.",
    "sqlite": {
      "heading": "SQLite Configuration",
      "description": "The database uses WAL mode for concurrent read/write access. The file is stored in the Tauri app data directory (~/.local/share/plantocode on Linux, ~/Library/Application Support/plantocode on macOS).",
      "migrations": "Schema migrations are consolidated in consolidated_schema.sql. The app checks schema version on startup and runs any pending migrations."
    },
    "entities": {
      "heading": "Core Entities",
      "items": [
        "sessions: Project context with task description, file selections, model preferences, search settings, video/merge prompts, history indexes",
        "background_jobs: LLM-backed operations with prompt, response, tokens, cost, is_finalized flag, error_message",
        "terminal_sessions: PTY sessions with output log, status, process info",
        "task_description_history: Version history for task descriptions",
        "file_selection_history: Version history for file selections",
        "project_system_prompts: Per-project prompt overrides",
        "key_value_store: App settings and configuration",
        "error_logs: Client-side error tracking",
        "migrations: Tracks applied database migrations with timestamps",
        "db_diagnostic_logs: Records database diagnostic issues and errors",
        "app_settings: Application configuration key-value pairs with descriptions"
      ]
    },
    "visuals": {
      "schema": {
        "title": "Entity relationship diagram",
        "description": "Visual representation of the SQLite schema and relationships.",
        "imageSrc": "/images/docs/data-model/schema.svg",
        "imageAlt": "Database schema diagram",
        "caption": "Placeholder for database schema diagram."
      }
    },
    "relationships": {
      "heading": "Entity Relationships",
      "description": "Entities are linked through foreign keys with cascade delete rules:",
      "links": [
        "sessions → background_jobs: One-to-many, cascade delete",
        "background_jobs → terminal_sessions: Optional one-to-one link via job_id",
        "sessions → task_description_history: One-to-many for version tracking",
        "sessions → file_selection_history: One-to-many for version tracking"
      ]
    },
    "repositories": {
      "heading": "Repository Layer",
      "description": "All database access goes through typed repositories in desktop/src-tauri/src/db_utils/:",
      "examples": [
        "background_job_repository/: Modular with base.rs, worker.rs, metadata.rs, cleanup.rs",
        "session_repository.rs: Session CRUD with history management",
        "terminal_repository.rs: Terminal session persistence and output logging",
        "settings_repository.rs: Key-value settings storage"
      ]
    },
    "rehydration": {
      "heading": "State Rehydration",
      "description": "When the app starts, state is restored from SQLite:",
      "sessions": "Active session is loaded with task description, file selections, and model preferences. Recent sessions are available in the session picker."
    },
    "retention": {
      "heading": "Data Retention",
      "description": "Old data is cleaned up based on configurable retention periods:",
      "exports": "Sessions and jobs can be exported for backup before cleanup."
    },
    "cta": {
      "heading": "Explore job processing",
      "description": "See how background jobs use this data model.",
      "links": {
        "jobs": "Background jobs",
        "terminals": "Terminal sessions"
      }
    }
  },
  "serverSetup": {
    "meta": {
      "title": "Dedicated server setup - PlanToCode",
      "description": "Ansible-based infrastructure setup: base hardening, PostgreSQL, Redis, and application deployment."
    },
    "category": "Deployment",
    "date": "2025-09-25",
    "readTime": "12 min",
    "title": "Dedicated Server Setup",
    "description": "Ansible-based infrastructure: base hardening, app deployment, and vault-managed secrets.",
    "intro": "PlanToCode runs on dedicated servers managed through Ansible playbooks. This document covers the infrastructure setup, security hardening, and deployment process.",
    "layers": {
      "heading": "Infrastructure Layers",
      "description": "The infrastructure is organized into layers, each managed by dedicated playbooks:",
      "items": [
        "Base layer: OS hardening, SSH configuration, firewall rules",
        "Database layer: PostgreSQL 17 with replication and backups",
        "Cache layer: Redis 7+ for session state and job queues",
        "Application layer: Rust server binary with systemd service",
        "Proxy layer: Nginx reverse proxy with SSL termination"
      ]
    },
    "servers": {
      "heading": "Server Regions",
      "description": "PlanToCode runs in two regions for geographic redundancy:",
      "items": [
        "EU region: Hetzner dedicated server (api-eu.plantocode.com)",
        "US region: InterServer dedicated server (api-us.plantocode.com)"
      ]
    },
    "requirements": {
      "heading": "Server Requirements",
      "items": [
        "Debian 12 or Ubuntu 22.04 LTS",
        "4+ CPU cores, 16GB+ RAM, 200GB+ SSD",
        "Public IPv4 with firewall access to ports 22, 80, 443",
        "SSH key access for Ansible deployment"
      ]
    },
    "hardening": {
      "heading": "Base Hardening",
      "description": "site-base.yml applies security hardening:",
      "items": [
        "Disable root SSH login, require key authentication",
        "Configure UFW firewall with minimal open ports",
        "Install fail2ban for brute force protection",
        "Enable automatic security updates",
        "Configure audit logging"
      ]
    },
    "postgresql": {
      "heading": "PostgreSQL Setup",
      "description": "PostgreSQL 17 is configured for production use:",
      "items": [
        "Connection pooling with PgBouncer",
        "Automated daily backups with pg_dump",
        "WAL archiving for point-in-time recovery",
        "SSL required for all connections",
        "Row-level security for multi-tenant data"
      ]
    },
    "redis": {
      "heading": "Redis Setup",
      "description": "Redis 7+ handles caching and session state:",
      "items": [
        "Password authentication required",
        "AOF persistence for durability",
        "Memory limits with eviction policy",
        "TLS encryption for connections"
      ]
    },
    "zeroDowntime": {
      "heading": "Zero-Downtime Deployment",
      "description": "Deployments use a rolling update strategy:",
      "items": [
        "New binary uploaded alongside running version",
        "Health check confirms new version is ready",
        "Systemd restarts with graceful shutdown",
        "Load balancer drains connections during switch",
        "Rollback available via previous binary symlink"
      ]
    },
    "quickStart": {
      "heading": "Quick Start",
      "steps": [
        "Clone the infrastructure repository",
        "Copy inventory.example to inventory and configure hosts",
        "Set vault password in .vault_pass",
        "Run: ansible-playbook -i inventory site-base.yml",
        "Run: ansible-playbook -i inventory site-app.yml"
      ]
    },
    "vault": {
      "heading": "Secrets Management",
      "description": "Sensitive configuration uses Ansible Vault:",
      "items": [
        "Database credentials",
        "API keys for LLM providers",
        "SSL certificates and private keys",
        "Auth0 client secrets",
        "Stripe webhook secrets"
      ]
    },
    "operations": {
      "heading": "Common Operations",
      "items": [
        "ansible-playbook -i inventory site-app.yml --tags deploy",
        "ansible-playbook -i inventory site-base.yml --tags backup",
        "ansible-playbook -i inventory site-app.yml --tags rollback",
        "ansible-playbook -i inventory site-base.yml --tags logs"
      ]
    },
    "ssl": {
      "heading": "SSL/TLS Configuration",
      "description": "Let's Encrypt provides free SSL certificates:",
      "items": [
        "Certbot configured with Nginx plugin",
        "Automatic renewal via cron job",
        "HSTS headers enabled",
        "TLS 1.2+ only, modern cipher suite"
      ]
    },
    "security": {
      "heading": "Security Checklist",
      "items": [
        "All default passwords changed",
        "SSH key rotation scheduled",
        "Firewall rules audited",
        "Security updates automated",
        "Backup restoration tested"
      ]
    },
    "recovery": {
      "heading": "Disaster Recovery",
      "description": "Recovery procedures for common failure scenarios:",
      "items": [
        "Database corruption: Restore from latest pg_dump backup",
        "Server failure: Provision new server and run playbooks",
        "SSL expiration: Manual certbot renew --force-renewal",
        "Security breach: Rotate all credentials, audit logs"
      ]
    }
  },
  "tauriV2": {
    "meta": {
      "title": "Tauri v2 development guide - PlanToCode",
      "description": "Project layout, commands, capabilities, and development workflow for Tauri v2."
    },
    "category": "Deployment",
    "date": "2025-09-25",
    "readTime": "10 min",
    "title": "Tauri v2 Development Guide",
    "description": "Project layout, commands, and capability-based permissions for Tauri v2.",
    "intro": "PlanToCode uses Tauri v2 for the desktop application. This guide covers the project structure, command system, capability-based permissions, and development workflow.",
    "projectLayout": {
      "heading": "Project Layout",
      "description": "The desktop application follows standard Tauri v2 conventions:",
      "items": [
        "desktop/src/: React frontend with components, hooks, and providers",
        "desktop/src-tauri/: Rust backend with commands, jobs, and services",
        "desktop/src-tauri/src/lib.rs: Application entry point",
        "desktop/src-tauri/src/commands/: Tauri command handlers (~35 modules)",
        "desktop/src-tauri/capabilities/: Permission definitions",
        "desktop/src-tauri/tauri.conf.json: Tauri configuration"
      ]
    },
    "configuration": {
      "heading": "Tauri Configuration",
      "description": "tauri.conf.json configures the application:",
      "items": [
        "productName, version, identifier for app metadata",
        "build.beforeDevCommand and beforeBuildCommand for frontend",
        "bundle settings for installers (DMG, NSIS, AppImage)",
        "security.csp for Content Security Policy",
        "plugins configuration for official plugins"
      ]
    },
    "capabilities": {
      "heading": "Capability-Based Permissions",
      "description": "Tauri v2 uses capabilities to control what the app can access:",
      "items": [
        "default.json: Base permissions for all windows",
        "desktop-default.json: Desktop-specific permissions",
        "plantocode-api.json: Custom permissions for PlanToCode commands",
        "Permissions grant access to: filesystem, shell, http, dialog, notification"
      ]
    },
    "plugins": {
      "heading": "Tauri Plugins",
      "description": "PlanToCode uses several official Tauri plugins:",
      "items": [
        "tauri-plugin-http: HTTP client for API calls",
        "tauri-plugin-dialog: Native file/folder pickers",
        "tauri-plugin-shell: Shell command execution",
        "tauri-plugin-store: Persistent key-value storage",
        "tauri-plugin-notification: Desktop notifications",
        "tauri-plugin-updater: In-app updates",
        "tauri-plugin-single-instance: Single instance enforcement"
      ]
    },
    "appState": {
      "heading": "Application State",
      "description": "Rust state managed through Tauri's state system:",
      "items": [
        "AppState struct holds shared state",
        "RuntimeConfig for server URLs and feature flags",
        "TokenManager for secure credential storage",
        "ConfigCache for AI model configuration"
      ]
    },
    "commands": {
      "heading": "Creating Commands",
      "description": "Tauri commands expose Rust functions to the frontend:",
      "items": [
        "Use #[tauri::command] attribute on async functions",
        "Return Result<T, String> for error handling",
        "Access state via State<AppState> parameter",
        "Register in lib.rs invoke_handler"
      ]
    },
    "singleInstance": {
      "heading": "Single Instance",
      "description": "The app enforces single instance to prevent data conflicts:",
      "items": [
        "tauri-plugin-single-instance handles detection",
        "Second launch focuses existing window",
        "Deep links forwarded to running instance"
      ]
    },
    "devWorkflow": {
      "heading": "Development Workflow",
      "description": "Common commands for development:",
      "items": [
        "pnpm tauri dev: Start development with hot reload",
        "pnpm tauri build: Build production release",
        "cargo test: Run Rust tests",
        "cargo clippy: Lint Rust code"
      ]
    },
    "mobile": {
      "heading": "Mobile Considerations",
      "description": "Tauri v2 supports mobile, but PlanToCode uses native Swift:",
      "items": [
        "iOS app built with SwiftUI for native experience",
        "Shared API contracts between desktop and mobile",
        "Device linking via WebSocket relay"
      ]
    },
    "distribution": {
      "heading": "Distribution",
      "description": "Build artifacts for each platform:",
      "items": [
        "macOS: .dmg with universal binary (Intel + Apple Silicon)",
        "Windows: NSIS installer and MSIX package",
        "Linux: AppImage for broad compatibility"
      ]
    }
  },
  "distributionMacos": {
    "meta": {
      "title": "macOS distribution - PlanToCode",
      "description": "Code signing, notarization, DMG packaging, and updater configuration for macOS."
    },
    "category": "Deployment",
    "date": "2025-09-25",
    "readTime": "10 min",
    "title": "macOS Distribution",
    "description": "Signing, notarization, DMG packaging, and updater artifacts.",
    "intro": "Distributing on macOS requires code signing, notarization, and proper packaging. This document covers the complete process for PlanToCode.",
    "signing": {
      "heading": "Code Signing",
      "description": "All binaries must be signed with an Apple Developer ID:",
      "items": [
        "Developer ID Application certificate for app signing",
        "Developer ID Installer certificate for PKG signing",
        "Certificates stored in CI secrets, imported to keychain",
        "Hardened runtime enabled for notarization compatibility"
      ]
    },
    "entitlements": {
      "heading": "Entitlements",
      "description": "Required entitlements for PlanToCode features:",
      "items": [
        "com.apple.security.cs.allow-jit",
        "com.apple.security.cs.allow-unsigned-executable-memory",
        "com.apple.security.device.audio-input",
        "com.apple.security.network.client",
        "com.apple.security.files.user-selected.read-write"
      ]
    },
    "build": {
      "heading": "Build Process",
      "description": "Steps to build a signed release:",
      "steps": [
        "Run pnpm tauri build --target universal-apple-darwin",
        "Tauri signs with APPLE_SIGNING_IDENTITY from environment",
        "Universal binary created with lipo for Intel + ARM",
        "DMG packaged with custom background and layout"
      ]
    },
    "universalBinaries": {
      "heading": "Universal Binaries",
      "description": "PlanToCode ships as a universal binary:",
      "items": [
        "Single .app supports both Intel and Apple Silicon",
        "Built with --target universal-apple-darwin",
        "Slightly larger binary but simpler distribution",
        "Native performance on both architectures"
      ]
    },
    "notarization": {
      "heading": "Notarization",
      "description": "Apple notarization is required for Gatekeeper approval:",
      "items": [
        "DMG submitted to Apple notary service",
        "Uses notarytool with App Store Connect credentials",
        "Stapling attaches notarization ticket to DMG",
        "Process takes 1-5 minutes typically"
      ]
    },
    "updater": {
      "heading": "In-App Updates",
      "description": "tauri-plugin-updater handles automatic updates:",
      "items": [
        "Checks update endpoint on launch",
        "Downloads new version in background",
        "Prompts user to restart to apply",
        "Signature verification before installation"
      ]
    },
    "latestJson": {
      "heading": "Update Manifest",
      "description": "latest.json describes available updates:",
      "items": [
        "version: Semantic version string",
        "platforms.darwin-universal: URL and signature",
        "notes: Release notes in markdown",
        "pub_date: ISO 8601 publish timestamp"
      ]
    },
    "pitfalls": {
      "heading": "Common Pitfalls",
      "description": "Issues frequently encountered:",
      "items": [
        "Keychain locked during CI: Unlock before signing",
        "Notarization timeout: Retry with exponential backoff",
        "Invalid signature: Check entitlements match capabilities",
        "Gatekeeper rejection: Verify notarization stapled correctly"
      ]
    },
    "verification": {
      "heading": "Verification Commands",
      "description": "Commands to verify signing and notarization:",
      "items": [
        "codesign -dv --verbose=4 PlanToCode.app",
        "spctl --assess --verbose PlanToCode.app",
        "stapler validate PlanToCode.dmg",
        "xcrun notarytool log <submission-id>"
      ]
    }
  },
  "distributionWindows": {
    "meta": {
      "title": "Windows distribution - PlanToCode",
      "description": "NSIS installer, MSIX packaging, Microsoft Store submission, and code signing for Windows."
    },
    "category": "Deployment",
    "date": "2025-09-25",
    "readTime": "10 min",
    "title": "Windows Distribution & Store",
    "description": "NSIS builds, MSIX packaging, and Microsoft Store submission.",
    "intro": "PlanToCode distributes on Windows through both direct download (NSIS installer) and the Microsoft Store (MSIX package). This document covers both distribution methods.",
    "prereqs": {
      "heading": "Prerequisites",
      "description": "Required tools and certificates:",
      "items": [
        "Code signing certificate (EV or standard)",
        "Windows SDK for signtool",
        "NSIS for installer building",
        "MSIX Packaging Tool for Store submissions"
      ]
    },
    "nsisBuild": {
      "heading": "NSIS Installer",
      "description": "Tauri builds NSIS installers by default:",
      "items": [
        "Custom installer UI with PlanToCode branding",
        "Per-user installation (no admin required)",
        "Start menu and desktop shortcuts",
        "Uninstaller with clean removal"
      ]
    },
    "codeSigning": {
      "heading": "Code Signing",
      "description": "Windows code signing with Authenticode:",
      "items": [
        "Sign with signtool from Windows SDK",
        "Timestamp from trusted TSA server",
        "EV certificate provides SmartScreen reputation",
        "CI uses secrets for certificate and password"
      ]
    },
    "msixPackaging": {
      "heading": "MSIX for Microsoft Store",
      "description": "MSIX provides Store-compatible packaging:",
      "items": [
        "AppxManifest.xml defines capabilities",
        "Virtual filesystem isolation",
        "Automatic updates through Store",
        "Sandboxed execution environment"
      ]
    },
    "msixConfig": {
      "heading": "MSIX Configuration",
      "description": "Key AppxManifest settings:",
      "items": [
        "Identity: Name, Publisher, Version",
        "Capabilities: internetClient, microphone",
        "Visual elements: Tiles, splash screen",
        "File associations and protocol handlers"
      ]
    },
    "msixSteps": {
      "heading": "MSIX Build Steps",
      "description": "Process to create MSIX package:",
      "steps": [
        "Build release with pnpm tauri build",
        "Create AppxManifest.xml with correct identity",
        "Package with MakeAppx.exe",
        "Sign with SignTool",
        "Validate with Windows App Cert Kit"
      ]
    },
    "store": {
      "heading": "Microsoft Store Submission",
      "description": "Store submission process:",
      "items": [
        "Create app in Partner Center",
        "Upload MSIX package",
        "Configure pricing (free with IAP credits)",
        "Submit for certification",
        "Review takes 1-3 business days"
      ]
    },
    "updaterWindows": {
      "heading": "Windows Updates",
      "description": "Update mechanisms for each distribution:",
      "items": [
        "NSIS: tauri-plugin-updater with GitHub releases",
        "MSIX/Store: Automatic through Microsoft Store",
        "Both check for updates on launch"
      ]
    },
    "webview2": {
      "heading": "WebView2 Runtime",
      "description": "Tauri uses WebView2 on Windows:",
      "items": [
        "Bundled WebView2 bootstrapper in installer",
        "Evergreen version auto-updates",
        "Fixed version available for isolation",
        "Windows 10 1803+ required"
      ]
    },
    "troubleshooting": {
      "heading": "Troubleshooting",
      "description": "Common Windows distribution issues:",
      "items": [
        "SmartScreen warning: Use EV certificate or build reputation",
        "Missing WebView2: Ensure bootstrapper runs",
        "Store rejection: Review certification report details",
        "Update failure: Check signature and manifest version"
      ]
    }
  },
  "promptTypes": {
    "meta": {
      "title": "Prompt types and templates - PlanToCode",
      "description": "Catalog of prompt-driven job types and template assembly process."
    },
    "category": "Reference",
    "date": "2025-09-25",
    "readTime": "8 min",
    "title": "Prompt Types & Templates",
    "description": "Catalog of prompt-driven job types and template assembly.",
    "intro": "Every LLM-backed job in PlanToCode uses a structured prompt built from templates. This document catalogs the job types and explains how prompts are assembled.",
    "catalog": {
      "heading": "Job Type Catalog",
      "items": [
        {
          "job": "implementation_plan",
          "title": "Implementation Plan",
          "description": "Generates file-by-file implementation plans with XML structure. Uses streaming for progressive display."
        },
        {
          "job": "implementation_plan_merge",
          "title": "Plan Merge",
          "description": "Combines multiple plans with user instructions. Source plans wrapped in XML tags."
        },
        {
          "job": "text_improvement",
          "title": "Text Improvement",
          "description": "Refines selected text while preserving formatting. Non-streaming for quick results."
        },
        {
          "job": "root_folder_selection",
          "title": "Root Folder Selection",
          "description": "Analyzes directory tree to select relevant project roots. Returns JSON array."
        },
        {
          "job": "regex_file_filter",
          "title": "Regex File Filter",
          "description": "Generates regex patterns for file filtering based on task description."
        },
        {
          "job": "file_relevance_assessment",
          "title": "File Relevance Assessment",
          "description": "Scores file content relevance to task. Processes in batches."
        },
        {
          "job": "extended_path_finder",
          "title": "Extended Path Finder",
          "description": "Discovers related files through imports and dependencies."
        },
        {
          "job": "web_search_prompts",
          "title": "Web Search Prompts",
          "description": "Generates research queries for deep research workflow."
        },
        {
          "job": "video_analysis",
          "title": "Video Analysis",
          "description": "Analyzes screen recordings for UI state and action sequences."
        }
      ]
    },
    "templateStructure": {
      "heading": "Template Structure",
      "description": "Prompts are assembled from system templates and user content:",
      "sampleLabel": "Example template structure:",
      "sample": "<system_prompt>\n  You are an AI assistant that generates implementation plans.\n  [template content from server]\n</system_prompt>\n\n<task>\n  [user's task description]\n</task>\n\n<files>\n  [selected file paths and content]\n</files>\n\n<directory_tree>\n  [project structure]\n</directory_tree>"
    },
    "visuals": {
      "template": {
        "title": "Prompt assembly flow",
        "description": "How templates combine with user content to form complete prompts.",
        "imageSrc": "/images/docs/implementation-plans/structure.svg",
        "imageAlt": "Prompt template assembly diagram",
        "caption": "Placeholder for prompt assembly diagram."
      }
    },
    "assembly": {
      "heading": "Assembly Process",
      "steps": [
        "Processor retrieves template ID from task model config",
        "System prompt template loaded from server cache",
        "User content wrapped in semantic XML tags",
        "Context (files, tree) added based on job type",
        "Complete prompt stored in job record before sending"
      ]
    },
    "serverConfig": {
      "heading": "Server-Side Configuration",
      "description": "Templates and model settings are configured server-side:",
      "fields": "task_model_config defines: default_model, allowed_models, system_prompt_template_id, max_tokens, temperature"
    },
    "tokenGuards": {
      "heading": "Token Guardrails",
      "description": "Each task type has token limits to prevent context overflow:",
      "items": [
        "max_tokens_input: Maximum prompt size",
        "max_tokens_output: Maximum response size",
        "Validation before sending prevents wasted API calls",
        "UI shows token count and warns when approaching limits"
      ]
    },
    "versioning": {
      "heading": "Template Versioning",
      "description": "System prompt templates are versioned for reproducibility. Each job records the template ID used, enabling traceability and comparison of results across template versions."
    },
    "designNotes": {
      "heading": "Design Notes",
      "items": [
        "XML tags provide clear boundaries for LLM parsing",
        "Semantic naming (task, files, context) aids model understanding",
        "Templates avoid instruction injection by sanitizing user input",
        "Streaming jobs use end tags for completion detection"
      ]
    },
    "cta": {
      "heading": "See job processing in action",
      "description": "Learn how these prompts flow through the job system.",
      "links": {
        "jobs": "Background jobs",
        "merge": "Merge instructions"
      }
    }
  },
  "mergeInstructionsDoc": {
    "meta": {
      "title": "Merge instructions - PlanToCode",
      "description": "How multiple plan drafts are merged using XML-tagged source plans and user guidance."
    },
    "category": "Planning",
    "date": "2025-09-25",
    "readTime": "8 min",
    "title": "Merge Instructions",
    "description": "How multiple plan drafts are merged using XML-tagged source plans and user guidance.",
    "intro": "When you have multiple implementation plans that need to be combined, the merge workflow lets you select plans, provide guidance, and generate a unified plan that incorporates the best elements from each source.",
    "processor": {
      "heading": "ImplementationPlanMergeProcessor",
      "description": "The ImplementationPlanMergeProcessor fetches source plan responses, wraps them in XML-tagged sections, and streams the merged result through the LlmTaskRunner.",
      "payload": "Accepts source_job_ids array, optional merge_instructions string, and inherits model configuration from the session.",
      "storage": "Merged plan stored as JobResultData::Text with metadata including source_job_ids, merge_instructions, source_count, merged_at timestamp, and session context."
    },
    "inputs": {
      "heading": "Merge Inputs",
      "items": [
        "Source plans: 2-5 implementation plans selected from the plan list",
        "Merge instructions: User guidance on how to combine (prioritize, resolve conflicts)",
        "Model selection: LLM model for merge generation",
        "Task context: Original task description for reference"
      ]
    },
    "xmlFormat": {
      "heading": "XML-Tagged Source Plans",
      "description": "Source plans are wrapped in XML tags with sequential identifiers:",
      "example": "<task_description>\n  [original task from session]\n</task_description>\n\n<source_plans>\n  <implementation_plan_1>\n    [full plan content from first source]\n  </implementation_plan_1>\n  <implementation_plan_2>\n    [full plan content from second source]\n  </implementation_plan_2>\n</source_plans>\n\n<user_instructions>\n  Prioritize API structure from plan 1.\n  Use database schema from plan 2.\n  Resolve conflicts by preferring newer patterns.\n</user_instructions>"
    },
    "prompt": {
      "heading": "Merge Prompt Structure",
      "description": "The merge prompt includes all context needed for intelligent combination:",
      "sections": [
        "System prompt with merge guidelines",
        "Source plans in XML tags",
        "User's merge instructions",
        "Task description for context",
        "Output format requirements"
      ]
    },
    "visuals": {
      "mergeWalkthrough": {
        "title": "Merge workflow walkthrough",
        "description": "Video showing the complete merge process from selection to output.",
        "videoSrc": "/videos/docs/merge-instructions/walkthrough.mp4",
        "posterSrc": "/images/docs/merge-instructions/flow.svg",
        "caption": "Placeholder for merge walkthrough video."
      },
      "mergeFlow": {
        "title": "Merge instructions flow",
        "description": "Diagram showing multi-model merge workflow with XML-tagged source plans.",
        "imageSrc": "/images/docs/merge-instructions/flow.svg",
        "caption": "Merge flow showing source selection, instruction processing, and output generation"
      }
    },
    "rules": {
      "heading": "Merge Rules",
      "description": "The LLM follows these rules when merging plans:",
      "examples": [
        "Preserve file paths exactly as specified in source plans",
        "Combine non-conflicting changes from all sources",
        "For conflicts, follow explicit user instructions",
        "Maintain consistent code style across merged content",
        "Include provenance comments indicating source plan"
      ]
    },
    "output": {
      "heading": "Merged Output",
      "description": "The merged plan is returned as raw text from the LLM, following the same flexible format as individual plans.",
      "provenance": "Each section includes comments indicating which source plan contributed the content.",
      "metadata": "source_job_ids, merge_instructions, source_count, merged_at timestamp, planTitle, summary, isStructured (false), and sessionName stored in job metadata."
    },
    "ui": {
      "heading": "UI Integration",
      "description": "The Implementation Plans panel supports merge workflow:",
      "audit": "Merged plans link back to source plans for traceability."
    },
    "cta": {
      "heading": "Learn about plan generation",
      "description": "Understand how individual plans are created before merging.",
      "links": {
        "plans": "Implementation plans",
        "runtime": "Runtime walkthrough"
      }
    }
  },
  "meetingIngestionDoc": {
    "meta": {
      "title": "Meeting and recording ingestion - PlanToCode",
      "description": "How recordings are analyzed into task summaries through the video analysis pipeline."
    },
    "category": "Inputs",
    "date": "2025-09-25",
    "readTime": "8 min",
    "title": "Meeting & Recording Ingestion",
    "description": "How recordings become task summaries and planning inputs.",
    "intro": "PlanToCode can analyze meeting recordings and screen captures with the video analysis job. The model is guided by a system prompt that adapts to your goal, whether you are debugging, reviewing UI, or documenting a workflow.",
    "visuals": {
      "ingestionFlow": {
        "title": "Recording ingestion flow",
        "description": "How recordings flow through upload and analysis.",
        "imageSrc": "/images/docs/deep-research/workflow.svg",
        "imageAlt": "Recording ingestion flow diagram",
        "caption": "Placeholder for ingestion flow diagram."
      }
    },
    "inputs": {
      "heading": "Supported Inputs",
      "description": "The ingestion workflow accepts video recordings captured in the app or uploaded from other tools.",
      "items": [
        "Screen recordings captured in the desktop app",
        "Meeting recordings exported from Zoom, Meet, or Teams (video files)",
        "Design walkthroughs or bug reproductions recorded as video",
        "For audio-only notes, use voice transcription"
      ]
    },
    "uploadProcess": {
      "heading": "Upload Process",
      "description": "Recordings are uploaded to the server as multipart form data for analysis.",
      "stepsHeading": "Processing Steps",
      "steps": [
        "Desktop saves the recording locally and calculates duration",
        "Video file and analysis prompt are uploaded to /api/llm/video/analyze",
        "Server stores the file temporarily and routes it to Gemini video models",
        "Long recordings are split into 2-minute chunks by the desktop and processed in parallel",
        "Analysis summary is returned and stored in the job response"
      ]
    },
    "normalization": {
      "heading": "Format Normalization",
      "description": "Recordings are sent mostly as-is. WebM recordings are remuxed to fix container metadata before analysis.",
      "outputs": "No separate transcript or frame artifacts are generated; the output is a text analysis summary."
    },
    "multimodalAnalysis": {
      "heading": "Multimodal Analysis",
      "description": "Recordings are analyzed with {code} video models, which accept video and audio in a single request.",
      "combined": "The default video analysis system prompt adapts the output to your goal rather than forcing a fixed schema."
    },
    "transcription": {
      "heading": "Audio context",
      "description": "Audio is analyzed as part of the video; the app does not generate a standalone transcript.",
      "attribution": "If spoken content is unclear, the model may mark it as partially visible rather than guessing.",
      "featuresHeading": "Audio analysis notes",
      "features": [
        "Narration steers the summary",
        "Spoken intent and errors can be quoted",
        "No diarization or timestamped transcript"
      ]
    },
    "frames": {
      "heading": "Frame rate hint",
      "description": "FPS is a sampling hint sent with the analysis request. For large files the provider may ignore it.",
      "timestamps": "Long recordings can be chunked to keep analysis responsive."
    },
    "structuredExtraction": {
      "heading": "Structured Extraction",
      "description": "The analysis output is freeform and adapts to your prompt. Typical outputs include:",
      "extractedHeading": "Extracted Elements",
      "items": [
        "Bug reproduction steps and observed errors",
        "UI walkthrough notes and navigation paths",
        "Design feedback or UX issues shown on screen",
        "Suggested fixes or follow-up tasks"
      ]
    },
    "artifacts": {
      "heading": "Analysis Artifacts",
      "description": "Video analysis produces artifacts stored with the job:",
      "items": [
        "analysis_summary: Text summary stored in background_jobs.response",
        "job_metadata: durationMs, framerate, videoPath",
        "chunk_info: chunk boundaries for long recordings (when applicable)"
      ]
    },
    "keyFiles": {
      "heading": "Key Source Files",
      "items": [
        "desktop/src/app/components/generate-prompt/_components/video-recording-dialog.tsx",
        "desktop/src/contexts/screen-recording/Provider.tsx",
        "desktop/src-tauri/src/jobs/processors/video_analysis_processor.rs",
        "server/src/handlers/proxy/specialized/video_analysis.rs",
        "server/src/utils/multipart_utils.rs",
        "server/src/clients/google_client.rs"
      ]
    },
    "handoff": {
      "heading": "Planning Handoff",
      "description": "Video analysis summaries can be incorporated into the task description for planning.",
      "pipeline": "The summary can be refined with text_improvement and task_refinement before file discovery."
    },
    "cta": {
      "heading": "Continue to video analysis",
      "description": "Learn more about how video frames are analyzed.",
      "links": {
        "video": "Video analysis",
        "textImprovement": "Text improvement"
      }
    }
  },
  "videoAnalysisDoc": {
    "meta": {
      "title": "Video analysis - PlanToCode",
      "description": "Adaptive analysis of screen recordings with Gemini video models."
    },
    "category": "Inputs",
    "date": "2025-09-25",
    "readTime": "6 min",
    "title": "Video Analysis",
    "description": "Adaptive analysis and prompts for screen recordings.",
    "intro": "Video analysis sends the recording to Gemini video models with a system prompt that adapts to your goal. The output is a text summary, not a frame-by-frame export or separate transcript.",
    "visuals": {
      "frameNotes": {
        "title": "Video analysis pipeline",
        "description": "How recordings flow through the analysis model.",
        "imageSrc": "/assets/images/demo-video-analysis.jpg",
        "imageAlt": "Video analysis interface",
        "caption": "The video analysis interface showing analysis options."
      }
    },
    "apiEndpoint": {
      "heading": "API Endpoint",
      "endpoint": "Video analysis is handled by {code} on the server. The endpoint accepts multipart form data with the video file and analysis parameters.",
      "payloadHeading": "Payload Fields",
      "payloadFields": [
        "video: The video file",
        "model: Model identifier for analysis (google/* required)",
        "prompt: Task description and optional focus prompt (wrapped in <description> and <video_attention_prompt>)",
        "temperature: Sampling temperature from task settings",
        "durationMs: Recording duration in milliseconds",
        "framerate: Sampling hint (0.1-20 from the UI)",
        "systemPrompt: Composed system prompt (server-generated)"
      ]
    },
    "inputs": {
      "heading": "Supported Input Formats",
      "items": [
        "MP4, WebM, MOV, and AVI are common inputs",
        "Large files may be uploaded with the provider File API",
        "Long recordings are chunked by the desktop app before analysis"
      ]
    },
    "sampling": {
      "heading": "Frame rate hint",
      "description": "FPS is a hint for how densely to sample the video. For large files the provider may ignore it; for long recordings the desktop may downsample when chunking.",
      "fps": "Default recorder rate is 5 FPS. Lower rates reduce cost but may miss rapid UI changes.",
      "parametersHeading": "Sampling Parameters",
      "parameters": [
        "framerate: 0.1-20 selection in the UI (provider requests are clamped to 1-20)",
        "chunking: long recordings split into 2-minute segments",
        "audio: include narration when \"Include dictation\" is enabled"
      ]
    },
    "modelRequirements": {
      "heading": "Model Requirements",
      "format": "Video analysis requires Gemini video models. Model identifiers follow {code} format; only {code} models are supported.",
      "reasoning": "The server restricts video analysis to Google Gemini models that accept video inputs."
    },
    "analysis": {
      "heading": "Analysis Process",
      "description": "The model analyzes the full video (and audio if present) and produces a goal-oriented summary.",
      "prompting": "The default system prompt (default_video_analysis) tells the model to adapt to your goal, quote visible text when relevant, and mark unclear content instead of guessing.",
      "promptElementsHeading": "Prompt Elements",
      "promptElements": [
        "Goal alignment: focus on the user's stated intent",
        "Evidence: quote visible errors, logs, or UI text when relevant",
        "Sequence: describe the order of events or steps shown",
        "Next steps: suggest fixes or follow-up tasks"
      ]
    },
    "outputs": {
      "heading": "Analysis Outputs",
      "items": [
        "Analysis summary text tailored to the prompt",
        "Quoted errors or UI text when visible",
        "Workflow notes describing what happened on screen",
        "Suggested fixes or follow-up tasks"
      ]
    },
    "billing": {
      "heading": "Token Usage & Billing",
      "description": "Video analysis usage and cost are tracked per job using provider-reported tokens or duration-based estimates.",
      "tracked": [
        "tokens_sent: Prompt + video tokens",
        "tokens_received: Analysis response tokens",
        "actual_cost: Computed from model pricing"
      ]
    },
    "storage": {
      "heading": "Result Storage",
      "description": "Analysis results are stored in background_jobs.response with task_type \"video_analysis\". Long recordings may include chunk metadata.",
      "reuse": "Results can be incorporated into task descriptions or used directly in the planning workflow."
    },
    "keyFiles": {
      "heading": "Key Source Files",
      "items": [
        "desktop/src/app/components/generate-prompt/_components/video-recording-dialog.tsx",
        "desktop/src/contexts/screen-recording/Provider.tsx",
        "desktop/src-tauri/src/jobs/processors/video_analysis_processor.rs",
        "server/src/handlers/proxy/specialized/video_analysis.rs",
        "server/src/clients/google_client.rs"
      ]
    },
    "integration": {
      "heading": "Integration with Planning",
      "description": "Video analysis summaries can be appended to the task description for context-aware planning.",
      "followup": "Use text_improvement or task_refinement to polish the summary before file discovery."
    },
    "cta": {
      "heading": "See meeting ingestion",
      "description": "Learn more about how video analysis works.",
      "links": {
        "meeting": "Meeting ingestion",
        "runtime": "Runtime walkthrough"
      }
    }
  },
  "mobileIos": {
    "meta": {
      "title": "iOS client architecture - PlanToCode",
      "description": "Swift workflows, Auth0 login flow, and device-link session management for the iOS companion app."
    },
    "category": "Architecture",
    "date": "2025-09-25",
    "readTime": "12 min",
    "title": "iOS Client Architecture",
    "description": "Swift workflows, Auth0 login flow, and device-link session management.",
    "intro": "The PlanToCode iOS app is a companion client that connects to linked desktop sessions. It provides mobile access to terminal output, job status, and voice transcription while maintaining the desktop as the primary planning workspace.",
    "visuals": {
      "app": {
        "title": "iOS app interface",
        "description": "Screenshots of the iOS app showing device linking and terminal view.",
        "imageSrc": "/images/docs/overview/system-map.svg",
        "imageAlt": "PlanToCode iOS app screenshots",
        "caption": "Placeholder for iOS app screenshots."
      }
    },
    "packageStructure": {
      "heading": "Swift Package Structure",
      "description": "The iOS app is organized into Swift packages:",
      "packages": [
        {
          "name": "Core",
          "path": "mobile/ios/Core/",
          "description": "Business logic and API clients",
          "components": [
            "WorkflowManager",
            "APIClient",
            "MobileSessionManager",
            "DeviceLinkClient"
          ]
        },
        {
          "name": "Security",
          "path": "mobile/ios/Security/",
          "description": "Authentication and credential storage",
          "components": [
            "Auth0Manager",
            "KeychainHelper",
            "TokenStore"
          ]
        },
        {
          "name": "VibeUI",
          "path": "mobile/ios/VibeUI/",
          "description": "SwiftUI components and design system",
          "components": [
            "TerminalView",
            "JobListView",
            "SettingsView",
            "DeviceLinkView"
          ]
        }
      ]
    },
    "auth": {
      "heading": "Auth0 PKCE Integration",
      "description": "The iOS app uses Auth0 with PKCE flow for secure authentication:",
      "flow": [
        "User taps Sign In, app generates code verifier and challenge",
        "ASWebAuthenticationSession opens Auth0 login page",
        "User authenticates and Auth0 redirects with authorization code",
        "App exchanges code for tokens using code verifier",
        "Tokens stored securely in iOS Keychain"
      ],
      "tokenManagement": {
        "heading": "Token Management",
        "items": [
          "Access token used for API requests",
          "Refresh token stored for silent renewal",
          "Token refresh triggered before expiry",
          "Logout clears all tokens from Keychain"
        ]
      }
    },
    "deviceLink": {
      "heading": "Device Linking via WebSocket Relay",
      "description": "iOS connects to desktop sessions through the server's WebSocket relay:",
      "protocol": {
        "heading": "Linking Protocol",
        "steps": [
          "Desktop generates link code and displays QR",
          "iOS scans QR or enters code manually",
          "Both connect to /ws/device-link with credentials",
          "Server validates and establishes relay",
          "Bidirectional communication enabled"
        ]
      },
      "messageTypes": {
        "heading": "Message Types",
        "items": [
          "terminal_output: PTY output from desktop terminal",
          "job_status: Background job status updates",
          "session_sync: Session state synchronization",
          "rpc_command: Commands from mobile to desktop"
        ]
      },
      "reconnection": {
        "heading": "Reconnection Handling",
        "description": "The WebSocket connection handles network interruptions with automatic reconnection, exponential backoff, and session state recovery."
      }
    },
    "rpcRouting": {
      "heading": "RPC Command Routing",
      "description": "iOS can send commands to the linked desktop:",
      "commands": {
        "heading": "Supported Commands",
        "items": [
          "send_terminal_input: Send keystrokes to terminal",
          "request_job_status: Get status of specific job",
          "start_voice_transcription: Begin recording on mobile",
          "sync_session: Request full session state"
        ]
      },
      "implementation": {
        "heading": "Implementation",
        "description": "Commands are JSON-RPC messages sent over WebSocket. Desktop validates commands and returns results asynchronously."
      }
    },
    "offlineQueue": {
      "heading": "Offline Action Queue",
      "description": "Actions performed while disconnected are queued for sync:",
      "architecture": {
        "heading": "Queue Architecture",
        "items": [
          "Actions stored in local SQLite database",
          "Queue processed on reconnection",
          "Conflicts resolved with server timestamps",
          "Failed actions reported to user"
        ]
      },
      "supportedActions": {
        "heading": "Supported Offline Actions",
        "items": [
          "Voice transcription recording (stored locally)",
          "Session notes and annotations",
          "Preference changes"
        ]
      }
    },
    "localStorage": {
      "heading": "SQLite Local Storage",
      "description": "iOS uses SQLite for local persistence:",
      "database": {
        "heading": "Database Schema",
        "path": "~/Documents/plantocode.sqlite",
        "tables": [
          "linked_devices: Desktop connections",
          "offline_queue: Pending sync actions",
          "cached_sessions: Recent session data",
          "transcriptions: Local voice recordings"
        ]
      },
      "migrations": {
        "heading": "Migrations",
        "description": "Schema version tracked in user_version pragma. Migrations run on app launch."
      }
    },
    "sessions": {
      "heading": "Mobile Sessions",
      "description": "MobileSessionManager coordinates session state:",
      "lifecycle": [
        "Load last active session on launch",
        "Connect to linked desktop if available",
        "Subscribe to session updates via WebSocket",
        "Cache session data for offline access"
      ]
    },
    "workflows": {
      "heading": "Workflow Entry Points",
      "description": "Key workflows accessible from mobile:",
      "items": [
        "Terminal monitoring: View output, send input",
        "Job status: Track background job progress",
        "Voice capture: Record and transcribe on mobile",
        "Session browsing: Review plans and history"
      ]
    },
    "region": {
      "heading": "Region Settings",
      "description": "iOS respects user region preference for API routing:",
      "implementation": "Region stored in UserDefaults, used to select api-eu.plantocode.com or api-us.plantocode.com for all requests."
    }
  },
  "providerRouting": {
    "meta": {
      "title": "Provider routing and streaming - PlanToCode",
      "description": "How PlanToCode routes LLM requests through a proxy, normalizes responses, and streams tokens to the desktop client."
    },
    "category": "Research & Models",
    "date": "2025-09-24",
    "readTime": "10 min",
    "title": "Provider Routing and Streaming",
    "description": "Routing layer that mediates all external LLM requests with normalization, streaming, and usage tracking.",
    "visuals": {
      "routingMap": {
        "title": "Provider routing map",
        "description": "Diagram of how requests flow from the desktop app to the proxy and out to providers.",
        "imageSrc": "/images/docs/provider-routing/routing-map.svg",
        "imageAlt": "Diagram of provider routing flow from desktop to external providers",
        "caption": "Placeholder for provider routing diagram."
      }
    },
    "cta": {
      "heading": "Continue into model configuration",
      "description": "Model configuration explains how allowed lists and token guardrails are exposed to the UI.",
      "links": {
        "modelConfiguration": "Model configuration",
        "runtimeWalkthrough": "Runtime walkthrough"
      }
    }
  }
}
