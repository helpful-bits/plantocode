{
  "technicalLanding": {
    "title": "PlanToCode: Technical System Walkthrough",
    "description": "A technical walk-through of how PlanToCode is built so you can study the architecture, reuse the patterns, or build a similar planning pipeline. The desktop app relies on external LLM providers configured with your API keys.",
    "walkthroughTitle": "Step-by-step: how the pieces work together",
    "walkthroughDescription": "Follow an end-to-end walkthrough of how the desktop app, Rust/Tauri backend, SQLite database, and LLM orchestration pipeline coordinate tasks and stream results back to the UI.",
    "tags": [
      "Tauri",
      "Rust",
      "SQLite",
      "LLM orchestration"
    ]
  },
  "gallery": {
    "heading": "System Architecture and Implementation Walkthrough",
    "intro": "Each panel below shows the desktop app and backend collaborating: Tauri shell, Rust job processors, SQLite persistence, and model orchestration coordinating planning and execution.",
    "video": {
      "title": "System Walkthrough Video",
      "description": "A short, end-to-end walkthrough of the planning pipeline: task input, file discovery jobs, plan generation, merge instructions, and execution handoff.",
      "bullets": [
        "Tauri shell and React UI surface the workflow",
        "Rust background jobs stream progress updates",
        "SQLite stores sessions, jobs, and logs",
        "LLM plans are generated and merged with structure"
      ]
    },
    "cards": {
      "fileFinder": {
        "title": "File Discovery Pipeline",
        "description": "A four-stage Rust workflow: LLM-assisted root selection, regex filtering, relevance scoring, and extended path finding to build a focused file set.",
        "features": [
          "Root folder selection uses the directory tree and task prompt",
          "Regex filter generates pattern groups and applies git ls-files",
          "Relevance scoring chunks file contents with token estimates",
          "Extended path finder expands context with file + tree data"
        ]
      },
      "fileFinderWorkflow": {
        "title": "Persisted File Context",
        "description": "Each workflow stage writes results to background_jobs so file sets can be reused across sessions and inspected later.",
        "features": [
          "Workflow stages stored as job records",
          "Selected file lists persisted as JSON responses",
          "Session included_files reused across jobs",
          "SQLite history survives restarts"
        ]
      },
      "videoAnalysis": {
        "title": "Video Analysis Ingestion",
        "description": "Screen recordings are sent to the `/api/llm/video/analyze` endpoint with prompts and FPS settings to generate structured summaries.",
        "features": [
          "Multipart upload includes duration_ms and framerate",
          "Model format is provider/model (google/* required)",
          "Usage and cost tracked in server billing logs",
          "Summary stored as job metadata artifacts"
        ]
      },
      "implementationPlans": {
        "title": "Multi-Model Plan Generation",
        "description": "ImplementationPlanProcessor streams plan drafts from full file contents; merge jobs consolidate multiple drafts into one plan.",
        "features": [
          "Plan jobs include selected file contents + directory tree",
          "Structured plan metadata captured per job",
          "Merge prompt uses <source_plans> and <user_instructions>",
          "Final plan stored alongside source drafts"
        ]
      },
      "backgroundTasks": {
        "title": "Background Job Monitoring",
        "description": "Rust job processors stream progress and state transitions to the UI while persisting job history in SQLite.",
        "features": [
          "Created, queued, preparing, running, completed/failed/canceled",
          "Streaming updates via Tauri events",
          "Token usage captured per run",
          "Cancel long-running jobs"
        ]
      },
      "settingsPrompts": {
        "title": "Prompt and Model Configuration",
        "description": "Runtime model settings are fetched from `/api/config/desktop-runtime-config`; prompt overrides are stored in SQLite.",
        "features": [
          "Per-task allowed models and defaults",
          "System prompts served by the server API",
          "Project-level prompt overrides in project_system_prompts",
          "Local key_value_store for runtime preferences"
        ]
      },
      "terminalVoiceRecording": {
        "title": "Workflow Automation Buttons",
        "description": "Copy buttons insert templated prompts with task context for handoff into terminals or external tools.",
        "features": [
          "Templates sourced from task model config",
          "Placeholders resolved against the active plan",
          "Handoff to PTY sessions or clipboard",
          "Actions tied to job metadata for audit"
        ]
      },
      "mergeInstructionsWorkflow": {
        "title": "Plan Merge Instructions",
        "description": "ImplementationPlanMergeProcessor merges multiple plan drafts using XML-tagged source plans and optional instructions.",
        "features": [
          "Source plans pulled by job ID",
          "Merge instructions stored in metadata",
          "File contents + directory tree add context",
          "Merged plan stored alongside inputs"
        ]
      },
      "billingTransactions": {
        "title": "Usage and Cost Ledger",
        "description": "Server-side usage entries and job metadata capture model usage across providers.",
        "features": [
          "Per-job token and cost metadata",
          "Provider-aware usage entries",
          "Billing endpoints expose usage summaries",
          "Audit trail for model spend"
        ]
      }
    },
    "viewFullSize": "View full size",
    "cta": {
      "title": "Explore the architecture in context",
      "description": "Map each UI panel to the underlying processors, then dive deeper into the architecture docs and build guides.",
      "primary": "Architecture overview",
      "secondary": "Runtime walkthrough"
    }
  },
  "governance": {
    "cards": {
      "filePlans": {
        "description": "Implementation plans break down changes by file and operation, making scope and impact explicit.",
        "title": "File-by-file plans with exact paths"
      },
      "handoff": {
        "description": "Approved plans are handed to terminals or external tools with full context and audit logs.",
        "title": "Execution handoff"
      },
      "workflow": {
        "description": "Plans can be revised, annotated, and approved before execution. Every revision is preserved.",
        "title": "Review, edit, approve"
      }
    },
    "subtitle": "Plans are stored as artifacts you can review, edit, and approve before any command is run.",
    "title": "Plan Review and Execution Gates"
  },
  "integrations": {
    "cards": {
      "allIntegrations": {
        "description": "How PTY sessions are created, logged, and resumed across restarts.",
        "link": "Terminal session docs \u2192",
        "title": "Terminal session internals"
      },
      "claudeCode": {
        "description": "Requests are normalized and streamed through the server layer so provider behavior stays consistent.",
        "link": "Architecture notes \u2192",
        "title": "Provider routing layer"
      },
      "cursor": {
        "description": "Export plans to external tools or run them in the integrated terminal with full context and logs.",
        "link": "Implementation plan docs \u2192",
        "title": "Execution handoff"
      }
    },
    "subtitle": "How the system connects to terminals, editors, and model providers.",
    "title": "External interfaces"
  },
  "faq": {
    "items": {
      "q1": {
        "q": "Do I need an external LLM provider?",
        "a": "Yes. PlanToCode orchestrates planning, transcription, and analysis jobs by sending prompts to external LLM providers. You must configure your own API keys for those providers."
      },
      "q10": {
        "q": "Can I edit plans before execution?",
        "a": "Yes. Plans open in a Monaco editor with structured steps and copy helpers. You can revise content and rerun generation without leaving the session."
      },
      "q11": {
        "q": "How do merge instructions work?",
        "a": "Multiple plan drafts can be merged by providing guidance. The merge processor aligns structured steps, resolves conflicts, and stores the merged result alongside the originals."
      },
      "q12": {
        "q": "What guardrails exist before execution?",
        "a": "Token estimates, prompt previews, model allowlists, and terminal health checks surface issues before any command runs. Every job records its inputs and outputs."
      },
      "q13": {
        "q": "What does a typical workflow look like?",
        "a": "Describe the task \u2192 run file discovery \u2192 generate one or more plans \u2192 review or merge \u2192 execute in the integrated terminal or export to another tool. Each step is tracked as a job with stored artifacts."
      },
      "q2": {
        "q": "What gets sent to LLM providers?",
        "a": "Only the prompts you approve plus the files or excerpts you select are sent. Local project state, terminal logs, and plan drafts remain in the SQLite database unless you explicitly export them."
      },
      "q3": {
        "q": "Do plans map to exact files in the repo?",
        "a": "Yes. Plans are structured around explicit file paths and operations (create, modify, delete) so you can review scope before execution."
      },
      "q4": {
        "q": "How is this different from chat-based coding agents?",
        "a": "Chat tools produce a single response. PlanToCode separates planning from execution: file discovery, plan generation, plan review, and terminal execution are tracked as discrete jobs with stored artifacts."
      },
      "q5": {
        "q": "Can I run it without an LLM provider?",
        "a": "Only limited UI flows work without a provider. Planning, transcription, and analysis rely on external LLM APIs, so the app is not useful without configured providers."
      },
      "q6": {
        "q": "Which models are supported?",
        "a": "The allowed models come from your server configuration and per-task settings. You choose which providers and models are available for planning and analysis."
      },
      "q7": {
        "q": "Where is project state stored?",
        "a": "Plans, job history, and terminal output are stored locally in SQLite. You can resume sessions because the job records and artifacts persist across restarts."
      },
      "q8": {
        "q": "How does the integrated terminal work?",
        "a": "Each tab is a PTY session managed by the Rust backend. Output is streamed to the UI and stored in SQLite so you can audit or resume later."
      },
      "q9": {
        "q": "How does it handle large codebases?",
        "a": "File discovery is a multi-stage pipeline that filters and scores candidate paths before any plan generation. Each stage is stored as a background job so you can inspect results."
      }
    },
    "subtitle": "Common questions about the planning pipeline, data flow, and execution handoff.",
    "title": "Workflow questions"
  },
  "hero": {
    "cta": {
      "viewDemo": "View walkthrough",
      "howItWorks": "Runtime walkthrough"
    }
  }
}
