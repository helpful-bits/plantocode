{
  "meta": {
    "title": "Dokumentation - PlanToCode",
    "description": "Erfahren Sie, wie Sie Code-Änderungen mit PlanToCode planen und ausliefern: Dateisuche, Implementierungspläne, Terminal-Sitzungen, Modell-Leitplanken und Sprache."
  },
  "architecture": {
    "meta": {
      "title": "PlanToCode Architekturübersicht",
      "description": "Desktop-, Orchestrierungs- und Persistenzschichten, die Implementierungspläne, Workflows und Terminal-Sitzungen ermöglichen."
    },
    "category": "Architektur",
    "date": "2025-09-19",
    "description": "Wie die Desktop-Shell, Hintergrund-Workflows und gemeinsame Dienste organisiert sind.",
    "frontend": {
      "heading": "Frontend-Oberfläche",
      "providers": "Gemeinsame Provider verwalten Benachrichtigungen, Laufzeitkonfiguration und Plan-Status. Das Implementierungspläne-Panel bewahrt Plan-Metadaten auf, verwaltet die Modal-Sichtbarkeit und fordert Token-Schätzungen oder Prompt-Inhalte bei Bedarf an.",
      "ui": "Die Desktop-Benutzeroberfläche ist mit React-Komponenten aufgebaut. Implementierungsplan-Inhalte werden durch einen Monaco-basierten Viewer angezeigt, der große Pläne virtualisiert, Sprachen erkennt und Kopieraktionen unterstützt, damit Prüfer Plan-Text ohne Leistungsprobleme untersuchen können. Terminal-Sitzungen werden in einer gepufferten Ansicht gerendert, die an die PTY-Ausgabe angebunden ist und Verbindungsstatus-Updates anzeigt."
    },
    "intro": "PlanToCode ist eine Tauri-Desktop-Anwendung mit einem React-Frontend. Die Benutzeroberfläche rendert Implementierungspläne, Terminals und Konfigurationssteuerungen, während das Rust-Backend Befehle für Workflows, Token-Schätzung und persistente Terminal-Sitzungen bereitstellt. Diese Übersicht fasst zusammen, wie diese Teile zusammenwirken.",
    "metaDescription": "Desktop-, Orchestrierungs- und Persistenzschichten, die Implementierungspläne, Workflows und Terminal-Sitzungen ermöglichen.",
    "metaTitle": "PlanToCode Architekturübersicht",
    "ogDescription": "Erfahren Sie, wie das React-Frontend, Tauri-Befehle und Hintergrunddienste in der Desktop-App zusammenarbeiten.",
    "ogTitle": "PlanToCode Architekturübersicht",
    "persistence": {
      "database": "Terminal-Ausgaben und Sitzungs-Metadaten werden in SQLite über das Terminal-Sitzungs-Repository gespeichert. Jeder Datensatz enthält Bezeichner, Zeitstempel, Arbeitsverzeichnisse, Umgebungsvariablen und das akkumulierte Protokoll, sodass Neustarts frühere Ausgaben wiederherstellen können. Dasselbe Repository löst Ereignisse aus, wenn sich der Sitzungsstatus ändert.",
      "heading": "Persistenz und Konfiguration",
      "modelConfig": "Modell-Standardwerte befinden sich in der Anwendungskonfigurationstabelle. Jede Aufgabe definiert ein Standardmodell, eine Liste erlaubter Alternativen, Token-Budgets und optionale Kopier-Button-Voreinstellungen. Die React-Schicht liest diese Einstellungen, um den Modell-Selektor und die Leitplanken zu befüllen."
    },
    "readTime": "7 Min.",
    "tauriCommands": {
      "commands": "Die Rust-Seite der Anwendung stellt Befehle für Workflows, Terminal-Sitzungen und Modell-Werkzeuge bereit. Die Workflow-Befehle starten Hintergrundjobs über den Workflow-Orchestrator, validieren Eingaben und lösen Fortschrittsereignisse aus, während die Dateisuche-Pipeline läuft. Token-Schätzungsbefehle berechnen Prompt-Größen für das aktuell ausgewählte Modell.",
      "heading": "Tauri-Befehle und Dienste",
      "terminal": "Terminal-Befehle verwalten PTY-Prozesse, verfolgen Remote-Clients und überprüfen, ob unterstützte CLI-Binärdateien verfügbar sind, bevor eine Sitzung gestartet wird. Zustandsprüfungen kombinieren den PTY-Status mit Datenbankeinträgen, um zu melden, ob eine Sitzung noch aktiv ist."
    },
    "title": "PlanToCode-Architektur",
    "voicePipeline": {
      "description": "Die Sprachtranskription ist als React-Hook implementiert, der Medienberechtigungen, Mikrofonauswahl und Streaming-Transkriptionsanfragen koordiniert. Der Hook integriert sich mit dem Plan-Terminal und Prompt-Editoren und fügt erkannten Text direkt in die aktive Komponente ein, wobei Benachrichtigungen angezeigt werden, wenn die Transkription fehlschlägt.",
      "heading": "Sprachtranskriptions-Pipeline"
    },
    "server": {
      "heading": "Server-Schicht",
      "description": "Der Server verarbeitet Anbieter-Konfiguration (API-Schlüssel im verschlüsselten Tresor, Ratenlimits, Routing-Regeln für OpenAI, Anthropic, Google), Modell-Routing (Anfrage-Proxying, automatisches Failover, Lastverteilung, Kostenverfolgung pro Benutzer/Projekt), Abrechnung (Abonnementverwaltung, Nutzungsmessung, Kontingentdurchsetzung, Kostenwarnungen) und Web-Such-APIs (Ergebnis-Caching mit 30-Tage/7-Tage-TTL, geografische Einschränkungen, JWT-Authentifizierung)."
    },
    "dataFlows": {
      "heading": "Datenflüsse",
      "description": "Aufgaben, Pläne, Jobs und Sitzungen fließen zwischen Komponenten: (1) Aufgabenverfeinerung: React-UI → TextImprovementPopover → Tauri-Befehl → WorkflowOrchestrator → text_improvement Prompt → SQLite → React-Provider ersetzt Text. (2) Dateisuche: Implementierungspläne-Panel → Tauri-Befehl → 4 sequenzielle Jobs → Fortschrittsereignisse → SQLite → UI-Anzeige. (3) Implementierungspläne: Dateisuche → Plan generieren → Tauri-Befehl → LLM-Streaming → SQLite → Monaco-Viewer → Überprüfung/Genehmigung → Export. (4) Terminal-Ausführung: PTY-Sitzung → SQLite → Befehlsausführung → Ausgabe-Streaming → Sprachtranskriptions-Injektion → Agenten-Aufmerksamkeitserkennung → Audit-Protokolle."
    },
    "visuals": {
      "systemMap": {
        "title": "System map snapshot",
        "description": "This diagram depicts the PlanToCode system architecture as four interconnected layers arranged vertically. Top Layer - Desktop Frontend: A React/Next.js box containing components (Plan Viewer, Terminal Panel, Session Manager) connected via labeled arrows \"invoke()\" and \"listen()\" to the Tauri IPC bridge. Second Layer - Rust Backend: WorkflowOrchestrator (scheduling multi-stage jobs), TerminalSessionManager (PTY lifecycle), and job processors (FileDiscovery, PlanGeneration, TextImprovement, DeepResearch). Third Layer - Persistence: SQLite tables for sessions, background_jobs, and terminal_sessions with read/write arrows. Fourth Layer - External Services: Server routes under /api/llm/* and /api/auth with provider icons (OpenAI, Anthropic, Google, OpenRouter). Data flows run down through the layers; streaming responses and job events flow back up to the UI.",
        "imageSrc": "/images/architecture/system-map.svg",
        "imageAlt": "Diagram showing PlanToCode system map",
        "caption": "Four-layer architecture with data flowing down and events streaming back up."
      }
    },
    "ipcBridge": {
      "heading": "IPC bridge and event streaming",
      "description": "The desktop UI calls Rust commands through the Tauri IPC bridge for workflows, terminal sessions, token estimation, and configuration updates.",
      "details": "Commands are invoked from React and results stream back via event listeners so the UI can update job progress, terminal output, and session state in real time."
    },
    "workflowOrchestrator": {
      "heading": "Workflow orchestrator",
      "description": "Multi-stage workflows (file discovery, research, plan generation) are defined in JSON and executed by the Workflow Orchestrator.",
      "details": "Each stage maps to a Rust processor; intermediate results are persisted to SQLite and forwarded to the next stage with progress events emitted to the UI."
    },
    "stateSync": {
      "heading": "State synchronization",
      "description": "React providers treat SQLite as the source of truth for plans, jobs, and terminal sessions, rehydrating state on startup.",
      "details": "Tauri events update in-memory state as jobs run, while repositories ensure plan history and session output survive restarts."
    },
    "llmRouting": {
      "heading": "LLM routing and provider normalization",
      "description": "Model requests are sent to the server proxy, which routes by provider prefix and normalizes payloads and streaming responses.",
      "details": "The proxy tracks usage, cost, and rate limits per user or project and can fail over across providers when configured."
    }
  },
  "deepResearch": {
    "meta": {
      "title": "Deep Research - PlanToCode",
      "description": "Technische Dokumentation für den Web-Such-Workflow: API-Integration, Abfrageoptimierung, Ergebnisverarbeitung und Integration in Entwicklungs-Workflows."
    },
    "apiIntegration": {
      "heading": "API-Integrationsdetails",
      "pipeline": {
        "description": "Forschungsergebnisse durchlaufen eine standardisierte Verarbeitungs-Pipeline, die aussagekräftige Informationen extrahiert und dabei Formatierung und Kontext bewahrt. Die Pipeline verarbeitet verschiedene Inhaltstypen und synthetisiert Erkenntnisse zu umsetzbaren Einsichten für Entwicklungs-Workflows.",
        "heading": "Inhaltsverarbeitungs-Pipeline"
      },
      "providerConfig": {
        "description": "Das System verwendet KI-Sprachmodelle über OpenRouter, um intelligente Web-Recherchen durchzuführen. Das LLM generiert zielgerichtete Recherche-Abfragen basierend auf Ihrem Aufgabenkontext und synthetisiert Erkenntnisse aus seinen Trainingsdaten und Web-Suchfähigkeiten. Modellauswahl und -konfiguration werden über die Anwendungseinstellungen verwaltet.",
        "heading": "KI-Recherche-Konfiguration"
      }
    },
    "architecture": {
      "description": "Das Deep-Research-System arbeitet als zweistufiger Workflow: (1) WebSearchPromptsGeneration - KI analysiert Ihre Aufgabe und den Projektkontext, um zielgerichtete Recherche-Abfragen zu generieren, und (2) WebSearchExecution - das LLM führt Recherche-Prompts parallel aus und synthetisiert Erkenntnisse. Jede Stufe ist auf Zuverlässigkeit, Kosteneffizienz und kontextuelle Relevanz ausgelegt.",
      "heading": "Architekturübersicht"
    },
    "bestPractices": {
      "examples": {
        "description": "Gängige Integrationsmuster zeigen, wie Web-Suchergebnisse verschiedene Entwicklungsszenarien verbessern, von der Fehlersuche bei spezifischen Fehlern bis zur Implementierung neuer Funktionen mit unbekannten APIs.",
        "heading": "Integrationsbeispiele"
      },
      "heading": "Best Practices und Beispiele",
      "strategies": {
        "description": "Um den Wert der Web-Such-Integration zu maximieren, befolgen Sie diese bewährten Strategien zur Formulierung von Abfragen, Interpretation von Ergebnissen und Integration von Erkenntnissen in Ihren Entwicklungs-Workflow.",
        "heading": "Effektive Suchstrategien",
        "queryFormulation": {
          "constraints": "Plattform- oder Umgebungsbeschränkungen einbeziehen",
          "errors": "Bibliotheksnamen mit spezifischen Fehlermeldungen kombinieren",
          "heading": "Abfrageformulierung",
          "practices": "\"Best Practices\" oder \"empfohlener Ansatz\" für Mustersuchen verwenden",
          "versions": "Spezifische Versionsnummern einbeziehen, wenn relevant"
        },
        "resultEvaluation": {
          "crossReference": "Lösungen aus mehreren Quellen gegenprüfen",
          "dates": "Veröffentlichungsdaten für zeitkritische Informationen prüfen",
          "heading": "Ergebnisbewertung",
          "official": "Offizielle Dokumentation gegenüber Drittanbieterquellen priorisieren",
          "verify": "Code-Beispiele in Ihrer Entwicklungsumgebung verifizieren"
        }
      }
    },
    "category": "Technische Referenz",
    "configuration": {
      "heading": "Konfiguration und Anpassung",
      "preferences": {
        "description": "Das Rechercheverhalten wird durch Modellauswahl und Aufgabeneinstellungen konfiguriert. Wählen Sie Ihr bevorzugtes KI-Modell für Rechercheaufgaben, konfigurieren Sie Timeouts und wählen Sie, welche Dateien für den Kontext einbezogen werden sollen.",
        "filters": "Modellauswahl bestimmt Recherchequalität und Kosten",
        "heading": "Rechercheeinstellungen",
        "limits": "Maximal 12 Recherche-Prompts pro Aufgabe generiert",
        "optionsHeading": "Konfigurierbare Optionen",
        "patterns": "Relevante Projektdateien für besseren Kontext einbeziehen",
        "sources": "Projektverzeichnis- und Dateiauswahl für Kontext",
        "triggers": "Recherche manuell über den Workflow-Befehl starten"
      },
      "projectSettings": {
        "description": "Die Recherchekonfiguration ist sitzungsbewusst. Das System verwendet das Projektverzeichnis und die einbezogenen Dateien der aktuellen Sitzung, um Kontext bereitzustellen. Ausgeschlossene Pfade (wie node_modules, dist) werden automatisch aus dem dem KI gezeigten Verzeichnisbaum gefiltert.",
        "heading": "Projektspezifische Einstellungen"
      }
    },
    "costs": {
      "heading": "Kostenüberlegungen",
      "optimization": {
        "description": "Recherchekosten werden durch intelligente Prompt-Generierung verwaltet - das System begrenzt Recherche-Prompts auf maximal 12 pro Aufgabe. Parallele Ausführung minimiert die Gesamtzeit. Jeder Job verfolgt Token-Nutzung und geschätzte Kosten in seinen Metadaten für volle Transparenz.",
        "heading": "Kostenoptimierung"
      },
      "rateLimiting": {
        "cacheFirst": "Rechercheergebnisse pro Sitzung zwischengespeichert, um redundante Abfragen zu vermeiden",
        "description": "Deep Research verwendet Ihre konfigurierten KI-Credits über OpenRouter. Jede Rechercheaufgabe generiert mehrere parallele LLM-Aufrufe, daher skalieren die Kosten mit der Anzahl der generierten Recherche-Prompts. Das System verfolgt Token-Nutzung und Kosten pro Job für Transparenz.",
        "guidelinesHeading": "Tipps zur Kostenverwaltung",
        "heading": "Nutzung und Kosten",
        "personal": "Token-Nutzung pro Recherche-Job mit detaillierter Kostenaufschlüsselung verfolgt",
        "team": "Kosten über Ihre OpenRouter- oder PlanToCode-Abonnement-Credits verwaltet",
        "throttling": "Job-Metadaten auf Token-Anzahl und geschätzte Kosten überwachen"
      }
    },
    "cta": {
      "description": "Die Deep Research- und Web-Such-Funktionen sind in der PlanToCode-Desktop-Anwendung verfügbar. Laden Sie den Build für Ihre Plattform herunter, um Web-Recherche in Ihren Entwicklungs-Workflow zu integrieren.",
      "heading": "Bereit, Deep Research zu nutzen?",
      "links": {
        "architecture": "Systemarchitektur ansehen",
        "buildYourOwn": "Eigene Integration erstellen"
      }
    },
    "date": "2025-09-20",
    "description": "Wie PlanToCode Web-Suchen durchführt, Ergebnisse verarbeitet und Erkenntnisse in Entwicklungs-Workflows integriert.",
    "devIntegration": {
      "caching": {
        "description": "Rechercheergebnisse werden in Job-Metadaten gespeichert und können über das Job-Details-Panel abgerufen werden. Ergebnisse bleiben für die Dauer der Sitzung bestehen und können bei der Erstellung von Implementierungsplänen oder Codierungsentscheidungen referenziert werden.",
        "heading": "Ergebnisspeicherung"
      },
      "contextAware": {
        "description": "Rechercheanfragen werden automatisch mit Kontext aus Ihrer aktuellen Sitzung erweitert. Das System bezieht den Verzeichnisbaum und ausgewählte Dateiinhalte Ihres Projekts in die Prompt-Generierungsphase ein, sodass die KI Rechercheabfragen formulieren kann, die spezifisch für Ihre Codebasis sind.",
        "heading": "Kontextbewusste Recherche"
      },
      "heading": "Integration in Entwicklungs-Workflows",
      "resultIntegration": {
        "description": "Rechercheergebnisse können zur Information von Implementierungsplänen verwendet werden. Wenn Rechercheaufgaben abgeschlossen sind, werden Erkenntnisse als research_finding-Tags formatiert, die in nachfolgende Planungsaufgaben einbezogen werden können, um sicherzustellen, dass Ihre Implementierung von aktuellen Best Practices und genauer Dokumentation geleitet wird.",
        "heading": "Ergebnisintegration"
      }
    },
    "intro": "Die Deep Research-Funktion ermöglicht PlanToCode, intelligente KI-gestützte Recherchen durchzuführen, relevante Informationen zu sammeln und Erkenntnisse direkt in Entwicklungs-Workflows zu integrieren. Dieses System verwendet große Sprachmodelle, um zielgerichtete Rechercheabfragen basierend auf Ihrem Projektkontext zu generieren, parallele Rechercheaufgaben auszuführen und umsetzbare Erkenntnisse zu synthetisieren, um Code-Generierung und Problemlösungsfähigkeiten zu verbessern.",
    "metaDescription": "Technische Dokumentation für den Web-Such-Workflow: API-Integration, Abfrageoptimierung, Ergebnisverarbeitung und Integration in Entwicklungs-Workflows.",
    "metaTitle": "Deep Research - PlanToCode",
    "ogDescription": "Verstehen Sie, wie die Web-Suche in PlanToCode funktioniert: von der Abfragegenerierung bis zur Ergebnisverarbeitung und Integration in Entwicklungs-Workflows.",
    "ogTitle": "Deep Research - PlanToCode",
    "readTime": "8 Min.",
    "title": "Deep Research & Web-Suche",
    "troubleshooting": {
      "commonIssues": {
        "description": "Die meisten Rechercheprobleme stammen von LLM-API-Konnektivität, unzureichenden Credits oder zu breiten Prompts. Das System bietet klare Fehlermeldungen und Job-Status-Verfolgung zur Fehlerbehebung.",
        "geographic": "Modellverfügbarkeit",
        "geographicSolution": "Einige Modelle können regionale Einschränkungen über OpenRouter haben",
        "heading": "Häufige Probleme",
        "noResults": "Keine Recherche-Prompts generiert",
        "noResultsSolution": "Spezifischere Aufgabenbeschreibungen bereitstellen oder relevante Dateien für Kontext einbeziehen",
        "rateLimit": "API-Fehler",
        "rateLimitSolution": "OpenRouter-API-Status und Credit-Guthaben prüfen"
      },
      "heading": "Fehlerbehebung und Support",
      "performance": {
        "description": "Für optimale Leistung geben Sie klare und spezifische Aufgabenbeschreibungen an. Beziehen Sie relevante Projektdateien ein, um der KI besseren Kontext zu geben. Das System führt Recherche-Prompts parallel aus, um die Gesamtausführungszeit zu minimieren.",
        "heading": "Leistungsoptimierung"
      }
    },
    "workflow": {
      "execution": {
        "blogs": "Best Practices und Implementierungsmuster",
        "description": "Recherche-Prompts werden parallel von KI-Sprachmodellen ausgeführt. Jeder Prompt wird unabhängig verarbeitet, sodass das System Informationen zu mehreren Aspekten Ihrer Aufgabe gleichzeitig sammeln kann. Ergebnisse werden in strukturierte Erkenntnisse mit Titeln und umsetzbaren Einsichten synthetisiert.",
        "documentation": "API-Dokumentation und technische Spezifikationen",
        "forums": "Fehlerbehebung und Problemlösungsansätze",
        "github": "Code-Beispiele und Implementierungsmuster",
        "heading": "Rechercheausführung",
        "releases": "Versionskompatibilität und Migrationsleitfäden",
        "sourcesHeading": "Recherche-Schwerpunktbereiche"
      },
      "heading": "Recherche-Workflow-Phasen",
      "processing": {
        "deduplication": "Erkenntnisse über mehrere Recherche-Prompts hinweg konsolidiert",
        "description": "Rechercheergebnisse werden im JSON-Format mit Titeln und detaillierten Erkenntnissen strukturiert. Das System aggregiert Ergebnisse aus parallelen Rechercheaufgaben, verfolgt Erfolgs- und Fehleranzahlen und bietet eine Zusammenfassung der Rechercheergebnisse. Ergebnisse werden in Job-Metadaten für einfachen Zugriff gespeichert.",
        "extraction": "Wichtige Erkenntnisse extrahiert und für die Integration formatiert",
        "heading": "Ergebnisverarbeitung & Synthese",
        "scoring": "Ergebnisse nach Recherchethema und Relevanz organisiert",
        "snippets": "Umsetzbare Einsichten und Empfehlungen hervorgehoben",
        "stepsHeading": "Verarbeitungsschritte",
        "timestamp": "Rechercheausführung mit Zeitmetriken verfolgt"
      },
      "queryGeneration": {
        "api": "API-Dokumentation und bibliotheksspezifische Recherche",
        "compatibility": "Versionskompatibilität und Migrationspfade",
        "description": "Recherche-Prompts werden automatisch von KI basierend auf Ihrer Aufgabenbeschreibung, dem Projektkontext und einbezogenen Dateien generiert. Das System analysiert Ihre Codebasis-Struktur über Verzeichnisbaum und Dateiinhalte, um zielgerichtete Rechercheabfragen zu formulieren. Bis zu 12 fokussierte Recherche-Prompts werden pro Aufgabe generiert.",
        "errors": "Fehlerbehebung und Debugging-Ansätze",
        "heading": "Prompt-Generierung",
        "practices": "Best Practices und empfohlene Muster",
        "security": "Sicherheitsüberlegungen und Schwachstellenbewusstsein",
        "typesHeading": "Recherchethemen"
      }
    },
    "visuals": {
      "pipeline": {
        "title": "Deep Research Pipeline",
        "description": "Der zweistufige Workflow: Prompt-Generierung und parallele Rechercheausführung.",
        "imageSrc": "/images/docs/deep-research/workflow.svg",
        "imageAlt": "Deep Research Pipeline-Diagramm mit Prompt-Generierung und Ausführungsphasen",
        "caption": "Deep Research Workflow mit Prompt-Generierung und parallelen Ausführungsphasen"
      },
      "workflow": {
        "title": "Deep Research Workflow",
        "description": "Der zweistufige Workflow: Prompt-Generierung und parallele Rechercheausführung.",
        "imageSrc": "/images/docs/deep-research/workflow.svg",
        "caption": "Deep Research Workflow mit allen Verarbeitungsphasen"
      }
    }
  },
  "fileDiscovery": {
    "meta": {
      "title": "File discovery workflow - PlanToCode",
      "description": "Comprehensive technical guide to the 4-stage AI workflow that identifies and filters relevant files for task execution."
    },
    "apiUsage": {
      "heading": "API Usage Examples",
      "monitoring": "Monitoring Progress",
      "retrieving": "Retrieving Results",
      "starting": "Starting a Workflow"
    },
    "architecture": {
      "caching": "Intermediate results are persisted in SQLite job records for reuse and debugging.",
      "costTracking": "Cost tracking and timeout management for AI operations",
      "distributed": "The system uses a distributed job architecture where each stage runs as an independent background job, enabling cancellation, retry logic, and detailed progress tracking. Real-time events are published throughout execution to provide immediate feedback to the user interface.",
      "errorHandling": "Comprehensive error handling with automatic retry mechanisms",
      "eventDriven": "Event-driven progress reporting with WebSocket-like updates",
      "featuresHeading": "Key Architecture Features:",
      "gitIntegration": "Git integration with fallback to directory traversal",
      "heading": "Workflow Architecture",
      "overview": "The workflow operates as an orchestrated background job system with four distinct stages that execute sequentially. Each stage builds upon the previous stage's output, progressively refining the file selection based on task requirements."
    },
    "category": "Technical Guide",
    "configuration": {
      "exclusion": {
        "description": "Define directories and file patterns to exclude from the discovery process.",
        "heading": "Exclusion Patterns"
      },
      "heading": "Configuration Options",
      "retry": {
        "description": "Set maximum retry attempts for failed stages with exponential backoff.",
        "heading": "Retry Configuration"
      },
      "timeout": {
        "description": "Configure maximum execution time for the entire workflow or individual stages to prevent indefinite hanging.",
        "heading": "Timeout Management"
      },
      "workflowConfig": "Workflow Configuration"
    },
    "cta": {
      "description": "The file discovery workflow runs inside the desktop client alongside implementation planning and terminal sessions.",
      "heading": "Need the desktop app?",
      "links": {
        "architecture": "Learn about architecture",
        "buildYourOwn": "Build your own pipeline"
      }
    },
    "date": "2025-09-21",
    "description": "Comprehensive technical guide to the 4-stage AI workflow that identifies and filters relevant files for task execution.",
    "errorHandling": {
      "commonIssues": {
        "binaryDetection": "Binary file detection: Uses both extension-based and content-based binary detection",
        "gitNotFound": "Git repository not found: Falls back to directory traversal with standard exclusions",
        "heading": "Common Issues",
        "networkTimeout": "Network timeouts: Automatic retry with exponential backoff for transient failures",
        "tokenLimit": "Token limit exceeded: Implements intelligent batching and provides clear error messages"
      },
      "debugging": {
        "description": "The workflow provides comprehensive logging, performance metrics export, and detailed error context including stage information, retry attempts, and intermediate data for troubleshooting.",
        "heading": "Debugging Tools"
      },
      "errorCategories": {
        "billing": "Billing Errors: Insufficient credits or payment failures with actionable guidance",
        "heading": "Error Categories",
        "system": "System Errors: File system access, git command failures, or memory constraints",
        "validation": "Validation Errors: Invalid session ID, missing task description, or invalid project directory",
        "workflow": "Workflow Errors: Stage-specific failures with detailed context and retry suggestions"
      },
      "heading": "Error Handling & Troubleshooting"
    },
    "integration": {
      "desktop": {
        "description": "The workflow integrates seamlessly with the desktop application through Tauri commands, providing native file system access and event-driven updates via the WorkflowTracker class.",
        "heading": "Desktop Application"
      },
      "heading": "Integration Patterns",
      "implementationPlans": {
        "description": "Selected files are automatically fed into the Implementation Plans panel, ensuring that plan generation uses the same optimized file context without requiring re-execution of the discovery workflow.",
        "heading": "Implementation Plans Integration"
      },
      "sessionManagement": {
        "description": "Selected files and task history persist per session so follow-up actions can reuse the same context without rerunning discovery.",
        "heading": "Session Management"
      }
    },
    "intro": "PlanToCode identifies the right files before you plan or run commands. The 4-stage workflow narrows scope and keeps context tight.",
    "metaDescription": "Comprehensive technical guide to the 4-stage AI workflow that identifies and filters relevant files for task execution.",
    "metaTitle": "File discovery workflow - PlanToCode",
    "ogDescription": "Technical documentation for the multi-stage file discovery workflow architecture.",
    "ogTitle": "File discovery workflow - PlanToCode",
    "performance": {
      "costOptimization": {
        "description": "AI stages track actual costs from API responses, implement intelligent batching to minimize token usage, and provide cost estimates before execution to help manage expenses.",
        "heading": "Cost Optimization"
      },
      "heading": "Performance Considerations",
      "memory": {
        "description": "The workflow uses token-aware chunking, streaming responses, and cleanup of temporary data to manage memory. There is no fixed file batch size.",
        "heading": "Memory Management"
      },
      "monitoring": {
        "description": "Built-in performance tracking monitors execution times, memory usage, throughput metrics, and provides recommendations for optimization based on historical data analysis.",
        "heading": "Performance Monitoring"
      }
    },
    "readTime": "12 min",
    "stages": {
      "heading": "4-Stage Workflow Process",
      "stage1": {
        "description": "Uses AI to intelligently select the most relevant root directories from a list of candidate paths based on the task description. The LLM analyzes the primary project directory and candidate roots to determine which directories are most likely to contain files relevant to the task.",
        "heading": "Stage 1: Root Folder Selection",
        "technical": "Technical Details: Receives candidate root directories (up to depth 2) and the task description. The LLM evaluates each path against the task context and returns a filtered list of root directories that will be searched in subsequent stages.",
        "inputOutput": "Input/Output: Receives candidate_roots array and task_description. Returns root_directories array containing the AI-selected directories most relevant to the task."
      },
      "stage2": {
        "binaryDetection": "Binary Detection: Filters out files with binary extensions (.jpg, .png, .pdf, .exe, etc.) and uses content analysis to detect binary files by null bytes and non-printable character ratios.",
        "description": "Uses AI to generate intelligent regex pattern groups based on the task description and directory structure. Each pattern group can include path patterns (positive and negative) and content patterns. The processor then applies these patterns to filter files from each selected root directory.",
        "gitIntegration": "Git Integration: Finds the git repository root for each selected directory and uses git_utils to get all non-ignored files, respecting .gitignore rules while including both tracked and untracked files.",
        "heading": "Stage 2: Regex File Filter",
        "technical": "Technical Details: Generates a directory tree for each root, calls the LLM to produce patternGroups with path_pattern, content_pattern, and negative_path_pattern fields. Uses fancy-regex for lookahead/lookbehind support. Processes roots in parallel with configurable concurrency."
      },
      "stage3": {
        "aiProcessing": "AI Processing: Uses large language models to evaluate file content against task requirements, with intelligent chunking based on actual file sizes and token estimates to manage context windows efficiently.",
        "description": "Employs AI models to analyze file content and assess relevance to the specific task description. This stage performs deep content analysis by reading file contents and having the LLM identify which files are most relevant to the task.",
        "heading": "Stage 3: AI File Relevance Assessment",
        "technical": "Technical Details: Estimates tokens per file using file-type-aware heuristics (code ~3 chars/token, structured data ~5 chars/token). Creates content-aware chunks to stay under the 90k token threshold. Processes chunks in parallel with streaming to avoid timeouts. Validates all LLM-suggested paths against the filesystem."
      },
      "stage4": {
        "description": "Discovers additional relevant files by providing the LLM with the previously identified files and their contents, along with the directory tree. The AI analyzes imports, dependencies, and project structure to find related files that enhance the context for the task.",
        "heading": "Stage 4: Extended Path Finder",
        "relationship": "Relationship Analysis: Reads content of all previously identified files and provides it to the LLM alongside the directory tree (scoped to selected roots if available). The AI identifies additional files based on imports, references, and structural relationships.",
        "technical": "Technical Details: Generates a combined directory tree for selected root directories. Reads content of all initial_paths files. Uses streaming LLM calls to avoid Cloudflare timeouts. Validates discovered paths against the filesystem and normalizes to relative paths within the project."
      }
    },
    "stateManagement": {
      "eventDriven": {
        "description": "The system publishes real-time events for workflow status changes, stage completions, and error conditions. These events enable responsive user interfaces and integration with external monitoring systems.",
        "heading": "Event-Driven Updates"
      },
      "heading": "Workflow State Management",
      "intermediateData": {
        "description": "Each stage stores its output in a structured intermediate data format, including directory tree content, regex patterns, filtered file lists results. This data is accessible for debugging and can be used to resume workflows from specific stages.",
        "heading": "Intermediate Data Storage"
      },
      "transitions": {
        "description": "The workflow progresses through clearly defined states: Created → Running → Paused (optional) → Completed/Failed/Canceled. Each state transition publishes events that can be monitored for real-time updates.",
        "heading": "State Transitions"
      }
    },
    "visuals": {
      "pipeline": {
        "title": "File discovery pipeline",
        "description": "The 4-stage workflow: root folder selection, regex filtering, AI relevance assessment, and extended path discovery.",
        "imageSrc": "/images/docs/file-discovery/pipeline.svg",
        "caption": "File discovery pipeline showing all 4 stages",
        "imageAlt": "Diagram showing the 4-stage file discovery workflow: Root Folder Selection, Regex File Filter, AI File Relevance Assessment, and Extended Path Finder"
      }
    },
    "title": "File Discovery Workflow",
    "sqliteStorage": {
      "heading": "SQLite Storage",
      "description": "All workflow state, intermediate results, and job metadata are persisted in SQLite. Each stage stores its output in the background_jobs table, enabling workflow resumption and debugging. The job records include token usage, cost tracking, and system prompt templates for each AI stage."
    }
  },
  "hub": {
    "ctaDescription": "Laden Sie PlanToCode herunter, um auf den Implementierungsplaner, Modell-Leitplanken, Terminal-Sitzungen und Transkriptionsfunktionen zuzugreifen, die in dieser Dokumentation beschrieben werden.",
    "ctaHeading": "Bereit, diese Workflows auszuprobieren?",
    "ctaLinks": {
      "overview": "Mit Übersicht starten",
      "runtime": "Laufzeit-Walkthrough"
    },
    "description": "Erfahren Sie, wie Sie Code-Änderungen mit PlanToCode planen und ausliefern: Dateisuche, Implementierungspläne, Terminal-Sitzungen, Modell-Leitplanken und Sprache.",
    "exploreHeading": "Dokumentation erkunden",
    "learnMore": "Mehr erfahren",
    "searchAriaLabel": "Dokumentation durchsuchen",
    "searchPlaceholder": "Dokumentation durchsuchen...",
    "searchShortcut": "⌘K",
    "title": "PlanToCode-Dokumentation"
  },
  "onThisPage": {
    "title": "Auf dieser Seite"
  },
  "sidebar": {
    "title": "Dokumentation"
  },
  "sections": {
    "architecture": {
      "title": "Architektur & Interna"
    },
    "inputs": {
      "title": "Eingaben & Erfassung"
    },
    "planning": {
      "title": "Planungs-Pipeline"
    },
    "execution": {
      "title": "Ausführung & Automatisierung"
    },
    "research": {
      "title": "Recherche & Modelle"
    },
    "platform": {
      "title": "Build & Deployment"
    }
  },
  "items": {
    "overview": {
      "title": "Systemübersicht",
      "description": "Starten Sie hier: was das System tut, wie die Kernschleife funktioniert und wo sich jede Komponente befindet."
    },
    "runtime-walkthrough": {
      "title": "Laufzeit-Walkthrough",
      "description": "End-to-End-Zeitlinie dessen, was von der Aufgabeneingabe bis zur Ausführung passiert."
    },
    "architecture": {
      "title": "Systemarchitektur",
      "description": "Wie Desktop-Shell, Rust-Dienste, Server-APIs und Persistenzschichten zusammenpassen."
    },
    "desktop-app": {
      "title": "Desktop-App-Interna",
      "description": "Tauri v2 Shell, Rust-Befehlsschicht, PTY-Sitzungen und UI-Zustandsverwaltung."
    },
    "server-api": {
      "title": "Server-API & LLM-Proxy",
      "description": "Authentifizierung, Anbieter-Routing, Modellkonfiguration und WebSocket-Endpunkte."
    },
    "mobile-ios": {
      "title": "iOS-Client-Architektur",
      "description": "Swift-Workflows, Auth0-Anmeldeablauf und Geräteverbindungs-Sitzungsverwaltung."
    },
    "background-jobs": {
      "title": "Hintergrundjobs & Orchestrierung",
      "description": "Job-Datensätze, Workflow-Orchestrierung, Prozessoren und Ereignis-Streaming."
    },
    "data-model": {
      "title": "Datenmodell & Speicherung",
      "description": "SQLite-Entitäten, Beziehungen und wie Zustand rehydriert wird."
    },
    "decisions-tradeoffs": {
      "title": "Technische Entscheidungen & Kompromisse",
      "description": "Warum Tauri, SQLite und ein dedizierter LLM-Proxy gewählt wurden und was sie kosten."
    },
    "build-your-own": {
      "title": "Eigene Pipeline erstellen",
      "description": "Konzeptioneller Leitfaden für das Design von Dateisuche- und Plan-Generierungs-Workflows."
    },
    "meeting-ingestion": {
      "title": "Meeting- & Aufnahme-Erfassung",
      "description": "Wie Aufnahmen zu strukturierten Aufgabeneingaben und Artefakten werden."
    },
    "video-analysis": {
      "title": "Video-Analyse",
      "description": "Frame-Sampling, Prompts und Analyse-Artefakte aus Aufnahmen."
    },
    "voice-transcription": {
      "title": "Sprachtranskription",
      "description": "Aufnahme-Lebenszyklus, projektbezogene Einstellungen und Geräteverwaltung."
    },
    "text-improvement": {
      "title": "Textverbesserung",
      "description": "Auswahl-Popover, Job-Warteschlange und Integrationen zur Prompt-Bereinigung."
    },
    "file-discovery": {
      "title": "Dateisuche-Workflow",
      "description": "Hintergrund-Workflow, der relevante Pfade für jede Aufgabe sammelt."
    },
    "implementation-plans": {
      "title": "Implementierungspläne",
      "description": "Wie Pläne in den Monaco-Viewer streamen und mit der Plan-Historie verknüpft bleiben."
    },
    "merge-instructions": {
      "title": "Merge-Anweisungen",
      "description": "Wie mehrere Plan-Entwürfe mit XML-getaggten Quellplänen und Benutzerführung zusammengeführt werden."
    },
    "prompt-types": {
      "title": "Prompt-Typen & Vorlagen",
      "description": "Katalog von Prompt-gesteuerten Job-Typen und Vorlagen-Zusammenstellung."
    },
    "terminal-sessions": {
      "title": "Terminal-Sitzungen",
      "description": "Persistente PTY-Sitzungen, CLI-Erkennung und Wiederherstellungsverhalten."
    },
    "copy-buttons": {
      "title": "Kopier-Buttons",
      "description": "Vorlagen-Übergabe von Plänen an Terminals und externe Tools."
    },
    "deep-research": {
      "title": "Deep Research & Web-Suche",
      "description": "Web-Such-Workflow, API-Integration, Abfrageoptimierung und Integration in Entwicklungs-Workflows."
    },
    "provider-routing": {
      "title": "Anbieter-Routing & Streaming",
      "description": "Wie Anbieter-Anfragen normalisiert, gestreamt und verfolgt werden."
    },
    "model-configuration": {
      "title": "Modellkonfiguration",
      "description": "Erlaubte Modelle pro Aufgabe und Token-Leitplanken im Selektor-Toggle."
    },
    "server-setup": {
      "title": "Dedizierte Server-Einrichtung",
      "description": "Ansible-basierte Infrastruktur: Basis-Härtung, App-Deployment und Vault-verwaltete Secrets."
    },
    "tauri-v2": {
      "title": "Tauri v2 Entwicklungsleitfaden",
      "description": "Projektstruktur, Befehle und fähigkeitsbasierte Berechtigungen für Tauri v2."
    },
    "distribution-macos": {
      "title": "macOS-Distribution",
      "description": "Signierung, Notarisierung, DMG-Paketierung und Updater-Artefakte."
    },
    "distribution-windows": {
      "title": "Windows-Distribution & Store",
      "description": "NSIS-Builds, MSIX-Paketierung und Microsoft Store-Einreichung."
    }
  },
  "implementationPlans": {
    "meta": {
      "title": "Implementierungspläne - KI-Änderungen überprüfen",
      "description": "Leitfaden zur KI-Implementierungsplanung. Generieren, überprüfen und genehmigen Sie dateibasierte Pläne vor der Ausführung. Duplikate und falsche Pfade verhindern."
    },
    "category": "Produktleitfaden",
    "context": {
      "audit": "Plan metadata persists with each job so you can review which inputs were used (task description, selected roots/files, model settings) and compare drafts later.",
      "heading": "Kontext und Metadaten für Unternehmensführung",
      "storage": "Das Panel speichert, welche Repository-Stammverzeichnisse während des Dateisuche-Workflows ausgewählt wurden, sodass Folgeaktionen denselben Umfang wiederverwenden. Es zeichnet auch planspezifische Metadaten auf, wie das Projektverzeichnis und vorbereitete Prompt-Inhalte, sodass nachgelagerte Prompts generiert oder kopiert werden können, ohne den Workflow neu zu berechnen.",
      "tokenEstimation": "Die Token-Schätzung wird vor dem Kopieren von Prompts ausgeführt. Das Panel ruft den Token-Schätzungsbefehl mit dem Projektverzeichnis, ausgewählten Dateien und dem aktuell gewählten Modell auf und zeigt sowohl System- als auch Benutzer-Prompt-Summen an, damit Teams unter den Modell-Limits bleiben können."
    },
    "cta": {
      "claudeCodeLink": "Claude Plan-Modus-Workflow ansehen",
      "codexLink": "Codex Plan-Modus-Workflow ansehen",
      "cursorLink": "Cursor Plan-Modus-Workflow ansehen",
      "description": "Human-in-the-Loop-Implementierungspläne sind in der PlanToCode-Desktop-Anwendung verfügbar. Laden Sie den Build für Ihre Plattform herunter, um sichere, kontrollierte KI-gestützte Entwicklung zu erleben.",
      "heading": "Bereit, KI-Coding-Agenten sicher einzusetzen?",
      "links": {
        "architecture": "Systemarchitektur",
        "decisions": "Entscheidungen & Kompromisse",
        "buildYourOwn": "Eigene Pipeline erstellen",
        "fileDiscovery": "Dateisuche-Workflow"
      }
    },
    "date": "2025-09-19",
    "description": "How PlanToCode enables confident adoption of AI coding agents through human-in-the-loop review, granular file-by-file plans, and clear handoff workflows.",
    "fileGranularity": {
      "created": "Erstellt (mit vollständigen Dateipfaden und initialer Inhaltsstruktur)",
      "declaredFiles": "Jeder Schritt in einem Plan deklariert explizit, welche Dateien:",
      "deleted": "Gelöscht (mit Begründung und Abhängigkeitsanalyse)",
      "heading": "Datei-für-Datei-Granularität",
      "impact": "Reviewers can immediately identify if critical legacy code will be modified, if breaking changes are proposed, or if the plan touches files that require additional scrutiny.",
      "intro": "Implementation plans use a highly granular structure that breaks down development tasks on a file-by-file basis, with exact file paths corresponding to the project's repository structure. This granularity makes scope explicit before any code is touched.",
      "modified": "Geändert (mit spezifischen Zeilenbereichen und beschriebenen Änderungen)",
      "referenced": "Referenziert (für Kontext, aber nicht geändert)",
      "transmission": "Der Datei-für-Datei-Ansatz ermöglicht auch die präzise Übertragung genehmigter Pläne an Coding-Agenten. Anstelle vager Anweisungen wie \"aktualisiere das Authentifizierungssystem\" erhalten Agenten exakte Spezifikationen: \"ändere src/auth/session_manager.rs Zeilen 45-67, um Token-Rotation hinzuzufügen, erstelle src/auth/token_store.rs mit folgender Struktur...\""
    },
    "hitl": {
      "approve": "Genehmigen:",
      "approveDesc": "When you are ready, you can hand the plan off to a coding agent or developer for execution.",
      "conclusion": "This workflow keeps execution aligned with the plan you reviewed and helps prevent surprise changes.",
      "edit": "Bearbeiten:",
      "editDesc": "You can directly modify steps, adjust approaches, add constraints, or remove risky operations using VS Code editing features.",
      "heading": "Human-in-the-Loop-Führung",
      "intro": "PlanToCode keeps planning human-in-the-loop so you can review, edit, and decide when to hand off a plan for execution.",
      "reject": "Discard:",
      "rejectDesc": "If a draft isn't useful, you can delete it from the session list.",
      "requestChanges": "Änderungen anfordern:",
      "requestChangesDesc": "Generate alternative plans or merge drafts with custom instructions to converge on the approach you want.",
      "review": "Überprüfen:",
      "reviewDesc": "Pläne werden im Monaco-Editor geöffnet, wo Prüfer jede vorgeschlagene Änderung mit vollständiger Syntaxhervorhebung und professionellen Bearbeitungswerkzeugen untersuchen können.",
      "workflow": "Plans are designed for a structured review workflow before any code modifications begin:"
    },
    "intro": "Review and approve every plan before execution. File-by-file granularity keeps scope explicit and changes aligned with your requirements.",
    "metaDescription": "Leitfaden zur KI-Implementierungsplanung. Generieren, überprüfen und genehmigen Sie dateibasierte Pläne vor der Ausführung. Duplikate und falsche Pfade verhindern.",
    "metaTitle": "Implementierungspläne - KI-Änderungen überprüfen",
    "multiplePlans": {
      "description": "Plans can be merged, deleted, or reopened later. The panel keeps a list of selected plan identifiers, manages a dedicated modal for terminal output tied to a plan, and exposes navigation helpers so reviewers can page through earlier plans without closing the viewer. Terminal access, prompt copy controls, and merge instructions all share the same job identifier so plan history stays consistent.",
      "heading": "Arbeiten mit mehreren Plänen"
    },
    "ogDescription": "Verstehen Sie, wie Human-in-the-Loop-Führung und Datei-für-Datei-Überprüfungs-Workflows sichere KI-Entwicklung mit vollständiger Kontrolle über Code-Änderungen gewährleisten.",
    "ogTitle": "Human-in-the-Loop-Implementierungspläne in PlanToCode",
    "plansOrigin": {
      "description": "Jeder Plan entspricht einem Hintergrundjob in der aktuellen Sitzung. Das Panel abonniert Plan-Daten, verfolgt, welcher Plan aktuell geöffnet ist, und stellt Navigation zwischen früheren und neueren Jobs bereit. Dieses Verhalten befindet sich in {code} und der umgebenden Panel-Komponente.",
      "heading": "Woher die Pläne kommen",
      "processor": "ImplementationPlanProcessor verarbeitet die Plan-Generierung. Er liest relevante Dateien, generiert optional einen Verzeichnisbaum basierend auf ausgewählten Stammverzeichnissen und stellt einen vereinheitlichten Prompt für das LLM zusammen.",
      "storage": "Plan responses are stored in the background_jobs table with metadata including planTitle, summary, sessionName, and token usage. The raw LLM response is preserved for review and debugging.",
      "streaming": "Pläne werden über den LlmTaskRunner mit Echtzeit-Fortschrittsereignissen gestreamt. Token-Warnungen werden für Prompts über 100k Token protokolliert, aber die Verarbeitung wird mit vollem Inhalt fortgesetzt."
    },
    "readTime": "6 Min.",
    "reviewingPlans": {
      "description": "Plan-Inhalte werden durch den gemeinsamen {code} gerendert, der den Monaco Editor umhüllt. Der Viewer erkennt automatisch gängige Sprachen, unterstützt In-Zwischenablage-Kopieren-Aktionen, virtualisiert sehr große Pläne und bietet optionale Metriken wie Zeichenanzahlen und syntaxbewusste Hervorhebung.",
      "heading": "Pläne mit Monaco überprüfen",
      "opening": "Wenn ein Plan geöffnet wird, löst das Panel den aktiven Plan nach Job-Bezeichner auf, übergibt den Inhalt an Monaco und lässt Prüfer zwischen benachbarten Jobs wechseln, ohne das aktuell geöffnete Modal zu verlieren."
    },
    "visuals": {
      "structure": {
        "title": "Implementierungsplan-Struktur",
        "description": "XML-Format für Implementierungspläne mit Datei-für-Datei-Granularität und Metadaten.",
        "imageSrc": "/images/docs/implementation-plans/structure.svg",
        "caption": "Plan-Struktur mit Schritten, Dateien und Abhängigkeitsverfolgung",
        "imageAlt": "Implementation plan XML structure diagram"
      }
    },
    "title": "Implementierungspläne",
    "planProcessor": {
      "heading": "Plan-Generierungs-Pipeline",
      "description": "Der ImplementationPlanProcessor orchestriert die Plan-Generierung, indem er Dateiinhalte lädt, Kontext aufbaut und Ergebnisse durch den LLM-Task-Runner streamt.",
      "inputs": "Sitzungskontext, Aufgabenbeschreibung, ausgewählte relevante Dateien, optionaler Verzeichnisbaum (konfigurierbar über include_project_structure-Flag) und Web-Such-Flag für externe Recherche.",
      "prompt": "Verwendet prompt_utils::build_unified_prompt, um Aufgabenbeschreibung, vollständige Dateiinhalte (ohne Kürzung) und Verzeichnisbaum in ein modellspezifisches Format mit geschätzten Token-Anzahlen zu kombinieren.",
      "output": "Rohe LLM-Antwort als JobResultData::Text gespeichert. Metadaten umfassen planTitle, summary, Token-Nutzung, Cache-Statistiken und tatsächliche Kosten.",
      "display": "Antworten werden über Fortschrittsereignisse an die UI gestreamt. Pläne werden in einem Monaco-basierten VirtualizedCodeViewer mit Syntaxhervorhebung und Kopieraktionen gerendert."
    },
    "schema": {
      "heading": "Plan-Datenstruktur",
      "description": "Implementierungspläne werden als rohe LLM-Antworten mit zugehörigen Metadaten gespeichert. Der Antworttext wird genau wie generiert aufbewahrt, während strukturierte Metadaten den Plan-Kontext und die Nutzung verfolgen.",
      "fieldsHeading": "Metadatenfelder",
      "fields": [
        "planTitle - Generierter oder vom Benutzer bereitgestellter Titel für den Plan",
        "summary - Lesbare Zusammenfassung des Plans",
        "sessionName - Name der Sitzung, die den Plan generiert hat",
        "isStructured - True for implementation_plan jobs; false for merge outputs",
        "isStreaming - False für abgeschlossene Pläne (true während der Generierung)",
        "planData - Enthält agent_instructions (optional) und steps-Array"
      ],
      "exampleHeading": "Metadaten-Beispiel",
      "example": "{\n  \"planTitle\": \"Authentication System Refactor\",\n  \"summary\": \"Implementation plan generated\",\n  \"sessionName\": \"my-project\",\n  \"isStructured\": true,\n  \"isStreaming\": false,\n  \"planData\": {\n    \"agent_instructions\": null,\n    \"steps\": []\n  }\n}"
    }
  },
  "modelConfiguration": {
    "meta": {
      "title": "Modellkonfiguration und Leitplanken - PlanToCode",
      "description": "Wie PlanToCode Ihnen ermöglicht, erlaubte Modelle pro Aufgabe auszuwählen und Prompts innerhalb des aktiven Kontextfensters zu halten."
    },
    "category": "Produktleitfaden",
    "date": "2025-09-20",
    "description": "Aufgabenebene-Modelllisten, Selektorsteuerungen und Token-Leitplanken im Desktop-Client.",
    "intro": "PlanToCode behandelt die Modellauswahl als Entscheidung auf Aufgabenebene. Jeder Workflow wird mit einem Standardmodell und einer erlaubten Liste ausgeliefert, und der Desktop-Client stellt diese Optionen über ein Toggle bereit, das das Senden von Prompts verhindert, die das aktive Kontextfenster überschreiten.",
    "metaDescription": "Wie PlanToCode Ihnen ermöglicht, erlaubte Modelle pro Aufgabe auszuwählen und Prompts innerhalb des aktiven Kontextfensters zu halten.",
    "metaTitle": "Modellkonfiguration und Leitplanken - PlanToCode",
    "ogDescription": "Erfahren Sie, wie Modelleinstellungen auf Aufgabenebene, Selektor-Toggles und Token-Schätzungen zusammenarbeiten.",
    "ogTitle": "Modellkonfiguration und Leitplanken - PlanToCode",
    "promptEstimation": {
      "description": "Token-Anzahlen werden durch den Token-Schätzungsbefehl berechnet. Das Panel übermittelt die Sitzungs-ID, Aufgabenbeschreibung, relevante Dateien und das ausgewählte Modell, damit das Backend System-, Benutzer- und Gesamt-Token-Werte zurückgeben kann. Diese Zahlen fließen direkt in die Selektor-Leitplanken ein und lassen Teams Prompts über dem Limit erkennen, bevor sie in ein anderes Tool kopiert werden.",
      "heading": "Prompt-Schätzung"
    },
    "readTime": "5 Min.",
    "selectorToggle": {
      "description": "Das Implementierungspläne-Panel rendert erlaubte Modelle mit dem {code}. Das Toggle zeigt jedes erlaubte Modell an, verfolgt die aktive Auswahl und prüft, ob der geschätzte Prompt plus geplante Ausgabe-Token in das beworbene Kontextfenster des Modells passen, bevor ein Wechsel erlaubt wird.",
      "guardrails": "Wenn ein Modell die gesamte Token-Anforderung nicht unterstützen kann, deaktiviert das Toggle die Schaltfläche und zeigt einen Tooltip mit der berechneten Überschreitung an, sodass Prüfer innerhalb sicherer Grenzen bleiben, bevor sie Arbeit an einen Agenten senden.",
      "heading": "Selektor-Toggle im Client"
    },
    "taskDefaults": {
      "description": "Standardmodelle und erlaubte Alternativen werden serverseitig in der Anwendungskonfiguration gespeichert. Jeder Aufgabentyp - wie Implementierungspläne, Merges, Prompt-Generierung oder Sprachtranskription - definiert ein bevorzugtes Modell, eine Liste erlaubter Optionen und Token-Limits, die die Desktop-App zur Laufzeit liest.",
      "heading": "Aufgabengesteuerte Standardwerte"
    },
    "title": "Modellkonfiguration",
    "visuals": {
      "selector": {
        "title": "Model selector toggle",
        "description": "How the model selector shows allowed models with token guardrails.",
        "imageSrc": "/images/docs/model-configuration/selector.png",
        "imageAlt": "Model selector toggle with token estimates and guardrails",
        "caption": "Placeholder for the model selector view."
      }
    }
  },
  "terminalSessions": {
    "meta": {
      "title": "Terminal-Sitzungen - PlanToCode",
      "description": "Technische Anleitung zur PTY-Terminal-Implementierung in PlanToCode. Erfahren Sie, wie Sitzungen persistiert werden, Agenten-Inaktivitätserkennung funktioniert und Wiederherstellungsmechanismen."
    },
    "attentionDetection": {
      "conclusion": "Dieser Ansatz hilft Ihnen zu verfolgen, wann Agenten Aufgaben abgeschlossen haben oder Anleitung benötigen, ohne zu erraten, warum sie gestoppt haben. Aufmerksamkeitsindikatoren werden automatisch gelöscht, wenn neue Ausgabe empfangen wird.",
      "heading": "Agenten-Aufmerksamkeitserkennung",
      "intro": "Das Terminal überwacht Agentenaktivität durch ein zweistufiges Inaktivitätserkennungssystem. Wenn ein Agent aufhört, Ausgabe zu produzieren, warnt das System Sie progressiv, um zu überprüfen, was passiert ist:",
      "level1": "Stufe 1 (30 Sekunden): \"Agent inaktiv - hat möglicherweise Aufgabe abgeschlossen\" mit gelbem Indikator",
      "level2": "Stufe 2 (2 Minuten): \"Agent erfordert Aufmerksamkeit - Terminal überprüfen\" mit rotem Indikator und Desktop-Benachrichtigung"
    },
    "category": "Produktleitfaden",
    "date": "2025-09-22",
    "dependencyChecks": {
      "description": "Vor dem Starten von Befehlen prüft das Terminal auf das Vorhandensein unterstützter CLI-Tools wie claude, cursor, codex und gemini. Derselbe Befehl meldet auch die Standard-Shell, sodass Benutzer wissen, welche Umgebung ausgeführt wird. Dies verhindert das Starten in eine Sitzung, die die erforderliche Binärdatei nicht finden kann.",
      "heading": "Abhängigkeitsprüfungen"
    },
    "description": "Persistente PTY-Sitzungen, Agenten-Aufmerksamkeitserkennung und Wiederherstellungsverhalten im Implementierungspläne-Terminal.",
    "intro": "Führen Sie Befehle in einer persistenten PTY mit Zustandsprüfungen und Protokollierung aus. Sprachtranskription ist verfügbar, wenn Sie sie benötigen.",
    "lifecycle": {
      "description": "Wenn ein Terminal geöffnet wird, erstellt die UI-Komponente eine PTY-Sitzung und streamt Ausgabe durch eine gepufferte Ansicht. Die Komponente zeigt sofortigen Verbindungsstatus, leitet Tastatureingaben an die PTY weiter und versucht automatisch erneut, wenn die Sitzung fehlschlägt. Sitzungs-Metadaten werden in SQLite mit Zeitstempeln, Exit-Codes, Arbeitsverzeichnissen und dem vollständigen Ausgabeprotokoll gespeichert, sodass Neustarts den vorherigen Kontext fortsetzen können.",
      "heading": "Sitzungs-Lebenszyklus"
    },
    "metaDescription": "Technische Anleitung zur PTY-Terminal-Implementierung in PlanToCode. Erfahren Sie, wie Sitzungen persistiert werden, Agenten-Inaktivitätserkennung funktioniert und Wiederherstellungsmechanismen.",
    "metaTitle": "Terminal-Sitzungen - PlanToCode",
    "ogDescription": "Verstehen Sie Sitzungspersistenz, Agenten-Aufmerksamkeitserkennung und Wiederherstellung im Plan-Terminal.",
    "ogTitle": "Terminal-Sitzungen - PlanToCode",
    "readTime": "6 Min.",
    "title": "Terminal-Sitzungen",
    "voiceRecovery": {
      "heading": "Sprachtranskription und Wiederherstellung",
      "recovery": "Wenn eine PTY-Sitzung die Verbindung trennt, zeigt die Terminal-Oberfläche Wiederherstellungssteuerungen an und versucht die Verbindung mit exponentiellem Backoff erneut. Zustandsprüfungen überwachen weiterhin den Sitzungsstatus und bieten automatische Wiederherstellungsaktionen, wenn Verbindungsprobleme erkannt werden.",
      "voice": "Inside the terminal modal, voice transcription can capture speech and paste it into the terminal input area. The recording hook looks up project-level transcription settings, tracks recording state, and inserts transcribed text when the recording stops."
    },
    "visuals": {
      "sessionView": {
        "title": "Terminal session architecture",
        "description": "PTY process lifecycle, output routing, and persistence layers.",
        "imageSrc": "/images/docs/terminal-sessions/session-view.png",
        "imageAlt": "Terminal session architecture showing PTY, channels, and persistence",
        "caption": "Placeholder for terminal session architecture diagram."
      }
    }
  },
  "copyButtons": {
    "meta": {
      "title": "Copy Buttons - PlanToCode",
      "description": "How template-driven copy buttons resolve placeholders against plans and hand off to terminals or clipboard for agent execution."
    },
    "category": "Execution",
    "date": "2025-09-23",
    "readTime": "10 min",
    "title": "Copy Buttons",
    "description": "Template-driven handoff from implementation plans to PTY terminals and external tools.",
    "intro": "Copy buttons resolve template placeholders against the active plan and then send the result to the clipboard (plan views) or the PTY (terminal modal).",
    "metaTitle": "Copy buttons - PlanToCode",
    "metaDescription": "How template-driven copy buttons resolve placeholders against plans and hand off to terminals or clipboard for agent execution.",
    "ogTitle": "Copy buttons - PlanToCode",
    "ogDescription": "Technical guide to copy button templates, placeholder resolution, and terminal handoff.",
    "visuals": {
      "templateFlow": {
        "title": "Template resolution flow",
        "description": "Templates resolve {{IMPLEMENTATION_PLAN}}, {{TASK_DESCRIPTION}}, and {{STEP_CONTENT}} before copying or sending to the terminal.",
        "imageSrc": "/images/docs/copy-buttons/templates.svg",
        "imageAlt": "Flow showing copy button template resolution",
        "caption": "Placeholder for a template resolution flow diagram."
      }
    },
    "templateConfiguration": {
      "heading": "Template Configuration Sources",
      "description": "Copy button templates follow a layered configuration model. Server defaults provide baseline templates, and project-level overrides customize the implementation_plan task for a given repo.",
      "serverDefaults": {
        "heading": "Server Defaults",
        "description": "Shared templates from /api/config/desktop-runtime-config. Includes button labels and template strings."
      },
      "projectOverrides": {
        "heading": "Project Overrides",
        "description": "Project overrides are stored in SQLite key_value_store under project_task_settings and merged with server defaults."
      },
      "taskSpecific": {
        "heading": "Task-Specific",
        "description": "Copy buttons are configured per task type (implementation_plan) and stored per project. There are no per-job overrides."
      }
    },
    "placeholderResolution": {
      "heading": "Placeholder Resolution",
      "description": "Templates use double-brace placeholders that are resolved against plan content and the current task description.",
      "placeholdersHeading": "Available Placeholders",
      "placeholders": [
        {
          "placeholder": "{{IMPLEMENTATION_PLAN}}",
          "description": "Full implementation plan content as generated by the LLM"
        },
        {
          "placeholder": "{{TASK_DESCRIPTION}}",
          "description": "The task description from the current session"
        },
        {
          "placeholder": "{{STEP_CONTENT}}",
          "description": "Content for the selected plan step (when a step is selected)"
        }
      ],
      "resolutionOrder": "Missing placeholders are replaced with empty strings. Step content is only available when a plan step is selected.",
      "exampleTemplate": "Example template:\n\n{{IMPLEMENTATION_PLAN}}\n\nUnderstand the implementation plan above thoroughly. Analyze the architecture, data flows, and sequence of events.\n\nTask: {{TASK_DESCRIPTION}}"
    },
    "processingPipeline": {
      "heading": "Template Processing Pipeline",
      "description": "When a button is clicked, placeholders are extracted, values are resolved, and the output is sent to clipboard or PTY depending on where the button is used.",
      "steps": [
        {
          "number": 1,
          "title": "Extract Placeholders",
          "description": "Regex scan for {{...}} patterns in the template string"
        },
        {
          "number": 2,
          "title": "Lookup Context",
          "description": "Resolve plan content and task description values for placeholders"
        },
        {
          "number": 3,
          "title": "Substitute Values",
          "description": "Replace placeholders with resolved values"
        },
        {
          "number": 4,
          "title": "Send Output",
          "description": "Copy to clipboard or write to the PTY input buffer"
        }
      ],
      "chunking": {
        "heading": "Large Plan Chunking",
        "description": "When sending to the PTY, the text is chunked into 4KB segments and a carriage return is appended."
      }
    },
    "terminalHandoff": {
      "heading": "PTY Terminal Handoff",
      "description": "In the plan terminal modal, copy buttons write the resolved template to the PTY input buffer as if typed by the user.",
      "detailsHeading": "Handoff Details",
      "details": [
        "Content sent via write_terminal_input_command to the PTY input buffer",
        "Chunked into 4KB segments for large plans",
        "Appends a carriage return after sending"
      ],
      "codeExample": "// Terminal handoff (PlanTerminalModal)\nconst textToSend = replacePlaceholders(button.content, {\n  IMPLEMENTATION_PLAN: planContent,\n  TASK_DESCRIPTION: taskDescription ?? \"\"\n});\nawait sendInChunks(sessionId, textToSend);"
    },
    "clipboardHandoff": {
      "heading": "Clipboard Handoff",
      "description": "In plan cards and plan modals, buttons copy the resolved template to the system clipboard using the browser clipboard API.",
      "crossPlatform": {
        "heading": "Cross-Platform API",
        "description": "Uses navigator.clipboard.writeText() inside the Tauri webview for clipboard access."
      },
      "feedback": {
        "heading": "User Feedback",
        "description": "Toast notification confirms the copy action."
      }
    },
    "defaultButtons": {
      "heading": "Default Copy Buttons",
      "description": "PlanToCode ships with several default copy buttons for implementation plans. These are templates you can edit in settings.",
      "buttonsHeading": "Built-in Buttons",
      "buttons": [
        {
          "id": "parallel-agents",
          "label": "Parallel Claude Coding Agents",
          "description": "Template that instructs Claude Code to launch parallel agents using the plan."
        },
        {
          "id": "investigate-results",
          "label": "Investigate Results",
          "description": "Template that asks the agent to review changes without launching new agents."
        },
        {
          "id": "task-only",
          "label": "Task",
          "description": "Copies only the task description."
        },
        {
          "id": "task-and-plan",
          "label": "Task + Plan",
          "description": "Combines task description and implementation plan for full context."
        },
        {
          "id": "plan-only",
          "label": "Plan",
          "description": "Copies only the implementation plan content."
        }
      ]
    },
    "customization": {
      "heading": "Customizing Copy Buttons",
      "description": "Copy buttons can be customized at multiple levels: global defaults, project-level overrides, and per-task configurations.",
      "globalDefaults": {
        "heading": "Global Defaults",
        "description": "Server-side configuration in /api/config/desktop-runtime-config defines the base set of copy buttons. These are loaded when the desktop app starts and cached for offline use."
      },
      "projectSettings": {
        "heading": "Project-Level Customization",
        "description": "Each project can override the default buttons through the Settings panel. Project-specific buttons are stored in key_value_store and merged with server defaults at runtime."
      },
      "taskSettings": {
        "heading": "Task-Level Configuration",
        "description": "Copy buttons are configured per task type (implementation_plan) and applied per project."
      },
      "editorDescription": "The copy button editor supports drag-and-drop ordering, inline label editing, and template content modification. Changes are persisted automatically."
    },
    "uiIntegration": {
      "heading": "UI Integration and Safety",
      "description": "Copy buttons appear in plan viewers and terminal headers. Clicking a button sends output immediately; there is no preview step by default.",
      "tokenEstimation": {
        "heading": "Token Estimation",
        "description": "Plan cards display token counts for the plan job; copy buttons do not compute per-template token estimates."
      },
      "previewModal": {
        "heading": "Full Preview Modal",
        "description": "There is no dedicated preview modal; open the plan content to inspect what will be copied."
      },
      "disabledState": {
        "heading": "Disabled State",
        "description": "Buttons are disabled when required context is missing (e.g., no active plan, missing session). Tooltips explain what context is needed to enable the button."
      }
    },
    "auditTrail": {
      "heading": "History and signoff",
      "description": "Copy button clicks are not stored in a dedicated table. Plan edits are stored in background_jobs.response and signoff state is recorded in background_jobs.metadata.userSignoff.",
      "schemaHeading": "Notes",
      "schema": "No copy_button_actions table exists in the current release.",
      "fieldsHeading": "Stored plan signals",
      "fields": [
        {
          "field": "background_jobs.response",
          "description": "Plan content after edits or merges"
        },
        {
          "field": "background_jobs.metadata.userSignoff",
          "description": "User signoff state and timestamp"
        }
      ],
      "retention": "No separate retention policy exists for copy button actions; job history retention is controlled in app settings."
    },
    "mobileIntegration": {
      "heading": "Mobile Integration",
      "description": "Copy buttons work in the iOS remote terminal actions bar. Resolved templates are sent to the linked desktop terminal.",
      "deviceLink": {
        "heading": "Device Link Support",
        "description": "When a mobile device is linked to a desktop session, copy buttons can target the desktop terminal directly. The resolved content is sent through the device link WebSocket connection."
      },
      "mobileButtons": {
        "heading": "Mobile-Specific Buttons",
        "description": "Mobile clients use the same copy button configuration stored in project task settings."
      }
    },
    "cta": {
      "heading": "Trace handoff to execution",
      "description": "Terminal sessions show where copy button output lands and how it is logged.",
      "terminalLink": "Terminal sessions",
      "plansLink": "Implementation plans"
    }
  },
  "textImprovement": {
    "meta": {
      "title": "Text improvement - PlanToCode",
      "description": "How the desktop workspace rewrites highlighted text, preserves formatting, and links the feature to voice and video inputs."
    },
    "category": "Product Guide",
    "cta": {
      "description": "Download PlanToCode to combine voice capture, video context, and inline rewriting before you generate implementation plans.",
      "heading": "Try text improvement in the desktop app",
      "links": {
        "architecture": "Architecture overview",
        "buildYourOwn": "Build your own"
      }
    },
    "date": "2025-09-21",
    "description": "How PlanToCode rewrites highlighted text without changing formatting and links the result back to your workspace.",
    "intro": "Refine text with AI context. Select text in any editor, trigger a background job, and get improved content that keeps your formatting intact.",
    "metaDescription": "How the desktop workspace rewrites highlighted text, preserves formatting, and links the feature to voice and video inputs.",
    "metaTitle": "Text improvement - PlanToCode",
    "ogDescription": "Understand the selection popover, job queue, model configuration, and integrations that power text improvement.",
    "ogTitle": "Text improvement - PlanToCode",
    "readTime": "7 min",
    "selectionPopover": {
      "component": "The popover itself is a minimal component rendered by {code}, which simply triggers the provider hook and shows a loading indicator while a rewrite is running. Because the provider registers global listeners, the popover appears in Monaco plan viewers, the plan terminal dictation field, and any task description inputs without extra wiring.",
      "heading": "Selection popover behaviour",
      "provider": "The {code} listens for selection events on standard inputs and Monaco editors. When you highlight non-empty text it positions a popover near the cursor, stores the selected range, and tracks whether the popover should be visible. Clicking the button kicks off the job and disables the control until the result returns. When the job completes the provider applies the improved text back into the same selection and flushes any pending saves to keep session state in sync."
    },
    "title": "Text Improvement",
    "triggerImprovement": {
      "action": "Pressing the popover button calls {code}. The action validates the selection, ensures a session identifier exists, and invokes the Rust command {code} via Tauri. The command builds a {code} containing the original text and queues a background job against the active session.",
      "backend": "On the backend, the {code} resolves the configured model for the {code} task, wraps the selection in XML tags, and runs the request through the {code} without streaming. When the model response returns it records token usage, cost, and the system prompt template before emitting the improved text back to the UI. The default configuration ships with Claude Sonnet 4.5 and Gemini 3 Pro as the approved models, capped at 4,096 tokens with a temperature of 0.7.",
      "heading": "What happens when you trigger an improvement",
      "metadata": "The background jobs sidebar records the original text in job metadata, so you can review what was sent alongside the rewritten copy. If the selection changes while a job is running, the provider skips replacing the text to avoid clobbering manual edits."
    },
    "videoCapture": {
      "dialog": "The video analysis dialog combines the current task description with an optional focus prompt wrapped in <description> and <video_attention_prompt> tags before sending the job. You can narrate while recording; the resulting summary can be pasted into the task description and refined with the improvement popover.",
      "features": "Video jobs include frame-rate controls, optional audio capture, and usage tracking. Results appear in the background jobs sidebar alongside text improvements.",
      "heading": "Video capture and prompt scaffolding"
    },
    "voiceIntegration": {
      "heading": "Voice transcription integration",
      "hook": "Voice recordings use the {code} hook. It loads per-project transcription defaults, requests microphone access, and inserts transcribed text at the cursor inside the task description or terminal dictation buffer. The inserted text can be highlighted and passed through the improvement popover.",
      "preferences": "Language, model, and temperature preferences persist at the project level, so teams get consistent transcription quality before refining the copy. Silence detection warns about bad audio levels, and a ten-minute cap prevents oversized recordings from blocking improvement jobs with large payloads."
    },
    "visuals": {
      "popoverFlow": {
        "title": "Text improvement flow",
        "description": "Selection popover triggers improvement job and returns enhanced text.",
        "imageSrc": "/images/docs/text-improvement/flow.svg",
        "imageAlt": "Text improvement flow diagram",
        "caption": "Selection popover to job pipeline overview."
      }
    },
    "processorDetails": {
      "heading": "Processor implementation details",
      "processor": "The {code} handles the text rewriting workflow on the Rust backend.",
      "stepsHeading": "Processing steps",
      "steps": [
        "Parse the incoming payload with original text and selection metadata",
        "Build the system prompt from the configured text_improvement template",
        "Submit the request to the LLM task runner without streaming",
        "Extract the improved text from the model response",
        "Record token usage, cost, and prompt template for billing",
        "Emit the result back to the UI via Tauri events"
      ]
    },
    "inlineRewriting": {
      "heading": "Inline rewriting behaviour",
      "description": "When the improved text returns, the provider automatically replaces the original selection. The rewriting preserves whitespace, line breaks, and any inline formatting present in the source. If the editor is Monaco-based, the change is applied as a single undo-able edit operation.",
      "contextsHeading": "Supported contexts",
      "contexts": [
        "Task description input fields",
        "Plan terminal dictation area",
        "Monaco plan viewers and editors",
        "Any standard HTML input or textarea"
      ]
    },
    "modelConfiguration": {
      "heading": "Model configuration",
      "description": "Text improvement uses the text_improvement task configuration from the desktop runtime config. You can override the default model and parameters in the settings panel.",
      "settingsHeading": "Configurable settings",
      "settings": [
        "Allowed models list (default: Claude Sonnet 4.5, Gemini 3 Pro)",
        "Maximum token limit (default: 4096)",
        "Temperature setting (default: 0.7)",
        "System prompt template override"
      ]
    },
    "keyFiles": {
      "heading": "Key implementation files",
      "items": [
        "desktop/src/contexts/TextImprovementProvider.tsx",
        "desktop/src/components/TextImprovementPopover.tsx",
        "desktop/src/actions/text-improvement/index.ts",
        "desktop/src-tauri/src/jobs/processors/text_improvement.rs",
        "server/src/config/task_model_config.rs"
      ]
    }
  },
  "voiceTranscription": {
    "meta": {
      "title": "Voice transcription - PlanToCode",
      "description": "How PlanToCode records audio, sends it to the configured transcription provider, and inserts text into task or terminal inputs."
    },
    "category": "Product Guide",
    "date": "2025-09-22",
    "description": "Recording lifecycle, device management, and transcription behavior for voice-driven prompts.",
    "deviceManagement": {
      "description": "The feature requests microphone permission, enumerates available audio inputs, and lets users choose the active device before recording. Changes take effect on the next recording.",
      "heading": "Device management",
      "monitoring": "Real-time audio level monitoring displays visual feedback during recording. The system warns when audio is silent so you can catch muted microphones before sending the recording."
    },
    "intro": "Voice transcription is available anywhere the desktop app exposes dictation controls, including the plan terminal and prompt editors. The feature records audio locally and sends a single recording to the transcription service when you stop, then inserts text into the active input field without blocking manual typing.",
    "metaDescription": "How PlanToCode records audio, sends it to the configured transcription provider, manages permissions, and inserts text into task or terminal inputs.",
    "metaTitle": "Voice transcription - PlanToCode",
    "ogDescription": "Learn how the recording hook manages devices, permissions, and streaming text.",
    "ogTitle": "Voice transcription - PlanToCode",
    "projectSettings": {
      "description": "When a recording session starts, the hook looks up the active project's transcription configuration so recordings follow the project's preferences.",
      "heading": "Project-aware settings",
      "storage": "Project transcription preferences are stored in SQLite key_value_store under project_task_settings and include the preferred model, language code, prompt, and temperature. Hosted uses managed providers; self-hosting can adjust the allowlist."
    },
    "readTime": "5 min",
    "recordingWorkflow": {
      "description": "The recording hook keeps a state machine with idle, recording, processing, and error states. It records audio into a single blob, enforces a ten-minute cap, and sends the recording on stop.",
      "heading": "Recording workflow",
      "statesHeading": "Recording states",
      "states": [
        "idle: No recording in progress, microphone permissions may or may not be granted",
        "recording: Capturing audio via MediaRecorder with live level monitoring",
        "processing: Uploading the recording to the transcription endpoint and awaiting a response",
        "error: Recording failed due to permission denial, device disconnection, or transcription API error"
      ]
    },
    "routingBehavior": {
      "heading": "Multi-destination routing",
      "description": "Transcribed text is routed based on the active UI context and inserted into the appropriate input.",
      "destinations": [
        "Task description editors: Cursor insertion with optional follow-up text_improvement",
        "Terminal dictation buffer: Command text inserted into PTY input",
        "Prompt editors: Direct insertion into active text inputs"
      ]
    },
    "pipeline": {
      "heading": "Transcription pipeline",
      "hook": "The {code} React hook manages the complete recording lifecycle. It initializes {code} for audio capture in WebM format with Opus codec, monitors audio levels, and handles device switching.",
      "command": "The desktop app invokes {code} to send audio data to the server endpoint {code}. The command validates minimum size (1KB), duration, temperature (0.0-1.0), and prompt length (max 1000 characters); the server enforces max file size (100MB).",
      "constraints": "Audio files must be between 1KB and 100MB. Supported formats: WAV, MP3, M4A, OGG, WebM, FLAC, AAC, and MP4. The transcription model must be specified explicitly and must be in the server allowlist (OpenAI models by default on hosted)."
    },
    "serverProcessing": {
      "heading": "Server-side processing",
      "endpoint": "The server exposes {code} which accepts multipart form data. It routes requests to OpenAI or Google based on the model's provider configuration, validates user credits, and calculates billing based on audio duration.",
      "parametersHeading": "Request parameters",
      "parameters": [
        "file: Audio file data (required) - WAV, MP3, M4A, OGG, WebM, FLAC, AAC, or MP4",
        "model: Transcription model ID (required) - from server allowlist (e.g., openai/gpt-4o-transcribe)",
        "durationMs: Recording duration in milliseconds (required for billing calculation)",
        "language: ISO 639-1 language code (optional) - improves accuracy for specific languages",
        "prompt: Context hint for transcription (optional, max 1000 characters) - helps with domain-specific vocabulary",
        "temperature: Sampling temperature 0.0-1.0 (optional, default 0.0) - lower values produce more deterministic output"
      ]
    },
    "dataFlow": {
      "heading": "Data flow",
      "description": "Audio data flows from the browser through the Tauri command layer to the server, which proxies requests to the appropriate transcription provider.",
      "stepsHeading": "Processing steps",
      "steps": [
        "Browser MediaRecorder captures audio in a single recording (WebM by default)",
        "useVoiceTranscription tracks duration and recording state",
        "On stop, the audio blob is converted to bytes and sent via transcribe_audio_command",
        "Tauri command validates size, duration, temperature, and prompt length",
        "Request sent to server /api/audio/transcriptions endpoint with auth token",
        "Server routes to the configured provider and returns transcribed text",
        "Transcribed text returned to desktop and inserted via callback"
      ]
    },
    "keyFiles": {
      "heading": "Key implementation files",
      "items": [
        "desktop/src/hooks/use-voice-recording/use-voice-transcription.ts",
        "desktop/src/actions/voice-transcription/transcribe.ts",
        "desktop/src-tauri/src/commands/audio_commands.rs",
        "server/src/handlers/proxy/specialized/transcription.rs",
        "server/src/clients/openai/transcription.rs",
        "server/src/clients/google_client.rs"
      ]
    },
    "examples": {
      "heading": "Usage examples",
      "description": "Common voice transcription workflows:",
      "items": [
        "Sprint planning: Dictate tasks, then run text_improvement and task_refinement",
        "Terminal commands: Dictation transcribed and typed directly into PTY for execution",
        "Bug reports: Verbal description captured, refined with text_improvement, then stored in task history",
        "Walkthrough notes: Narrate a screen recording and attach the video analysis summary to the task"
      ]
    },
    "cta": {
      "heading": "Continue exploring",
      "description": "Learn how transcribed text can be refined and how meeting recordings are processed into actionable tasks.",
      "links": {
        "textImprovement": "Text Improvement",
        "meetingIngestion": "Meeting Ingestion"
      }
    },
    "title": "Voice Transcription",
    "visuals": {
      "recordingFlow": {
        "title": "Voice transcription pipeline",
        "description": "Audio capture, provider transcription, and text insertion flow.",
        "imageSrc": "/images/docs/voice-transcription/pipeline.svg",
        "imageAlt": "Voice transcription pipeline diagram",
        "caption": "Audio flows from browser capture through Tauri commands to the configured transcription provider."
      }
    }
  },
  "overview": {
    "meta": {
      "title": "Systemübersicht - PlanToCode",
      "description": "Starten Sie hier: was PlanToCode tut, wie die Kernschleife funktioniert und wo sich jede Komponente im Repository befindet."
    },
    "category": "Übersicht",
    "date": "2025-09-25",
    "readTime": "15 Min.",
    "title": "Systemübersicht",
    "description": "Eine prägnante Karte des Systems, der Kernschleife und der erforderlichen Abhängigkeiten.",
    "intro": "PlanToCode ist ein Desktop-Arbeitsbereich, der Code-Änderungen vor der Ausführung plant und validiert. Er koordiniert eine lokale Rust-Job-Engine, eine React-UI und einen Server-Proxy für LLM-Aufrufe. Das System folgt einer Offline-First-Architektur, bei der die Desktop-App unabhängig mit SQLite für lokalen Zustand arbeitet, während der Server Authentifizierung, LLM-Anbieter-Routing und Abrechnung verwaltet. Ohne LLM-Zugriff (gehostet verwaltet oder selbst gehostet mit eigenen Schlüsseln) werden die Planungs- und Analyse-Pipelines nicht ausgeführt.",
    "visuals": {
      "systemMap": {
        "title": "Systemkarte",
        "description": "Karte der Desktop-App, des Rust-Kerns, der lokalen SQLite-Speicherung und des Server-Proxys.",
        "imageSrc": "/images/docs/overview/system-map.svg",
        "imageAlt": "PlanToCode-Systemkarten-Diagramm",
        "caption": "Vierschichtige Architektur mit Daten, die nach unten fließen, und Ereignissen, die nach oben streamen."
      }
    },
    "systemLayers": {
      "heading": "Systemschichten",
      "description": "Das System ist in vier verschiedene Schichten organisiert, die über wohldefinierte Schnittstellen kommunizieren:",
      "items": [
        "Präsentationsschicht: React-UI mit Monaco-Editoren, Terminal-Panels und Workflow-Steuerungen (desktop/src/)",
        "Befehlsschicht: Tauri-Befehle, die React und Rust verbinden, IPC und Zustandsverwaltung behandeln (desktop/src-tauri/src/commands/)",
        "Verarbeitungsschicht: Job-Prozessoren, Workflow-Orchestrator und Geschäftslogik in Rust (desktop/src-tauri/src/jobs/)",
        "Persistenzschicht: SQLite-Repositories für lokalen Zustand und Server PostgreSQL für Authentifizierung/Abrechnung (desktop/src-tauri/src/db_utils/)"
      ]
    },
    "coreLoop": {
      "heading": "Kernschleife in der Praxis",
      "description": "Jede Aufgabe durchläuft einen wohldefinierten Lebenszyklus von der Erfassung bis zur Ausführung:",
      "steps": [
        "Die Aufgabe aus Text, Sprachtranskription (über useVoiceTranscription-Hook) oder Video-Aufnahme-Analyse erfassen.",
        "Die Aufgabenbeschreibung und Ziele mit text_improvement-Jobs über TextImprovementProcessor verfeinern.",
        "Den Dateisuche-Workflow ausführen: RootFolderSelectionProcessor wählt Verzeichnisse, RegexFileFilterProcessor wendet Muster an, FileRelevanceAssessmentProcessor bewertet Inhalte, ExtendedPathFinderProcessor erweitert den Kontext.",
        "Implementierungspläne über ImplementationPlanProcessor generieren, der XML-formatierte Pläne an den Monaco-Viewer streamt.",
        "Optional mehrere Plan-Entwürfe mit ImplementationPlanMergeProcessor unter Verwendung von XML-getaggten Quellplänen zusammenführen.",
        "Den genehmigten Plan über PTY-Terminal-Sitzungen oder Kopier-Button-Vorlagen für externe Agenten ausführen oder exportieren.",
        "Persist every job, artifact, and terminal log to SQLite (background_jobs, terminal_sessions tables) for history and recovery."
      ]
    },
    "components": {
      "heading": "Hauptkomponenten",
      "description": "Jede Komponente hat eine spezifische Verantwortung und kommuniziert über typisierte Schnittstellen:",
      "items": [
        "Desktop-UI (React) in desktop/src/ mit Monaco-Plan-Ansichten, Terminal-Panels und Providern (SessionProvider, TextImprovementProvider).",
        "Rust-Kern (Tauri v2) in desktop/src-tauri/ für Befehle, Jobs und Persistenz mit fähigkeitsbasierten Berechtigungen.",
        "Lokales SQLite-Schema in desktop/src-tauri/migrations/consolidated_schema.sql mit WAL-Modus für gleichzeitigen Zugriff.",
        "Server-Proxy (Actix-Web) in server/src/ für Authentifizierung, Anbieter-Routing, Streaming-Antworten und Abrechnung über Stripe.",
        "Mobiler iOS-Client in mobile/ios/Core/ mit SwiftUI-Oberfläche, Auth0 PKCE und WebSocket-Geräteverbindung.",
        "Infrastruktur-Automatisierung in infrastructure/ansible/ für Hetzner (EU) und InterServer (US) dedizierte Server."
      ]
    },
    "dependencies": {
      "heading": "Erforderliche Abhängigkeiten",
      "description": "Das System erfordert diese externen Dienste und Ressourcen:",
      "items": [
        "Externe LLM-Anbieter (OpenAI, Anthropic, Google, X.AI, OpenRouter) für Plan-Generierung, Transkription und Analyse.",
        "Auth0-basierte Authentifizierung mit PKCE-Ablauf für Desktop- und Mobile-Sitzungen.",
        "PostgreSQL 17 und Redis 7+ für serverseitige Benutzerkonten, Abrechnungsstatus und Job-Warteschlangen (selbst gehostete Deployments).",
        "Lokaler Dateisystemzugriff über git ls-files oder Verzeichnisdurchquerung für Dateisuche-Workflows.",
        "Whisper-kompatibler Transkriptions-Endpunkt für Spracheingabe-Verarbeitung."
      ]
    },
    "codeMap": {
      "heading": "Wo sich das Verhalten im Repository befindet",
      "description": "Schnellreferenz zu wichtigen Verzeichnissen und Dateien:",
      "items": [
        "Tauri-Befehle: desktop/src-tauri/src/commands/ (35+ Befehlsmodule: job_commands.rs, workflow_commands.rs, terminal_commands.rs, session_commands.rs, auth0_commands.rs)",
        "Workflow-Orchestrierung: desktop/src-tauri/src/jobs/workflow_orchestrator/ (definition_loader.rs, stage_scheduler.rs, event_emitter.rs, payload_builder.rs)",
        "Job-Prozessoren: desktop/src-tauri/src/jobs/processors/ (implementation_plan_processor.rs, text_improvement_processor.rs, root_folder_selection_processor.rs)",
        "SQLite-Repositories: desktop/src-tauri/src/db_utils/ (background_job_repository/, session_repository.rs, terminal_repository.rs)",
        "Server-Routen: server/src/routes.rs (configure_routes, configure_public_auth_routes, configure_webhook_routes)",
        "LLM-Proxy-Handler: server/src/handlers/proxy_handlers.rs und server/src/handlers/proxy/ (router.rs, providers/)",
        "Anbieter-Transformatoren: server/src/handlers/provider_transformers/ (openai.rs, google.rs, anthropic.rs, xai.rs)",
        "iOS-Workflows: mobile/ios/Core/Sources/Workflows/WorkflowManager.swift mit MobileSessionManager und APIClient",
        "Infrastruktur-Playbooks: infrastructure/ansible/site-base.yml (Härtung, PostgreSQL, Redis) und site-app.yml (Deployment)"
      ]
    },
    "keyAbstractions": {
      "heading": "Schlüsselabstraktionen",
      "description": "Das Verständnis dieser Kernkonzepte hilft bei der Navigation der Codebasis:",
      "items": [
        "Session: Projektkontext gespeichert in sessions-Tabelle mit task_description, included_files und Modellpräferenzen. Identifiziert durch UUID.",
        "Background Job: LLM-gestützte Operation gespeichert in background_jobs-Tabelle mit task_type, prompt, response, Token-Verfolgung und Kosten.",
        "Workflow: Mehrstufiger orchestrierter Prozess (z.B. file_finder_workflow), koordiniert vom WorkflowOrchestrator mit IntermediateData, das zwischen Stufen übergeben wird.",
        "Terminal Session: PTY process stored in terminal_sessions with output_log, status, and optional job_id linking for traceability.",
        "Provider: LLM-Dienst-Abstraktion in server/src/handlers/proxy/providers/ mit Anfrage-Transformation und Antwort-Normalisierung."
      ]
    },
    "dataFlowSummary": {
      "heading": "Datenfluss-Zusammenfassung",
      "description": "Wie Daten durch das System für eine typische Planungsaufgabe fließen:",
      "items": [
        "Benutzereingabe erfolgt durch React-Komponenten und fließt zu Tauri-Befehlen über @tauri-apps/api/core invoke().",
        "Befehle erstellen background_jobs-Datensätze und dispatchen an Job-Prozessoren über die Job-Warteschlange.",
        "Prozessoren erstellen Prompts, senden Anfragen durch den Server-LLM-Proxy und streamen Antworten über Tauri-Ereignisse.",
        "Antworten werden in SQLite gespeichert und an React-Provider emittiert, die den UI-Zustand aktualisieren.",
        "Terminal-Ausführung streamt PTY-Ausgabe an die UI und persistiert Protokolle für Sitzungs-Wiederherstellung."
      ]
    }
  },
  "runtimeWalkthrough": {
    "meta": {
      "title": "Laufzeit-Walkthrough - PlanToCode",
      "description": "End-to-End-Zeitlinie einer Aufgabe von der Eingabe bis zur Plan-Ausgabe, mit Job-Typen und Artefakt-Flüssen."
    },
    "category": "Architektur",
    "date": "2025-09-25",
    "readTime": "12 Min.",
    "title": "Laufzeit-Walkthrough",
    "description": "End-to-End-Laufzeit-Zeitlinie von der Aufgabeneingabe bis zur Plan-Ausgabe.",
    "intro": "Dieser Walkthrough verfolgt eine einzelne Aufgabe von der ersten Erfassung über die Dateisuche, Plan-Generierung bis zur Terminal-Ausführung. Jede Phase entspricht spezifischen Job-Typen und erzeugt Artefakte, die in SQLite gespeichert werden.",
    "visuals": {
      "timeline": {
        "title": "Laufzeit-Zeitlinie",
        "description": "Visuelle Zeitlinie mit Aufgabeneingabe, Workflow-Phasen und Plan-Ausgabe.",
        "imageSrc": "/images/docs/runtime-walkthrough/timeline.svg",
        "imageAlt": "Laufzeit-Zeitlinien-Diagramm",
        "caption": "Aufgabenausführung fließt durch sechs Phasen, wobei alle Artefakte in SQLite persistiert werden."
      },
      "walkthroughVideo": {
        "title": "Laufzeit-Walkthrough-Video",
        "description": "Video-Demonstration einer vollständigen Aufgabenausführung von der Eingabe bis zur Plan-Ausgabe.",
        "videoSrc": "",
        "posterSrc": "",
        "caption": "Video-Walkthrough-Platzhalter - eine Demonstration des vollständigen Planungs-Workflows aufnehmen."
      }
    },
    "timeline": {
      "heading": "Übergeordnete Laufzeitsequenz",
      "description": "Eine vollständige Aufgabenausführung folgt dieser Operationssequenz:",
      "steps": [
        "Benutzer gibt eine Aufgabenbeschreibung ein oder diktiert sie in der Desktop-UI über die TaskDescriptionEditor-Komponente.",
        "Optional: text_improvement-Job verfeinert die Roheingabe durch TextImprovementProcessor.",
        "Benutzer löst Dateisuche-Workflow über das Implementierungspläne-Panel start_file_finder_workflow-Befehl aus.",
        "WorkflowOrchestrator in desktop/src-tauri/src/jobs/workflow_orchestrator/ erstellt einen Workflow-Datensatz und plant Stufe 1.",
        "Stufe 1 (root_folder_selection): RootFolderSelectionProcessor sendet Verzeichnisbaum an LLM, speichert ausgewählte Stammverzeichnisse in IntermediateData.selectedRoots.",
        "Stufe 2 (regex_file_filter): RegexFileFilterProcessor generiert Muster, führt git ls-files aus, speichert Treffer in IntermediateData.locallyFilteredFiles.",
        "Stufe 3 (file_relevance_assessment): FileRelevanceAssessmentProcessor chunked Dateiinhalte, bewertet Relevanz, speichert in IntermediateData.aiFilteredFiles.",
        "Stufe 4 (extended_path_finder): ExtendedPathFinderProcessor erweitert Kontext mit Importen und Abhängigkeiten, speichert in IntermediateData.verifiedPaths.",
        "UI empfängt workflow-completed-Ereignis über event_emitter.rs, aktualisiert Dateiauswahl-Anzeige.",
        "Benutzer löst Plan-Generierung mit ausgewählten Dateien über generate_implementation_plan-Befehl aus.",
        "ImplementationPlanProcessor in desktop/src-tauri/src/jobs/processors/implementation_plan_processor.rs streamt XML-Plan-Inhalt an Monaco-Viewer über job:stream-progress-Ereignisse.",
        "Benutzer überprüft Plan in VirtualizedCodeViewer-Komponente, kann direkt bearbeiten oder Merge anfordern.",
        "Genehmigter Plan wird über Kopier-Button-Vorlagen an Terminal kopiert oder für externe Agenten exportiert.",
        "Terminal-Sitzung in terminal_commands.rs erfasst PTY-Ausgabe, erkennt Agenten-Aufmerksamkeitszustände.",
        "All artifacts persist in SQLite background_jobs and terminal_sessions tables for history and session recovery."
      ]
    },
    "jobTypes": {
      "heading": "Job-Typen in der Laufzeit",
      "description": "Jeder task_type entspricht einem spezifischen Prozessor und erzeugt unterschiedliche Artefakte:",
      "items": [
        "text_improvement: TextImprovementProcessor umhüllt Text mit XML, sendet an LLM, gibt verfeinerten Text zurück. Gespeichert in background_jobs.response.",
        "root_folder_selection: RootFolderSelectionProcessor empfängt Verzeichnisbaum, gibt JSON-Array ausgewählter Verzeichnisse zurück.",
        "regex_file_filter: RegexFileFilterProcessor generiert Muster aus Aufgabenbeschreibung, wendet auf git-Dateiliste an.",
        "file_relevance_assessment: FileRelevanceAssessmentProcessor lädt Dateiinhalte, chunked nach Token-Limit, bewertet Relevanz.",
        "extended_path_finder: ExtendedPathFinderProcessor analysiert Importe/Abhängigkeiten, erweitert Kontext mit verwandten Dateien.",
        "implementation_plan: ImplementationPlanProcessor streamt XML-formatierte Pläne mit plan_step-Elementen.",
        "implementation_plan_merge: ImplementationPlanMergeProcessor kombiniert Pläne mit source_plans XML-Tags und Benutzeranweisungen.",
        "video_analysis: Processes screen recordings via /api/llm/video/analyze with a framerate hint.",
        "web_search_prompts_generation: Generiert research_task XML-Blöcke für Deep Research-Workflow.",
        "web_search_execution: Führt Recherche-Prompts parallel aus, aggregiert Erkenntnisse."
      ]
    },
    "inputCapture": {
      "heading": "Aufgabeneingabe-Erfassung",
      "description": "Aufgaben gelangen durch mehrere Eingabeoberflächen ins System:",
      "text": "Aufgabenbeschreibungen werden in TaskDescriptionEditor getippt oder eingefügt, der in sessions.task_description persistiert und Historie-Einträge in task_description_history-Tabelle mit device_id für Multi-Geräte-Synchronisation erstellt.",
      "voice": "Spracheingabe verwendet useVoiceTranscription-Hook, der über MediaRecorder API aufnimmt, an /api/audio/transcriptions sendet und an Cursorposition einfügt.",
      "video": "Video-Analyse verwendet VideoAnalysisDialog, um Bildschirmaufnahmen zu erfassen, zu /api/llm/video/analyze hochzuladen und UI-Zustandsbeobachtungen zu extrahieren."
    },
    "workflowExecution": {
      "heading": "Workflow-Ausführungsdetails",
      "description": "Der WorkflowOrchestrator koordiniert mehrstufige Workflows:",
      "scheduling": "workflow_lifecycle_manager.rs erstellt Workflow-Datensätze und stage_scheduler.rs dispatcht Stufen sequenziell basierend auf Workflow-JSON-Definitionen.",
      "data": "IntermediateData-Strukturen in workflow_types.rs übergeben Ausgaben zwischen Stufen: selectedRoots, rawRegexPatterns, locallyFilteredFiles, aiFilteredFiles, verifiedPaths.",
      "events": "event_emitter.rs veröffentlicht workflow-status und workflow-stage Tauri-Ereignisse, die vom WorkflowTracker in der React-UI konsumiert werden."
    },
    "persistence": {
      "heading": "Zustandspersistenz",
      "description": "All artifacts are persisted for review and recovery:",
      "jobs": "background_job_repository/ speichert Job-Datensätze mit session_id, task_type, status, prompt, response, tokens_sent/received, actual_cost.",
      "sessions": "session_repository.rs verwaltet sessions-Tabelle mit task_description, included_files, model_used, Historie-Versionen.",
      "terminals": "terminal_repository.rs persistiert terminal_sessions mit output_log, status, exit_code, working_directory für Sitzungs-Wiederherstellung.",
      "rehydration": "Beim App-Neustart rehydriert der Rust-Kern den Sitzungsstatus aus SQLite, markiert veraltete laufende Jobs als fehlgeschlagen und stellt Terminal-Ausgabeprotokolle wieder her."
    },
    "inputs": {
      "heading": "Task input capture",
      "capture": "Tasks enter the system through multiple input surfaces: typed text in TaskDescriptionEditor, voice dictation via useVoiceTranscription hook, or video analysis through VideoAnalysisDialog.",
      "artifacts": "Each input type updates SQLite state: task_description in sessions and task_description_history, voice transcription inserts text into the session or terminal input, and video analysis responses are stored in background_jobs."
    },
    "refinement": {
      "heading": "Eingabeverfeinerung",
      "jobs": "Der text_improvement-Job-Typ verfeinert Roheingabe durch TextImprovementProcessor, umhüllt Text mit XML und sendet an LLM für Grammatik-, Klarheits- und Strukturverbesserungen.",
      "storage": "Verfeinerter Text wird in background_jobs.response gespeichert und kann sessions.task_description über den React-Provider aktualisieren."
    },
    "discovery": {
      "heading": "Dateisuche-Workflow",
      "workflow": "FileFinderWorkflow führt vier sequenzielle Stufen aus: root_folder_selection grenzt Verzeichnisse ein, regex_file_filter wendet Muster an, file_relevance_assessment bewertet Inhalt und extended_path_finder erweitert mit Abhängigkeiten.",
      "outputs": "Jede Stufe speichert Ergebnisse in IntermediateData-Strukturen, die zwischen Prozessoren übergeben werden, wobei finale Dateiauswahlen in sessions.included_files persistiert werden."
    },
    "planGeneration": {
      "heading": "Plan-Generierung",
      "jobs": "Der implementation_plan-Job-Typ verwendet ImplementationPlanProcessor, um XML-formatierte Pläne mit plan_step-Elementen zu generieren, die Dateipfade, Operationstypen und Code-Änderungen enthalten.",
      "streaming": "Plan-Inhalt wird über job:stream-progress Tauri-Ereignisse an die UI gestreamt und in der VirtualizedCodeViewer Monaco-Komponente mit Syntaxhervorhebung angezeigt."
    },
    "merge": {
      "heading": "Plan-Zusammenführung",
      "instructions": "Der implementation_plan_merge-Job kombiniert mehrere Pläne mit source_plans XML-Tags und vom Benutzer bereitgestellten Merge-Anweisungen, um Konflikte zu lösen und Änderungen zu konsolidieren.",
      "outputs": "Zusammengeführte Pläne bewahren die Nachverfolgbarkeit zu Quellplänen und enthalten merged_from-Metadaten im finalen background_jobs-Datensatz."
    },
    "review": {
      "heading": "Plan review",
      "editor": "Plans open in the Monaco-based VirtualizedCodeViewer for review. Users can edit plan text directly, request modifications, or approve for execution.",
      "audit": "Plan edits are persisted in background_jobs.response; signoff state is recorded in background_jobs.metadata.userSignoff."
    },
    "execution": {
      "heading": "Ausführungsübergabe",
      "terminal": "Genehmigte Pläne werden über Kopier-Button-Vorlagen an das integrierte Terminal kopiert oder für externe Agenten wie Claude Code, Cursor oder Codex exportiert.",
      "logging": "Terminal-Sitzungen in terminal_commands.rs erfassen PTY-Ausgabe, erkennen Agenten-Aufmerksamkeitszustände und protokollieren alle Ausführungsaktivitäten in der terminal_sessions-Tabelle."
    },
    "state": {
      "heading": "Zustandspersistenz",
      "jobs": "Alle Job-Artefakte werden in der background_jobs-Tabelle mit session_id, task_type, status, prompt, response, Token-Anzahlen und Kostenverfolgung persistiert.",
      "rehydration": "Beim App-Neustart rehydriert der Rust-Kern den Sitzungsstatus aus SQLite, markiert veraltete laufende Jobs als fehlgeschlagen und stellt Terminal-Ausgabeprotokolle wieder her."
    },
    "jobMap": {
      "heading": "Job-Typ-Zuordnung",
      "items": [
        "text_improvement → TextImprovementProcessor → verfeinerte Aufgabenbeschreibungen",
        "root_folder_selection → RootFolderSelectionProcessor → ausgewählte Verzeichnisse",
        "regex_file_filter → RegexFileFilterProcessor → musterübereinstimmende Dateien",
        "file_relevance_assessment → FileRelevanceAssessmentProcessor → bewertete Dateien",
        "extended_path_finder → ExtendedPathFinderProcessor → erweiterter Kontext",
        "implementation_plan → ImplementationPlanProcessor → XML-Plan-Dokumente",
        "implementation_plan_merge → ImplementationPlanMergeProcessor → zusammengeführte Pläne"
      ]
    },
    "cta": {
      "heading": "Architektur erkunden",
      "description": "Verstehen Sie im Detail, wie die Komponenten zusammenpassen.",
      "links": {
        "architecture": "Architekturübersicht",
        "jobs": "Hintergrundjobs",
        "desktop": "Desktop-App-Interna",
        "dataModel": "Datenmodell",
        "plans": "Implementierungspläne"
      }
    }
  },
  "desktopApp": {
    "meta": {
      "title": "Desktop-App-Interna - PlanToCode",
      "description": "Wie die Tauri-Desktop-Shell, Rust-Befehlsschicht, SQLite-Persistenz und PTY-Sitzungen zusammenarbeiten."
    },
    "category": "Desktop",
    "date": "2025-09-25",
    "readTime": "14 Min.",
    "title": "Desktop-App-Interna",
    "description": "Tauri v2 Shell, Rust-Befehlsschicht, PTY-Sitzungen und UI-Zustandsverwaltung.",
    "intro": "Die Desktop-App ist eine Tauri v2 Shell (Version 2.9.1), die eine React-UI ausführt. Rust-Dienste stellen Befehle für Workflows, Terminal-Sitzungen und Konfiguration bereit, während der Zustand lokal in SQLite persistiert wird. Das fähigkeitsbasierte Berechtigungsmodell bietet feinkörnige Sicherheitskontrollen für Dateisystemzugriff, HTTP-Anfragen, Shell-Ausführung und Systembenachrichtigungen.",
    "visuals": {
      "shell": {
        "title": "Desktop-Shell-Übersicht",
        "description": "Screenshot mit Plan-Editor, Terminal-Tabs und Job-Status-Seitenleiste.",
        "imageSrc": "/assets/images/demo-implementation-plans.jpg",
        "imageAlt": "PlanToCode Desktop-Shell",
        "caption": "Die Desktop-App mit dem Implementierungspläne-Panel und der Seitenleiste."
      }
    },
    "projectLayout": {
      "heading": "Projektstruktur",
      "description": "Die Desktop-Anwendung folgt der Standard-Tauri-v2-Struktur:",
      "items": [
        "desktop/src/: React-UI-Komponenten, Hooks, Provider und Desktop-spezifische Adapter.",
        "desktop/src-tauri/: Rust-Kern einschließlich Befehle, Jobs, Repositories und Dienste.",
        "desktop/src-tauri/src/lib.rs: Anwendungs-Einstiegspunkt mit Plugin-Registrierung und AppState-Verwaltung.",
        "desktop/src-tauri/src/commands/: 35+ Tauri-Befehlshandler-Module, nach Domäne organisiert.",
        "desktop/src-tauri/src/jobs/: Hintergrundjob-Prozessoren, Workflow-Orchestrierung und Warteschlangenverwaltung.",
        "desktop/src-tauri/capabilities/: JSON-Fähigkeitsdefinitionen für Sicherheitsberechtigungen (default.json, desktop-default.json, plantocode-api.json).",
        "desktop/src-tauri/migrations/: SQLite-Schema-Migrationen in consolidated_schema.sql."
      ]
    },
    "ui": {
      "heading": "React-UI und Oberfläche",
      "description": "Die React-UI rendert den Aufgabenbeschreibungs-Editor, Plan-Viewer und Terminal-Panels:",
      "components": [
        "TaskDescriptionEditor: Mehrzeilige Eingabe mit Sprachtranskriptions-Integration und Textverbesserungs-Popover.",
        "VirtualizedCodeViewer: Monaco-basierte Plan-Anzeige mit Syntaxhervorhebung und Kopieraktionen.",
        "TerminalSurface: PTY-Ausgabepuffer mit Verbindungsstatus, Agenten-Aufmerksamkeitsindikatoren und Spracheingabe.",
        "SessionProvider: Globale Zustandsverwaltung für aktive Sitzung, Dateiauswahlen und Modellpräferenzen.",
        "TextImprovementProvider: Auswahl-Listener und Popover-Positionierung für Inline-Umschreibungen.",
        "WorkflowTracker: Echtzeit-Fortschrittsanzeige für mehrstufige Workflows."
      ]
    },
    "commands": {
      "heading": "Tauri-Befehle",
      "description": "Befehle in desktop/src-tauri/src/commands/ stellen Rust-Funktionalität der React-UI bereit. Wichtige Module umfassen:",
      "modules": [
        "job_commands.rs: create_job, get_job, cancel_job, get_jobs_for_session, clear_job_history.",
        "workflow_commands.rs: start_file_finder_workflow, get_workflow_status, retry_workflow, pause_workflow, resume_workflow.",
        "terminal_commands.rs: start_terminal_session, attach_terminal_output, write_terminal_input, resize_terminal_session, get_terminal_metadata, graceful_exit_terminal.",
        "session_commands.rs: create_session, get_session, update_session, sync_task_description_history, sync_file_selection_history.",
        "auth0_commands.rs: initiate_login, complete_login, refresh_token, logout, get_user_info.",
        "implementation_plan_commands.rs: generate_implementation_plan, merge_implementation_plans, estimate_tokens.",
        "config_commands.rs: get_runtime_config, get_model_config, get_system_prompts, refresh_config_cache.",
        "settings_commands.rs: get_setting, set_setting, get_project_system_prompt, set_project_system_prompt."
      ]
    },
    "appState": {
      "heading": "AppState-Verwaltung",
      "description": "Der Rust-Kern verwaltet den Anwendungszustand über Tauris Zustandssystem:",
      "structure": "AppState-Struct in lib.rs enthält: config_load_error (Option<String>), HTTP-Client (reqwest::Client), RuntimeConfig (Server-URL, Onboarding-Status) hinter Mutex und Auth0State für Authentifizierung.",
      "config": "RuntimeConfig enthält server_url, onboarding_complete-Flag und wird über set_runtime_config-Befehl aktualisiert. ConfigCache speichert Laufzeit-KI-Konfiguration mit projektbezogenen Überschreibungen.",
      "tokens": "TokenManager verwendet den OS-Schlüsselbund (über keyring-Crate), um access_token, refresh_token und jwt sicher mit automatischer Erneuerung vor Ablauf zu speichern."
    },
    "jobs": {
      "heading": "Job-Prozessoren und Workflows",
      "description": "Job-Verarbeitungsarchitektur in desktop/src-tauri/src/jobs/:",
      "queue": "queue.rs verwaltet die Job-Warteschlange mit In-Memory-ausstehenden Jobs und SQLite-Persistenz. Jobs durchlaufen Status: idle, created, queued, acknowledged_by_worker, preparing, preparing_input, running, generating_stream, processing_stream, completed, failed, canceled.",
      "processors": "processors/-Verzeichnis enthält aufgabenspezifische Prozessoren: ImplementationPlanProcessor (Streaming-Pläne), TextImprovementProcessor (Inline-Umschreibungen), RootFolderSelectionProcessor, RegexFileFilterProcessor, FileRelevanceAssessmentProcessor, ExtendedPathFinderProcessor.",
      "orchestrator": "workflow_orchestrator/ koordiniert mehrstufige Workflows: definition_loader.rs lädt JSON-Workflow-Definitionen, stage_scheduler.rs dispatcht Stufen, payload_builder.rs konstruiert Eingaben, event_emitter.rs veröffentlicht Fortschrittsereignisse.",
      "streaming": "processors/generic_llm_stream_processor.rs verarbeitet Streaming-LLM-Antworten, emittiert job:stream-progress-Ereignisse und akkumuliert Inhalt in background_jobs.response."
    },
    "persistence": {
      "heading": "Lokale Persistenz",
      "description": "SQLite-Speicherung in desktop/src-tauri/migrations/consolidated_schema.sql:",
      "tables": [
        "sessions: id (UUID), name, project_directory, project_hash, task_description, included_files, force_excluded_files, model_used, Historie-Versionen.",
        "background_jobs: id (UUID), session_id (FK), task_type, status, prompt, response, tokens_sent/received, cache_read/write_tokens, actual_cost, metadata (JSON), server_request_id.",
        "terminal_sessions: id, job_id (nullable FK), session_id, status, process_pid, output_log, working_directory, environment_vars, last_output_at.",
        "task_description_history: session_id (FK), description, device_id, sequence_number, version für Multi-Geräte-Sync.",
        "file_selection_history: session_id (FK), included_files, force_excluded_files, device_id, sequence_number.",
        "project_system_prompts: project_hash, task_type, system_prompt für projektbezogene Prompt-Überschreibungen.",
        "key_value_store: key, value (JSON), updated_at für App-Einstellungen.",
        "error_logs: timestamp, level, error_type, message, context, stack, metadata für clientseitige Fehlerverfolgung."
      ],
      "repositories": "Repositories in db_utils/ bieten typisierten Zugriff: background_job_repository/ (modular mit base.rs, worker.rs, metadata.rs, cleanup.rs), session_repository.rs, terminal_repository.rs, settings_repository.rs, error_log_repository.rs."
    },
    "terminal": {
      "heading": "Terminal-Sitzungen",
      "description": "PTY-Terminal-Implementierung:",
      "commands": "terminal_commands.rs verwaltet den Sitzungs-Lebenszyklus: create_terminal_session spawnt PTY über portable-pty-Crate, send_terminal_input leitet Tastatureingaben weiter, resize_terminal passt Dimensionen an, check_cli_availability verifiziert Tool-Präsenz (claude, cursor, codex, gemini).",
      "persistence": "terminal_repository.rs persistiert Sitzungen mit output_log (akkumulierte Terminal-Ausgabe), status (idle/running/completed/failed/agent_requires_attention), exit_code, working_directory. Sitzungen können nach App-Neustart wiederhergestellt werden.",
      "attention": "Agenten-Aufmerksamkeitserkennung überwacht last_output_at-Zeitstempel. Stufe 1 (30s inaktiv): gelber Indikator. Stufe 2 (2min inaktiv): roter Indikator mit Desktop-Benachrichtigung."
    },
    "inputStability": {
      "heading": "Aufgabenbeschreibungs-Stabilität",
      "description": "Der Aufgabenbeschreibungs-Editor enthält Schutzmaßnahmen, um Cursorsprünge zu verhindern:",
      "items": [
        "Remote-Updates werden eingereiht, während der Benutzer tippt, und bei Leerlauf oder Blur geleert.",
        "Auswahl-Zustand wird verfolgt und nach React-Neurendering wiederhergestellt.",
        "Hintergrund-Schreiber rufen sessionActions.updateCurrentSessionFields auf, um Updates zu koordinieren.",
        "Multi-Geräte-Sync verwendet sequence_number und version-Felder, um Konflikte zu lösen."
      ]
    },
    "plugins": {
      "heading": "Tauri-Plugins",
      "description": "PlanToCode verwendet das Tauri v2 Plugin-Ökosystem:",
      "list": [
        "tauri-plugin-http (2.5.2): HTTP-Client mit CSP-bewusstem Fetch für API-Aufrufe.",
        "tauri-plugin-dialog (2.4.2): Native Datei-/Ordnerauswahl und Meldungsdialoge.",
        "tauri-plugin-shell (2.3.3): Shell-Befehlsausführung für externe CLI-Tools.",
        "tauri-plugin-store (2.4.1): Persistente Schlüssel-Wert-Speicherung für App-Einstellungen.",
        "tauri-plugin-notification (2.3.0): Desktop-Benachrichtigungen für Agenten-Aufmerksamkeit.",
        "tauri-plugin-updater (2.9.0): In-App-Updates mit Signaturverifizierung.",
        "tauri-plugin-single-instance (2.3.4): Einzelinstanz-Durchsetzung.",
        "tauri-plugin-process (2.3.1): Prozess-Neustart-Fähigkeit."
      ]
    }
  },
  "serverApi": {
    "meta": {
      "title": "Server API and LLM proxy - PlanToCode",
      "description": "Auth, provider routing, model configuration, and WebSocket endpoints used by desktop and mobile clients."
    },
    "category": "Server",
    "date": "2025-09-25",
    "readTime": "12 min",
    "title": "Server API & LLM Proxy",
    "description": "Auth, provider routing, model configuration, billing, and WebSocket endpoints.",
    "intro": "The server is an Actix-Web service written in Rust that provides authentication, model configuration, LLM proxying, and billing. Desktop and mobile clients depend on it for secure provider routing and streaming responses. The server runs on dedicated infrastructure in two regions: Hetzner (EU) at api-eu.plantocode.com and InterServer (US) at api-us.plantocode.com.",
    "visuals": {
      "flow": {
        "title": "Server request flow",
        "description": "Diagram showing clients, API routes, and the LLM proxy.",
        "imageSrc": "/images/docs/provider-routing/routing-map.svg",
        "imageAlt": "Server request flow diagram",
        "caption": "Placeholder for the server request flow."
      }
    },
    "routeOrganization": {
      "heading": "Route organization",
      "description": "Routes are organized in server/src/routes.rs with three configuration functions:",
      "functions": [
        "configure_routes(): JWT-authenticated routes under /api scope. Includes auth, billing, config, providers, models, llm proxy, audio, system-prompts, consent, devices, notifications.",
        "configure_public_auth_routes(): Browser-based auth flow under /auth scope. Includes Auth0 initiate-login, callback, and logged-out routes.",
        "configure_webhook_routes(): Unauthenticated webhook endpoints under /webhooks scope. Currently handles Stripe webhooks."
      ]
    },
    "auth": {
      "heading": "Authentication endpoints",
      "description": "Authentication uses Auth0 with PKCE flow:",
      "routes": [
        "/auth/auth0/initiate-login (GET): Starts OAuth flow with code_challenge, redirects to Auth0.",
        "/auth/auth0/callback (GET): Handles Auth0 redirect, exchanges code for tokens.",
        "/api/auth/userinfo (GET): Returns authenticated user info from Auth0.",
        "/api/auth/logout (POST): Revokes tokens and clears session.",
        "/api/auth/account (DELETE): Account deletion with cascading cleanup.",
        "/api/auth0/refresh-app-token (POST): Refreshes access token using refresh token."
      ],
      "implementation": "Auth handlers in server/src/handlers/auth0_handlers.rs and server/src/handlers/auth/. JWT validation uses services/auth/jwt.rs with JWKS rotation. Revoked tokens tracked in revoked_token_repository.rs."
    },
    "llmProxy": {
      "heading": "LLM proxy and streaming",
      "description": "The LLM proxy normalizes requests across providers and streams responses:",
      "routes": [
        "/api/llm/chat/completions (POST): Main chat completion endpoint. Routes to OpenAI, Anthropic, Google, X.AI, or OpenRouter based on model ID.",
        "/api/llm/video/analyze (POST): Multipart video upload for video analysis (FPS hint). Requires google/* models with video capability.",
        "/api/llm/cancel (POST): Cancels in-flight streaming request by request_id.",
        "/api/llm/status/{request_id} (GET): Returns status of a request (active, completed, cancelled).",
        "/api/audio/transcriptions (POST): Whisper-compatible transcription. Multipart upload with audio file and parameters."
      ],
      "routing": "Router in server/src/handlers/proxy/router.rs selects provider based on model ID prefix (openai/, anthropic/, google/, xai/, openrouter/). Provider-specific handlers in server/src/handlers/proxy/providers/ transform requests and normalize responses.",
      "streaming": "Streaming responses use Server-Sent Events (SSE) via streaming/sse_adapter.rs. The proxy forwards chunks from providers, transforms them to a common format, and tracks token usage in real-time."
    },
    "providers": {
      "heading": "Provider routing",
      "description": "Provider handlers in server/src/handlers/proxy/providers/:",
      "handlers": [
        "openai.rs: OpenAI and OpenAI-compatible APIs (GPT-4, o1, o3).",
        "anthropic.rs: Anthropic Claude models with prompt caching support.",
        "google.rs: Google Gemini models including video analysis capability.",
        "xai.rs: X.AI Grok models.",
        "openrouter.rs: OpenRouter aggregation for model routing."
      ],
      "transformers": "Request/response transformers in server/src/handlers/provider_transformers/ normalize API differences. Each transformer handles: request body format, authentication headers, streaming chunk format, usage extraction, error normalization."
    },
    "config": {
      "heading": "Configuration endpoints",
      "description": "Configuration and model metadata endpoints:",
      "routes": [
        "/api/config/all-configurations (GET): Returns all application configurations including model settings per task type.",
        "/api/config/desktop-runtime-config (GET): Desktop-specific runtime configuration.",
        "/api/config/billing (GET/PUT): Billing configuration management.",
        "/api/providers (GET): List of available LLM providers with capabilities.",
        "/api/providers/with-counts (GET): Providers with model counts.",
        "/api/providers/by-capability/{capability} (GET): Filter providers by capability.",
        "/api/models (GET): All available models with pricing.",
        "/api/models/{id} (GET): Single model details.",
        "/api/models/by-provider/{provider_code} (GET): Models for a specific provider.",
        "/api/models/estimate-cost (POST): Cost estimation for a request.",
        "/api/models/estimate-tokens (POST): Token count estimation.",
        "/api/system-prompts/defaults (GET): Default system prompts by task type."
      ]
    },
    "billing": {
      "heading": "Billing endpoints",
      "description": "Credit-based billing system integrated with Stripe:",
      "routes": [
        "/api/billing/dashboard (GET): User billing dashboard data.",
        "/api/billing/usage-summary (GET): Detailed usage with cost breakdown.",
        "/api/billing/credits/balance (GET): Current credit balance.",
        "/api/billing/credits/details (GET): Credit details including grants and purchases.",
        "/api/billing/credits/unified-history (GET): Transaction history.",
        "/api/billing/checkout/credit-purchase (POST): Create Stripe checkout for credits.",
        "/api/billing/checkout/setup (POST): Create Stripe setup session for payment method.",
        "/api/billing/auto-top-off (GET/PUT): Auto top-off settings management."
      ],
      "implementation": "Billing handlers in server/src/handlers/billing/. Credit service in services/credit_service.rs. Stripe integration via services/stripe_service.rs with webhook handling in webhook_handlers.rs."
    },
    "devices": {
      "heading": "Device management",
      "description": "Device registration and push notifications:",
      "routes": [
        "/api/devices/register (POST): Register desktop device with device_id.",
        "/api/devices/mobile/register (POST): Register mobile device with platform info.",
        "/api/devices/{device_id}/heartbeat (POST): Device heartbeat for presence.",
        "/api/devices/{device_id}/push-token (POST): Save push notification token.",
        "/api/devices/{device_id}/connection-descriptor (GET): WebSocket connection info for device linking.",
        "/api/notifications/job-completed (POST): Send push notification for completed job.",
        "/api/notifications/job-progress (POST): Send progress notification."
      ]
    },
    "websockets": {
      "heading": "WebSocket endpoints",
      "description": "Real-time communication via WebSocket:",
      "endpoints": [
        "/ws/device-link: Relay for desktop-mobile device linking. Handles terminal output streaming, job status updates, and RPC commands between linked devices.",
        "/ws/events: General event stream for real-time updates."
      ],
      "implementation": "Device link relay in server/src/handlers/device_link_ws.rs. Sessions managed by services/relay_session_store.rs with heartbeat monitoring and reconnection support."
    },
    "serverStorage": {
      "heading": "Server-side persistence",
      "description": "PostgreSQL database with repositories in server/src/db/repositories/:",
      "repositories": [
        "user_repository.rs: User accounts linked to Auth0 sub.",
        "customer_billing_repository.rs: Stripe customer and credit state.",
        "credit_transaction_repository.rs: Credit transaction history.",
        "provider_repository.rs: LLM provider configuration.",
        "system_prompts_repository.rs: System prompt templates.",
        "consent_repository.rs: Legal consent tracking.",
        "audit_log_repository.rs: Audit trail for sensitive operations.",
        "revoked_token_repository.rs: JWT revocation list.",
        "api_key_repository.rs: API key management with secure hashing."
      ]
    }
  },
  "backgroundJobs": {
    "meta": {
      "title": "Background jobs - PlanToCode",
      "description": "Job queue architecture, processor types, state machine, and artifact storage for the desktop job engine."
    },
    "category": "Architecture",
    "date": "2025-09-25",
    "readTime": "14 min",
    "title": "Background Jobs",
    "description": "Job queue, processors, state machine, event streaming, and artifact storage.",
    "intro": "All LLM-backed work runs through the background job system in the desktop app. The job queue dispatches work to processors, streams progress events, and persists every prompt and response in SQLite for review and recovery. This architecture enables cancellation, retry, cost tracking, and real-time UI updates.",
    "visuals": {
      "stateMachine": {
        "title": "Job state machine",
        "description": "Diagram showing job status transitions from created through completion or failure.",
        "imageSrc": "/images/docs/background-jobs/state-machine.svg",
        "imageAlt": "Job state machine diagram",
        "caption": "Placeholder for job state machine diagram."
      }
    },
    "jobRecord": {
      "heading": "Job record structure",
      "description": "Each job creates a background_jobs row in SQLite with these fields:",
      "fields": [
        "id (TEXT PRIMARY KEY): UUID for the job.",
        "session_id (TEXT NOT NULL, FK): References sessions.id with CASCADE DELETE.",
        "task_type (TEXT DEFAULT 'unknown'): Processor identifier (e.g., implementation_plan, text_improvement, root_folder_selection).",
        "status (TEXT): Current state with CHECK constraint for valid values.",
        "prompt (TEXT NOT NULL): Full text sent to the LLM, stored for review and debugging.",
        "response (TEXT): LLM output or error message.",
        "error_message (TEXT): Detailed error information on failure.",
        "tokens_sent (INTEGER DEFAULT 0): Input token count from provider response.",
        "tokens_received (INTEGER DEFAULT 0): Output token count.",
        "cache_read_tokens (INTEGER DEFAULT 0): Tokens read from provider cache (Anthropic).",
        "cache_write_tokens (INTEGER DEFAULT 0): Tokens written to cache.",
        "model_used (TEXT): Model identifier used for the request.",
        "actual_cost (REAL): Computed cost based on token usage and model pricing.",
        "metadata (TEXT): JSON with task-specific data, workflow IDs, stage names.",
        "system_prompt_template (TEXT): Template identifier used for the system prompt.",
        "server_request_id (TEXT): Links to server-side usage tracking.",
        "created_at, updated_at, start_time, end_time (INTEGER): Timestamps.",
        "is_finalized (INTEGER DEFAULT 0): Whether final cost/usage has been recorded."
      ]
    },
    "statusValues": {
      "heading": "Status values and transitions",
      "description": "Jobs transition through well-defined statuses tracked in the database:",
      "statuses": [
        "idle: Initial state before processing starts.",
        "created: Job record created, not yet queued.",
        "queued: Added to job queue, waiting for processor.",
        "acknowledged_by_worker: Processor has picked up the job.",
        "preparing: Processor is gathering inputs (files, prompts).",
        "preparing_input: Building the LLM request payload.",
        "running: Request sent to LLM, awaiting response.",
        "generating_stream: Streaming response in progress.",
        "processing_stream: Processing streamed chunks.",
        "completed: Job finished successfully.",
        "completed_by_tag: Completed via stream end tag detection.",
        "failed: Job failed with error_message populated.",
        "canceled: User requested cancellation."
      ],
      "transitions": "Transitions are enforced in background_job_repository/worker.rs. Invalid transitions are rejected. Status changes emit job:status-changed Tauri events."
    },
    "orchestrator": {
      "heading": "Workflow orchestrator",
      "description": "Multi-stage workflows are managed by WorkflowOrchestrator in desktop/src-tauri/src/jobs/workflow_orchestrator/:",
      "modules": [
        "mod.rs: Main orchestrator struct and workflow execution entry point.",
        "definition_loader.rs: Loads workflow JSON definitions (e.g., file_finder_workflow.json) specifying stage order and processor types.",
        "stage_scheduler.rs: Schedules stages sequentially, waits for upstream completion.",
        "stage_job_manager.rs: Creates background_job records for each stage.",
        "payload_builder.rs: Constructs stage inputs from IntermediateData.",
        "data_extraction.rs: Extracts outputs from completed stage jobs.",
        "event_emitter.rs: Publishes workflow-status and workflow-stage Tauri events.",
        "state_updater.rs: Updates workflow state in memory and database.",
        "completion_handler.rs: Handles workflow completion and cleanup.",
        "failure_handler.rs: Manages stage failures and retry decisions.",
        "retry_handler.rs: Implements retry logic with exponential backoff."
      ],
      "dataFlow": "Workflows use WorkflowIntermediateData (defined in workflow_types.rs) to pass outputs between stages: directoryTreeContent, selectedRoots, rawRegexPatterns, locallyFilteredFiles, aiFilteredFiles, verifiedPaths, unverifiedPaths."
    },
    "processors": {
      "heading": "Job processors",
      "description": "Each task_type maps to a processor in desktop/src-tauri/src/jobs/processors/:",
      "implementations": [
        "implementation_plan_processor.rs: Loads selected file contents, builds structured prompt with directory tree, streams XML plan to UI. Uses generic_llm_stream_processor for streaming.",
        "text_improvement_processor.rs: Wraps selection in XML tags, sends non-streaming request, returns improved text. Runs via LlmTaskRunner.",
        "root_folder_selection_processor.rs: Sends directory tree to LLM, parses JSON array response of selected directories.",
        "RegexFileFilterProcessor (in processors/mod.rs): Generates regex patterns from task, applies to git file list, filters binaries.",
        "FileRelevanceAssessmentProcessor: Chunks file contents by token limit, scores relevance in batches, aggregates relevant paths.",
        "ExtendedPathFinderProcessor (path_finder_types.rs): Analyzes imports/dependencies, suggests related files, validates paths exist.",
        "web_search_prompts_generator_processor.rs: Generates research_task XML blocks for deep research.",
        "web_search_executor_processor.rs: Executes research prompts in parallel via server search API.",
        "generic_llm_stream_processor.rs: Reusable streaming processor that handles chunk accumulation, event emission, and response finalization."
      ]
    },
    "events": {
      "heading": "Event streaming",
      "description": "Job progress emits Tauri events consumed by the React UI:",
      "eventTypes": [
        "job:status-changed: Payload {jobId, status, error?}. Emitted on every status transition.",
        "job:stream-progress: Payload {jobId, content, tokensReceived}. Emitted for each streaming chunk.",
        "job:completed: Payload {jobId, response, tokensTotal, cost}. Emitted on successful completion.",
        "workflow-status: Payload {workflowId, status, currentStage?}. Workflow-level status updates.",
        "workflow-stage: Payload {workflowId, stageName, status}. Individual stage status."
      ],
      "reactConsumption": "React components subscribe via useEffect with listen() from @tauri-apps/api/event. WorkflowTracker aggregates workflow events. JobStatusIndicator displays real-time status."
    },
    "retry": {
      "heading": "Retry and cancellation",
      "description": "Job retry and cancellation mechanisms:",
      "retryLogic": "retry_handler.rs manages retry counts and delays. Retries use exponential backoff with configurable max attempts. Retry state stored in job.metadata.retryCount.",
      "cancellation": "Cancellation sets a flag checked between streaming chunks in generic_llm_stream_processor.rs. Server-side cancellation sends /api/llm/cancel with request_id.",
      "cleanup": "workflow_cleanup.rs handles cleanup of incomplete workflows. Stale jobs (running status after app restart) are marked failed."
    },
    "artifacts": {
      "heading": "Artifact storage",
      "description": "Job inputs and outputs are fully persisted for review:",
      "stored": [
        "prompt: Complete LLM prompt including system prompt and user content.",
        "response: Full LLM response text or streaming accumulation.",
        "metadata: JSON with task-specific data (original text for improvements, file lists, workflow context).",
        "system_prompt_template: Identifier linking to server-side prompt template version.",
        "Token counts and cost: Captured from provider response for billing and analysis."
      ],
      "access": "background_job_repository provides queries: get_jobs_for_session, get_job_by_id, get_jobs_by_task_type, get_recent_jobs. Job history displayed in BackgroundJobsSidebar component."
    },
    "costTracking": {
      "heading": "Cost tracking",
      "description": "Per-job cost tracking enables budget management:",
      "calculation": "Cost calculated using model pricing from server/src/models/model_pricing.rs. Formula: (tokens_sent * input_price + tokens_received * output_price) with cache adjustments.",
      "accumulation": "Session-level cost aggregated from background_jobs. UI displays cumulative cost in session header.",
      "serverSync": "server_request_id links desktop jobs to server-side usage records for billing reconciliation."
    },
    "cta": {
      "heading": "See the data model",
      "description": "Understand the SQLite schema that stores jobs, sessions, and terminal session logs.",
      "links": {
        "dataModel": "Data model",
        "runtime": "Runtime walkthrough"
      }
    }
  },
  "buildYourOwn": {
    "meta": {
      "title": "Build your own pipeline - PlanToCode",
      "description": "Conceptual guide for designing file discovery and plan generation workflows similar to PlanToCode."
    },
    "category": "Reference",
    "date": "2025-09-25",
    "readTime": "12 min",
    "title": "Build Your Own Pipeline",
    "description": "Conceptual guide for designing file discovery and plan generation workflows.",
    "intro": "This guide distills the key architectural patterns from PlanToCode into a conceptual blueprint. Whether you want to build a similar system or understand why certain design decisions were made, this document covers the foundational patterns you can reuse or adapt.",
    "visuals": {
      "pipelineMap": {
        "title": "Pipeline architecture map",
        "description": "Overview of the multi-stage pipeline from task input to plan output.",
        "imageSrc": "/images/docs/overview/system-map.svg",
        "imageAlt": "Pipeline architecture diagram",
        "caption": "Placeholder for pipeline architecture diagram."
      }
    },
    "keyPatterns": {
      "heading": "Key Architectural Patterns",
      "jobQueue": {
        "title": "Job Queue Pattern",
        "description": "All LLM-backed operations run as background jobs with status tracking, cancellation support, and retry logic. Jobs are persisted to SQLite so state survives app restarts.",
        "benefits": [
          "Decouples UI responsiveness from LLM latency",
          "Enables cancellation mid-stream",
          "Provides job history of all operations",
          "Supports retry with exponential backoff"
        ],
        "pitfalls": [
          "Job status management adds complexity",
          "Need careful handling of stale jobs on restart",
          "Stream accumulation can consume memory for large responses"
        ]
      },
      "workflowOrchestrator": {
        "title": "Workflow Orchestrator Pattern",
        "description": "Multi-stage workflows are coordinated by an orchestrator that schedules stages sequentially, passes intermediate data between them, and handles failures at any stage.",
        "components": [
          "Definition loader reads workflow JSON specs",
          "Stage scheduler dispatches stages in order",
          "Payload builder constructs inputs from prior outputs",
          "Event emitter publishes progress for UI updates"
        ]
      },
      "repositoryPattern": {
        "title": "Repository Pattern",
        "description": "All persistence goes through typed repositories that abstract SQLite operations. This provides a clean API, enables testing, and centralizes database access.",
        "benefits": [
          "Typed access prevents SQL injection",
          "Repositories can be mocked for testing",
          "Centralized query optimization",
          "Consistent error handling"
        ]
      }
    },
    "steps": {
      "step1": {
        "title": "1. Define your task model",
        "description": "Start by defining what constitutes a task in your system. PlanToCode uses sessions with task descriptions, file selections, and model preferences.",
        "details": "Store task metadata in a dedicated table with versioning for history tracking."
      },
      "step2": {
        "title": "2. Build the job queue",
        "description": "Create a job queue that persists jobs to storage, emits status events, and supports cancellation. Jobs should track prompts, responses, tokens, and cost.",
        "details": "Use a semaphore-based concurrency limiter to control parallel LLM requests."
      },
      "step3": {
        "title": "3. Implement processors",
        "description": "Each job type needs a processor that builds prompts, calls the LLM, and parses responses. Use streaming for long outputs.",
        "details": "Processors should be stateless and receive all context through job parameters."
      },
      "step4": {
        "title": "4. Create the workflow orchestrator",
        "description": "For multi-stage workflows, build an orchestrator that schedules stages, manages intermediate data, and handles failures.",
        "details": "Store workflow definitions as JSON for easy modification without code changes."
      },
      "step5": {
        "title": "5. Add the routing layer",
        "description": "Route LLM requests through a server proxy that normalizes payloads, manages API keys, and tracks usage.",
        "details": "Keep provider credentials on the server; never embed them in desktop clients."
      }
    },
    "architectureDecisions": {
      "heading": "Architecture Decisions",
      "decisions": [
        {
          "question": "Should you use a local database or server-side storage?",
          "recommendation": "Use local SQLite for job state and artifacts. This enables offline operation and fast queries. Sync to server only for billing and cross-device state."
        },
        {
          "question": "Streaming vs non-streaming responses?",
          "recommendation": "Use streaming for plan generation and any output shown progressively. Use non-streaming for short transformations like text improvement."
        },
        {
          "question": "How to handle LLM provider failures?",
          "recommendation": "Implement automatic retry with exponential backoff. Consider a fallback provider like OpenRouter for resilience."
        },
        {
          "question": "Where should file content be loaded?",
          "recommendation": "Load file content in the processor just before building the prompt. This ensures fresh content and avoids storing large blobs in job records."
        }
      ]
    },
    "customizeVsReuse": {
      "heading": "What to Customize vs Reuse",
      "customize": [
        "Prompt templates for your specific use case",
        "File discovery patterns for your project types",
        "Output format (XML, JSON, Markdown)",
        "Model selection per task type"
      ],
      "reuse": [
        "Job queue architecture with status tracking",
        "Workflow orchestrator pattern",
        "Repository pattern for persistence",
        "Streaming response handling",
        "Provider routing and normalization"
      ]
    },
    "commonPitfalls": {
      "heading": "Common Pitfalls to Avoid",
      "items": [
        {
          "pitfall": "Embedding API keys in the client",
          "solution": "Route all LLM requests through a server proxy that manages credentials securely."
        },
        {
          "pitfall": "Not persisting job state",
          "solution": "Store every job with full prompt and response for review and recovery."
        },
        {
          "pitfall": "Blocking UI on LLM calls",
          "solution": "Use background jobs with event-driven UI updates for responsive interfaces."
        },
        {
          "pitfall": "Ignoring token limits",
          "solution": "Estimate tokens before sending and chunk large inputs to stay within context windows."
        },
        {
          "pitfall": "No cancellation support",
          "solution": "Check cancellation flags between streaming chunks and propagate to server."
        }
      ]
    },
    "artifacts": {
      "heading": "Artifacts to Persist",
      "items": [
        "Full prompt sent to the LLM (for debugging and review)",
        "Complete response including streaming accumulation",
        "Token counts from provider response",
        "Computed cost based on model pricing",
        "System prompt template identifier for versioning",
        "Workflow intermediate data for multi-stage flows"
      ]
    },
    "implementationNotes": {
      "heading": "Implementation Notes",
      "items": [
        "Use SQLite with WAL mode for concurrent read/write access",
        "Implement graceful shutdown that marks running jobs as failed",
        "Add health checks for external dependencies before job processing",
        "Log all LLM errors with full context for debugging",
        "Consider caching file content with short TTL to avoid redundant reads"
      ]
    }
  },
  "decisionsTradeoffs": {
    "meta": {
      "title": "Technical decisions and tradeoffs - PlanToCode",
      "description": "Why Tauri, SQLite, and a dedicated LLM proxy were chosen, and what operational tradeoffs they create."
    },
    "category": "Architecture",
    "date": "2025-09-25",
    "readTime": "10 min",
    "title": "Technical Decisions & Tradeoffs",
    "description": "Why Tauri, SQLite, and a dedicated LLM proxy were chosen and what they cost.",
    "intro": "Every architecture involves tradeoffs. This document explains the major technology choices in PlanToCode, what benefits they provide, and what costs or limitations they introduce.",
    "visuals": {
      "tradeoffMatrix": {
        "title": "Tradeoff matrix",
        "description": "Visual comparison of technology choices with their benefits and costs.",
        "imageSrc": "/images/docs/overview/system-map.svg",
        "imageAlt": "Technology tradeoff matrix",
        "caption": "System architecture overview illustrating the technology stack decisions."
      }
    },
    "sections": {
      "tauri": {
        "title": "Tauri v2 for Desktop",
        "description": "Tauri provides a Rust backend with a web-based frontend, enabling cross-platform desktop apps with native performance and small binary sizes.",
        "benefits": [
          "Small binary size (~15MB vs 200MB+ for Electron)",
          "Native Rust performance for file operations and job processing",
          "Capability-based security model with fine-grained permissions",
          "Single codebase for macOS, Windows, and Linux",
          "Access to system APIs (PTY, keychain, notifications)"
        ],
        "tradeoffs": [
          "Smaller ecosystem than Electron",
          "Rust learning curve for backend development",
          "WebView rendering differences across platforms",
          "Less mature tooling for debugging IPC issues"
        ],
        "implementation": "PlanToCode uses Tauri 2.9.1 with ~35 command modules, capability-based permissions, and plugins for shell, dialog, and notifications."
      },
      "sqlite": {
        "title": "SQLite for Local Persistence",
        "description": "SQLite stores all local state including sessions, jobs, terminal output, and settings. This enables offline operation and fast queries.",
        "benefits": [
          "Zero-config embedded database",
          "Fast queries for local data",
          "Enables offline operation",
          "Single file backup and restore",
          "WAL mode for concurrent access"
        ],
        "tradeoffs": [
          "No built-in replication or sync",
          "Large terminal logs can grow the database",
          "Need manual schema migrations",
          "Single-writer limitation (mitigated by WAL)"
        ],
        "implementation": "Schema in consolidated_schema.sql with ~10 tables. Repositories provide typed access with rusqlite."
      },
      "llmProxy": {
        "title": "Dedicated LLM Proxy Server",
        "description": "All LLM requests route through a server proxy that manages API keys, normalizes requests, tracks usage, and handles billing.",
        "benefits": [
          "API keys never leave the server",
          "Single request format for all providers",
          "Centralized usage tracking and billing",
          "Provider failover without client updates",
          "Content filtering and rate limiting"
        ],
        "tradeoffs": [
          "Requires server infrastructure",
          "Adds network latency to requests",
          "Server becomes single point of failure",
          "Need to maintain provider integrations"
        ],
        "implementation": "Actix-Web server with handlers in server/src/handlers/proxy/. Transformers in provider_transformers/ normalize requests."
      },
      "websocket": {
        "title": "WebSocket Relay for Mobile",
        "description": "Desktop and mobile clients connect through a WebSocket relay for device linking, terminal streaming, and job synchronization.",
        "benefits": [
          "Real-time bidirectional communication",
          "No direct P2P networking required",
          "Works across NAT and firewalls",
          "Supports multiple linked devices"
        ],
        "tradeoffs": [
          "Requires persistent server connections",
          "Relay adds latency for large payloads",
          "Connection management complexity",
          "Need reconnection and heartbeat logic"
        ],
        "implementation": "device_link_ws.rs implements the relay with session tracking, heartbeats, and PTC1 binary framing for terminal output."
      }
    },
    "operational": {
      "heading": "Operational Consequences",
      "items": [
        "Tauri: Need separate builds for each platform. CI/CD must cross-compile or use platform-specific runners.",
        "SQLite: Database file grows with terminal output. May need periodic cleanup for long-running instances.",
        "LLM Proxy: Server downtime blocks all LLM operations. Need monitoring and redundancy for production.",
        "WebSocket: Reconnection logic adds complexity. Clients must handle connection drops gracefully."
      ]
    },
    "securityBoundaries": {
      "heading": "Security Boundaries",
      "description": "The architecture creates clear security boundaries that limit exposure:",
      "items": [
        "API keys stored in server vault, never sent to clients",
        "JWT tokens validated on every request with JWKS rotation",
        "Capability-based permissions limit filesystem access",
        "Content sent to LLMs requires explicit user approval",
        "Audit logs track all LLM requests with user context"
      ]
    },
    "whenToReconsider": {
      "heading": "When to Reconsider",
      "description": "These decisions may need revisiting if requirements change significantly:",
      "items": [
        "If browser-only access is required, consider a web-based alternative to Tauri",
        "If multi-device sync is critical, consider server-side job storage",
        "If provider lock-in is acceptable, direct API calls may reduce latency",
        "If mobile is primary, consider native apps instead of device linking"
      ]
    }
  },
  "dataModel": {
    "meta": {
      "title": "Data model and storage - PlanToCode",
      "description": "SQLite entities, relationships, and how state is rehydrated on app restart."
    },
    "category": "Architecture",
    "date": "2025-09-25",
    "readTime": "10 min",
    "title": "Data Model & Storage",
    "description": "SQLite entities, relationships, and how state is rehydrated.",
    "intro": "PlanToCode uses SQLite for all local state. This document describes the schema, entity relationships, and how state is restored when the app restarts.",
    "sqlite": {
      "heading": "SQLite Configuration",
      "description": "The database uses WAL mode for concurrent read/write access. The file is stored in the Tauri app data directory (~/.local/share/plantocode on Linux, ~/Library/Application Support/plantocode on macOS).",
      "migrations": "Schema migrations are consolidated in consolidated_schema.sql. The app checks schema version on startup and runs any pending migrations."
    },
    "entities": {
      "heading": "Core Entities",
      "items": [
        "sessions: Project context with task description, file selections, model preferences, search settings, video/merge prompts, history indexes",
        "background_jobs: LLM-backed operations with prompt, response, tokens, cost, is_finalized flag, error_message",
        "terminal_sessions: PTY sessions with output log, status, process info",
        "task_description_history: Version history for task descriptions",
        "file_selection_history: Version history for file selections",
        "project_system_prompts: Per-project prompt overrides",
        "key_value_store: App settings and configuration",
        "error_logs: Client-side error tracking",
        "migrations: Tracks applied database migrations with timestamps",
        "db_diagnostic_logs: Records database diagnostic issues and errors",
        "app_settings: Application configuration key-value pairs with descriptions"
      ]
    },
    "visuals": {
      "schema": {
        "title": "Entity relationship diagram",
        "description": "Visual representation of the SQLite schema and relationships.",
        "imageSrc": "/images/docs/data-model/schema.svg",
        "imageAlt": "Database schema diagram",
        "caption": "Placeholder for database schema diagram."
      }
    },
    "relationships": {
      "heading": "Entity Relationships",
      "description": "Entities are linked through foreign keys with cascade delete rules:",
      "links": [
        "sessions → background_jobs: One-to-many, cascade delete",
        "background_jobs → terminal_sessions: Optional one-to-one link via job_id",
        "sessions → task_description_history: One-to-many for version tracking",
        "sessions → file_selection_history: One-to-many for version tracking"
      ]
    },
    "repositories": {
      "heading": "Repository Layer",
      "description": "All database access goes through typed repositories in desktop/src-tauri/src/db_utils/:",
      "examples": [
        "background_job_repository/: Modular with base.rs, worker.rs, metadata.rs, cleanup.rs",
        "session_repository.rs: Session CRUD with history management",
        "terminal_repository.rs: Terminal session persistence and output logging",
        "settings_repository.rs: Key-value settings storage"
      ]
    },
    "rehydration": {
      "heading": "State Rehydration",
      "description": "When the app starts, state is restored from SQLite:",
      "sessions": "Active session is loaded with task description, file selections, and model preferences. Recent sessions are available in the session picker."
    },
    "retention": {
      "heading": "Data Retention",
      "description": "Old data is cleaned up based on configurable retention periods:",
      "exports": "Sessions and jobs can be exported for backup before cleanup."
    },
    "cta": {
      "heading": "Explore job processing",
      "description": "See how background jobs use this data model.",
      "links": {
        "jobs": "Background jobs",
        "terminals": "Terminal sessions"
      }
    }
  },
  "serverSetup": {
    "meta": {
      "title": "Dedicated server setup - PlanToCode",
      "description": "Ansible-based infrastructure setup: base hardening, PostgreSQL, Redis, and application deployment."
    },
    "category": "Deployment",
    "date": "2025-09-25",
    "readTime": "12 min",
    "title": "Dedicated Server Setup",
    "description": "Ansible-based infrastructure: base hardening, app deployment, and vault-managed secrets.",
    "intro": "PlanToCode runs on dedicated servers managed through Ansible playbooks. This document covers the infrastructure setup, security hardening, and deployment process.",
    "layers": {
      "heading": "Infrastructure Layers",
      "description": "The infrastructure is organized into layers, each managed by dedicated playbooks:",
      "items": [
        "Base layer: OS hardening, SSH configuration, firewall rules",
        "Database layer: PostgreSQL 17 with replication and backups",
        "Cache layer: Redis 7+ for session state and job queues",
        "Application layer: Rust server binary with systemd service",
        "Proxy layer: Nginx reverse proxy with SSL termination"
      ]
    },
    "servers": {
      "heading": "Server Regions",
      "description": "PlanToCode runs in two regions for geographic redundancy:",
      "items": [
        "EU region: Hetzner dedicated server (api-eu.plantocode.com)",
        "US region: InterServer dedicated server (api-us.plantocode.com)"
      ]
    },
    "requirements": {
      "heading": "Server Requirements",
      "items": [
        "Debian 12 or Ubuntu 22.04 LTS",
        "4+ CPU cores, 16GB+ RAM, 200GB+ SSD",
        "Public IPv4 with firewall access to ports 22, 80, 443",
        "SSH key access for Ansible deployment"
      ]
    },
    "hardening": {
      "heading": "Base Hardening",
      "description": "site-base.yml applies security hardening:",
      "items": [
        "Disable root SSH login, require key authentication",
        "Configure UFW firewall with minimal open ports",
        "Install fail2ban for brute force protection",
        "Enable automatic security updates",
        "Configure audit logging"
      ]
    },
    "postgresql": {
      "heading": "PostgreSQL Setup",
      "description": "PostgreSQL 17 is configured for production use:",
      "items": [
        "Connection pooling with PgBouncer",
        "Automated daily backups with pg_dump",
        "WAL archiving for point-in-time recovery",
        "SSL required for all connections",
        "Row-level security for multi-tenant data"
      ]
    },
    "redis": {
      "heading": "Redis Setup",
      "description": "Redis 7+ handles caching and session state:",
      "items": [
        "Password authentication required",
        "AOF persistence for durability",
        "Memory limits with eviction policy",
        "TLS encryption for connections"
      ]
    },
    "zeroDowntime": {
      "heading": "Zero-Downtime Deployment",
      "description": "Deployments use a rolling update strategy:",
      "items": [
        "New binary uploaded alongside running version",
        "Health check confirms new version is ready",
        "Systemd restarts with graceful shutdown",
        "Load balancer drains connections during switch",
        "Rollback available via previous binary symlink"
      ]
    },
    "quickStart": {
      "heading": "Quick Start",
      "steps": [
        "Clone the infrastructure repository",
        "Copy inventory.example to inventory and configure hosts",
        "Set vault password in .vault_pass",
        "Run: ansible-playbook -i inventory site-base.yml",
        "Run: ansible-playbook -i inventory site-app.yml"
      ]
    },
    "vault": {
      "heading": "Secrets Management",
      "description": "Sensitive configuration uses Ansible Vault:",
      "items": [
        "Database credentials",
        "API keys for LLM providers",
        "SSL certificates and private keys",
        "Auth0 client secrets",
        "Stripe webhook secrets"
      ]
    },
    "operations": {
      "heading": "Common Operations",
      "items": [
        "ansible-playbook -i inventory site-app.yml --tags deploy",
        "ansible-playbook -i inventory site-base.yml --tags backup",
        "ansible-playbook -i inventory site-app.yml --tags rollback",
        "ansible-playbook -i inventory site-base.yml --tags logs"
      ]
    },
    "ssl": {
      "heading": "SSL/TLS Configuration",
      "description": "Let's Encrypt provides free SSL certificates:",
      "items": [
        "Certbot configured with Nginx plugin",
        "Automatic renewal via cron job",
        "HSTS headers enabled",
        "TLS 1.2+ only, modern cipher suite"
      ]
    },
    "security": {
      "heading": "Security Checklist",
      "items": [
        "All default passwords changed",
        "SSH key rotation scheduled",
        "Firewall rules audited",
        "Security updates automated",
        "Backup restoration tested"
      ]
    },
    "recovery": {
      "heading": "Disaster Recovery",
      "description": "Recovery procedures for common failure scenarios:",
      "items": [
        "Database corruption: Restore from latest pg_dump backup",
        "Server failure: Provision new server and run playbooks",
        "SSL expiration: Manual certbot renew --force-renewal",
        "Security breach: Rotate all credentials, audit logs"
      ]
    }
  },
  "tauriV2": {
    "meta": {
      "title": "Tauri v2 development guide - PlanToCode",
      "description": "Project layout, commands, capabilities, and development workflow for Tauri v2."
    },
    "category": "Deployment",
    "date": "2025-09-25",
    "readTime": "10 min",
    "title": "Tauri v2 Development Guide",
    "description": "Project layout, commands, and capability-based permissions for Tauri v2.",
    "intro": "PlanToCode uses Tauri v2 for the desktop application. This guide covers the project structure, command system, capability-based permissions, and development workflow.",
    "projectLayout": {
      "heading": "Project Layout",
      "description": "The desktop application follows standard Tauri v2 conventions:",
      "items": [
        "desktop/src/: React frontend with components, hooks, and providers",
        "desktop/src-tauri/: Rust backend with commands, jobs, and services",
        "desktop/src-tauri/src/lib.rs: Application entry point",
        "desktop/src-tauri/src/commands/: Tauri command handlers (~35 modules)",
        "desktop/src-tauri/capabilities/: Permission definitions",
        "desktop/src-tauri/tauri.conf.json: Tauri configuration"
      ]
    },
    "configuration": {
      "heading": "Tauri Configuration",
      "description": "tauri.conf.json configures the application:",
      "items": [
        "productName, version, identifier for app metadata",
        "build.beforeDevCommand and beforeBuildCommand for frontend",
        "bundle settings for installers (DMG, NSIS, AppImage)",
        "security.csp for Content Security Policy",
        "plugins configuration for official plugins"
      ]
    },
    "capabilities": {
      "heading": "Capability-Based Permissions",
      "description": "Tauri v2 uses capabilities to control what the app can access:",
      "items": [
        "default.json: Base permissions for all windows",
        "desktop-default.json: Desktop-specific permissions",
        "plantocode-api.json: Custom permissions for PlanToCode commands",
        "Permissions grant access to: filesystem, shell, http, dialog, notification"
      ]
    },
    "plugins": {
      "heading": "Tauri Plugins",
      "description": "PlanToCode uses several official Tauri plugins:",
      "items": [
        "tauri-plugin-http: HTTP client for API calls",
        "tauri-plugin-dialog: Native file/folder pickers",
        "tauri-plugin-shell: Shell command execution",
        "tauri-plugin-store: Persistent key-value storage",
        "tauri-plugin-notification: Desktop notifications",
        "tauri-plugin-updater: In-app updates",
        "tauri-plugin-single-instance: Single instance enforcement"
      ]
    },
    "appState": {
      "heading": "Application State",
      "description": "Rust state managed through Tauri's state system:",
      "items": [
        "AppState struct holds shared state",
        "RuntimeConfig for server URLs and feature flags",
        "TokenManager for secure credential storage",
        "ConfigCache for AI model configuration"
      ]
    },
    "commands": {
      "heading": "Creating Commands",
      "description": "Tauri commands expose Rust functions to the frontend:",
      "items": [
        "Use #[tauri::command] attribute on async functions",
        "Return Result<T, String> for error handling",
        "Access state via State<AppState> parameter",
        "Register in lib.rs invoke_handler"
      ]
    },
    "singleInstance": {
      "heading": "Single Instance",
      "description": "The app enforces single instance to prevent data conflicts:",
      "items": [
        "tauri-plugin-single-instance handles detection",
        "Second launch focuses existing window",
        "Deep links forwarded to running instance"
      ]
    },
    "devWorkflow": {
      "heading": "Development Workflow",
      "description": "Common commands for development:",
      "items": [
        "pnpm tauri dev: Start development with hot reload",
        "pnpm tauri build: Build production release",
        "cargo test: Run Rust tests",
        "cargo clippy: Lint Rust code"
      ]
    },
    "mobile": {
      "heading": "Mobile Considerations",
      "description": "Tauri v2 supports mobile, but PlanToCode uses native Swift:",
      "items": [
        "iOS app built with SwiftUI for native experience",
        "Shared API contracts between desktop and mobile",
        "Device linking via WebSocket relay"
      ]
    },
    "distribution": {
      "heading": "Distribution",
      "description": "Build artifacts for each platform:",
      "items": [
        "macOS: .dmg with universal binary (Intel + Apple Silicon)",
        "Windows: NSIS installer and MSIX package",
        "Linux: AppImage for broad compatibility"
      ]
    }
  },
  "distributionMacos": {
    "meta": {
      "title": "macOS distribution - PlanToCode",
      "description": "Code signing, notarization, DMG packaging, and updater configuration for macOS."
    },
    "category": "Deployment",
    "date": "2025-09-25",
    "readTime": "10 min",
    "title": "macOS Distribution",
    "description": "Signing, notarization, DMG packaging, and updater artifacts.",
    "intro": "Distributing on macOS requires code signing, notarization, and proper packaging. This document covers the complete process for PlanToCode.",
    "signing": {
      "heading": "Code Signing",
      "description": "All binaries must be signed with an Apple Developer ID:",
      "items": [
        "Developer ID Application certificate for app signing",
        "Developer ID Installer certificate for PKG signing",
        "Certificates stored in CI secrets, imported to keychain",
        "Hardened runtime enabled for notarization compatibility"
      ]
    },
    "entitlements": {
      "heading": "Entitlements",
      "description": "Required entitlements for PlanToCode features:",
      "items": [
        "com.apple.security.cs.allow-jit",
        "com.apple.security.cs.allow-unsigned-executable-memory",
        "com.apple.security.device.audio-input",
        "com.apple.security.network.client",
        "com.apple.security.files.user-selected.read-write"
      ]
    },
    "build": {
      "heading": "Build Process",
      "description": "Steps to build a signed release:",
      "steps": [
        "Run pnpm tauri build --target universal-apple-darwin",
        "Tauri signs with APPLE_SIGNING_IDENTITY from environment",
        "Universal binary created with lipo for Intel + ARM",
        "DMG packaged with custom background and layout"
      ]
    },
    "universalBinaries": {
      "heading": "Universal Binaries",
      "description": "PlanToCode ships as a universal binary:",
      "items": [
        "Single .app supports both Intel and Apple Silicon",
        "Built with --target universal-apple-darwin",
        "Slightly larger binary but simpler distribution",
        "Native performance on both architectures"
      ]
    },
    "notarization": {
      "heading": "Notarization",
      "description": "Apple notarization is required for Gatekeeper approval:",
      "items": [
        "DMG submitted to Apple notary service",
        "Uses notarytool with App Store Connect credentials",
        "Stapling attaches notarization ticket to DMG",
        "Process takes 1-5 minutes typically"
      ]
    },
    "updater": {
      "heading": "In-App Updates",
      "description": "tauri-plugin-updater handles automatic updates:",
      "items": [
        "Checks update endpoint on launch",
        "Downloads new version in background",
        "Prompts user to restart to apply",
        "Signature verification before installation"
      ]
    },
    "latestJson": {
      "heading": "Update Manifest",
      "description": "latest.json describes available updates:",
      "items": [
        "version: Semantic version string",
        "platforms.darwin-universal: URL and signature",
        "notes: Release notes in markdown",
        "pub_date: ISO 8601 publish timestamp"
      ]
    },
    "pitfalls": {
      "heading": "Common Pitfalls",
      "description": "Issues frequently encountered:",
      "items": [
        "Keychain locked during CI: Unlock before signing",
        "Notarization timeout: Retry with exponential backoff",
        "Invalid signature: Check entitlements match capabilities",
        "Gatekeeper rejection: Verify notarization stapled correctly"
      ]
    },
    "verification": {
      "heading": "Verification Commands",
      "description": "Commands to verify signing and notarization:",
      "items": [
        "codesign -dv --verbose=4 PlanToCode.app",
        "spctl --assess --verbose PlanToCode.app",
        "stapler validate PlanToCode.dmg",
        "xcrun notarytool log <submission-id>"
      ]
    }
  },
  "distributionWindows": {
    "meta": {
      "title": "Windows distribution - PlanToCode",
      "description": "NSIS installer, MSIX packaging, Microsoft Store submission, and code signing for Windows."
    },
    "category": "Deployment",
    "date": "2025-09-25",
    "readTime": "10 min",
    "title": "Windows Distribution & Store",
    "description": "NSIS builds, MSIX packaging, and Microsoft Store submission.",
    "intro": "PlanToCode distributes on Windows through both direct download (NSIS installer) and the Microsoft Store (MSIX package). This document covers both distribution methods.",
    "prereqs": {
      "heading": "Prerequisites",
      "description": "Required tools and certificates:",
      "items": [
        "Code signing certificate (EV or standard)",
        "Windows SDK for signtool",
        "NSIS for installer building",
        "MSIX Packaging Tool for Store submissions"
      ]
    },
    "nsisBuild": {
      "heading": "NSIS Installer",
      "description": "Tauri builds NSIS installers by default:",
      "items": [
        "Custom installer UI with PlanToCode branding",
        "Per-user installation (no admin required)",
        "Start menu and desktop shortcuts",
        "Uninstaller with clean removal"
      ]
    },
    "codeSigning": {
      "heading": "Code Signing",
      "description": "Windows code signing with Authenticode:",
      "items": [
        "Sign with signtool from Windows SDK",
        "Timestamp from trusted TSA server",
        "EV certificate provides SmartScreen reputation",
        "CI uses secrets for certificate and password"
      ]
    },
    "msixPackaging": {
      "heading": "MSIX for Microsoft Store",
      "description": "MSIX provides Store-compatible packaging:",
      "items": [
        "AppxManifest.xml defines capabilities",
        "Virtual filesystem isolation",
        "Automatic updates through Store",
        "Sandboxed execution environment"
      ]
    },
    "msixConfig": {
      "heading": "MSIX Configuration",
      "description": "Key AppxManifest settings:",
      "items": [
        "Identity: Name, Publisher, Version",
        "Capabilities: internetClient, microphone",
        "Visual elements: Tiles, splash screen",
        "File associations and protocol handlers"
      ]
    },
    "msixSteps": {
      "heading": "MSIX Build Steps",
      "description": "Process to create MSIX package:",
      "steps": [
        "Build release with pnpm tauri build",
        "Create AppxManifest.xml with correct identity",
        "Package with MakeAppx.exe",
        "Sign with SignTool",
        "Validate with Windows App Cert Kit"
      ]
    },
    "store": {
      "heading": "Microsoft Store Submission",
      "description": "Store submission process:",
      "items": [
        "Create app in Partner Center",
        "Upload MSIX package",
        "Configure pricing (free with IAP credits)",
        "Submit for certification",
        "Review takes 1-3 business days"
      ]
    },
    "updaterWindows": {
      "heading": "Windows Updates",
      "description": "Update mechanisms for each distribution:",
      "items": [
        "NSIS: tauri-plugin-updater with GitHub releases",
        "MSIX/Store: Automatic through Microsoft Store",
        "Both check for updates on launch"
      ]
    },
    "webview2": {
      "heading": "WebView2 Runtime",
      "description": "Tauri uses WebView2 on Windows:",
      "items": [
        "Bundled WebView2 bootstrapper in installer",
        "Evergreen version auto-updates",
        "Fixed version available for isolation",
        "Windows 10 1803+ required"
      ]
    },
    "troubleshooting": {
      "heading": "Troubleshooting",
      "description": "Common Windows distribution issues:",
      "items": [
        "SmartScreen warning: Use EV certificate or build reputation",
        "Missing WebView2: Ensure bootstrapper runs",
        "Store rejection: Review certification report details",
        "Update failure: Check signature and manifest version"
      ]
    }
  },
  "promptTypes": {
    "meta": {
      "title": "Prompt types and templates - PlanToCode",
      "description": "Catalog of prompt-driven job types and template assembly process."
    },
    "category": "Reference",
    "date": "2025-09-25",
    "readTime": "8 min",
    "title": "Prompt Types & Templates",
    "description": "Catalog of prompt-driven job types and template assembly.",
    "intro": "Every LLM-backed job in PlanToCode uses a structured prompt built from templates. This document catalogs the job types and explains how prompts are assembled.",
    "catalog": {
      "heading": "Job Type Catalog",
      "items": [
        {
          "job": "implementation_plan",
          "title": "Implementation Plan",
          "description": "Generates file-by-file implementation plans with XML structure. Uses streaming for progressive display."
        },
        {
          "job": "implementation_plan_merge",
          "title": "Plan Merge",
          "description": "Combines multiple plans with user instructions. Source plans wrapped in XML tags."
        },
        {
          "job": "text_improvement",
          "title": "Text Improvement",
          "description": "Refines selected text while preserving formatting. Non-streaming for quick results."
        },
        {
          "job": "root_folder_selection",
          "title": "Root Folder Selection",
          "description": "Analyzes directory tree to select relevant project roots. Returns JSON array."
        },
        {
          "job": "regex_file_filter",
          "title": "Regex File Filter",
          "description": "Generates regex patterns for file filtering based on task description."
        },
        {
          "job": "file_relevance_assessment",
          "title": "File Relevance Assessment",
          "description": "Scores file content relevance to task. Processes in batches."
        },
        {
          "job": "extended_path_finder",
          "title": "Extended Path Finder",
          "description": "Discovers related files through imports and dependencies."
        },
        {
          "job": "web_search_prompts",
          "title": "Web Search Prompts",
          "description": "Generates research queries for deep research workflow."
        },
        {
          "job": "video_analysis",
          "title": "Video Analysis",
          "description": "Analyzes screen recordings for UI state and action sequences."
        }
      ]
    },
    "templateStructure": {
      "heading": "Template Structure",
      "description": "Prompts are assembled from system templates and user content:",
      "sampleLabel": "Example template structure:",
      "sample": "<system_prompt>\n  You are an AI assistant that generates implementation plans.\n  [template content from server]\n</system_prompt>\n\n<task>\n  [user's task description]\n</task>\n\n<files>\n  [selected file paths and content]\n</files>\n\n<directory_tree>\n  [project structure]\n</directory_tree>"
    },
    "visuals": {
      "template": {
        "title": "Prompt assembly flow",
        "description": "How templates combine with user content to form complete prompts.",
        "imageSrc": "/images/docs/implementation-plans/structure.svg",
        "imageAlt": "Prompt template assembly diagram",
        "caption": "Placeholder for prompt assembly diagram."
      }
    },
    "assembly": {
      "heading": "Assembly Process",
      "steps": [
        "Processor retrieves template ID from task model config",
        "System prompt template loaded from server cache",
        "User content wrapped in semantic XML tags",
        "Context (files, tree) added based on job type",
        "Complete prompt stored in job record before sending"
      ]
    },
    "serverConfig": {
      "heading": "Server-Side Configuration",
      "description": "Templates and model settings are configured server-side:",
      "fields": "task_model_config defines: default_model, allowed_models, system_prompt_template_id, max_tokens, temperature"
    },
    "tokenGuards": {
      "heading": "Token Guardrails",
      "description": "Each task type has token limits to prevent context overflow:",
      "items": [
        "max_tokens_input: Maximum prompt size",
        "max_tokens_output: Maximum response size",
        "Validation before sending prevents wasted API calls",
        "UI shows token count and warns when approaching limits"
      ]
    },
    "versioning": {
      "heading": "Template Versioning",
      "description": "System prompt templates are versioned for reproducibility. Each job records the template ID used, enabling traceability and comparison of results across template versions."
    },
    "designNotes": {
      "heading": "Design Notes",
      "items": [
        "XML tags provide clear boundaries for LLM parsing",
        "Semantic naming (task, files, context) aids model understanding",
        "Templates avoid instruction injection by sanitizing user input",
        "Streaming jobs use end tags for completion detection"
      ]
    },
    "cta": {
      "heading": "See job processing in action",
      "description": "Learn how these prompts flow through the job system.",
      "links": {
        "jobs": "Background jobs",
        "merge": "Merge instructions"
      }
    }
  },
  "mergeInstructionsDoc": {
    "meta": {
      "title": "Merge instructions - PlanToCode",
      "description": "How multiple plan drafts are merged using XML-tagged source plans and user guidance."
    },
    "category": "Planning",
    "date": "2025-09-25",
    "readTime": "8 min",
    "title": "Merge Instructions",
    "description": "How multiple plan drafts are merged using XML-tagged source plans and user guidance.",
    "intro": "When you have multiple implementation plans that need to be combined, the merge workflow lets you select plans, provide guidance, and generate a unified plan that incorporates the best elements from each source.",
    "processor": {
      "heading": "ImplementationPlanMergeProcessor",
      "description": "The ImplementationPlanMergeProcessor fetches source plan responses, wraps them in XML-tagged sections, and streams the merged result through the LlmTaskRunner.",
      "payload": "Accepts source_job_ids array, optional merge_instructions string, and inherits model configuration from the session.",
      "storage": "Merged plan stored as JobResultData::Text with metadata including source_job_ids, merge_instructions, source_count, merged_at timestamp, and session context."
    },
    "inputs": {
      "heading": "Merge Inputs",
      "items": [
        "Source plans: 2-5 implementation plans selected from the plan list",
        "Merge instructions: User guidance on how to combine (prioritize, resolve conflicts)",
        "Model selection: LLM model for merge generation",
        "Task context: Original task description for reference"
      ]
    },
    "xmlFormat": {
      "heading": "XML-Tagged Source Plans",
      "description": "Source plans are wrapped in XML tags with sequential identifiers:",
      "example": "<task_description>\n  [original task from session]\n</task_description>\n\n<source_plans>\n  <implementation_plan_1>\n    [full plan content from first source]\n  </implementation_plan_1>\n  <implementation_plan_2>\n    [full plan content from second source]\n  </implementation_plan_2>\n</source_plans>\n\n<user_instructions>\n  Prioritize API structure from plan 1.\n  Use database schema from plan 2.\n  Resolve conflicts by preferring newer patterns.\n</user_instructions>"
    },
    "prompt": {
      "heading": "Merge Prompt Structure",
      "description": "The merge prompt includes all context needed for intelligent combination:",
      "sections": [
        "System prompt with merge guidelines",
        "Source plans in XML tags",
        "User's merge instructions",
        "Task description for context",
        "Output format requirements"
      ]
    },
    "visuals": {
      "mergeWalkthrough": {
        "title": "Merge workflow walkthrough",
        "description": "Video showing the complete merge process from selection to output.",
        "videoSrc": "/videos/docs/merge-instructions/walkthrough.mp4",
        "posterSrc": "/images/docs/merge-instructions/flow.svg",
        "caption": "Placeholder for merge walkthrough video."
      },
      "mergeFlow": {
        "title": "Merge instructions flow",
        "description": "Diagram showing multi-model merge workflow with XML-tagged source plans.",
        "imageSrc": "/images/docs/merge-instructions/flow.svg",
        "caption": "Merge flow showing source selection, instruction processing, and output generation"
      }
    },
    "rules": {
      "heading": "Merge Rules",
      "description": "The LLM follows these rules when merging plans:",
      "examples": [
        "Preserve file paths exactly as specified in source plans",
        "Combine non-conflicting changes from all sources",
        "For conflicts, follow explicit user instructions",
        "Maintain consistent code style across merged content",
        "Include provenance comments indicating source plan"
      ]
    },
    "output": {
      "heading": "Merged Output",
      "description": "The merged plan is returned as raw text from the LLM, following the same flexible format as individual plans.",
      "provenance": "Each section includes comments indicating which source plan contributed the content.",
      "metadata": "source_job_ids, merge_instructions, source_count, merged_at timestamp, planTitle, summary, isStructured (false), and sessionName stored in job metadata."
    },
    "ui": {
      "heading": "UI Integration",
      "description": "The Implementation Plans panel supports merge workflow:",
      "audit": "Merged plans link back to source plans for traceability."
    },
    "cta": {
      "heading": "Learn about plan generation",
      "description": "Understand how individual plans are created before merging.",
      "links": {
        "plans": "Implementation plans",
        "runtime": "Runtime walkthrough"
      }
    }
  },
  "meetingIngestionDoc": {
    "meta": {
      "title": "Meeting and recording ingestion - PlanToCode",
      "description": "How recordings are analyzed into task summaries through the video analysis pipeline."
    },
    "category": "Inputs",
    "date": "2025-09-25",
    "readTime": "8 min",
    "title": "Meeting & Recording Ingestion",
    "description": "How recordings become task summaries and planning inputs.",
    "intro": "PlanToCode can analyze meeting recordings and screen captures with the video analysis job. The model is guided by a system prompt that adapts to your goal, whether you are debugging, reviewing UI, or documenting a workflow.",
    "visuals": {
      "ingestionFlow": {
        "title": "Recording ingestion flow",
        "description": "How recordings flow through upload and analysis.",
        "imageSrc": "/images/docs/deep-research/workflow.svg",
        "imageAlt": "Recording ingestion flow diagram",
        "caption": "Placeholder for ingestion flow diagram."
      }
    },
    "inputs": {
      "heading": "Supported Inputs",
      "description": "The ingestion workflow accepts video recordings captured in the app or uploaded from other tools.",
      "items": [
        "Screen recordings captured in the desktop app",
        "Meeting recordings exported from Zoom, Meet, or Teams (video files)",
        "Design walkthroughs or bug reproductions recorded as video",
        "For audio-only notes, use voice transcription"
      ]
    },
    "uploadProcess": {
      "heading": "Upload Process",
      "description": "Recordings are uploaded to the server as multipart form data for analysis.",
      "stepsHeading": "Processing Steps",
      "steps": [
        "Desktop saves the recording locally and calculates duration",
        "Video file and analysis prompt are uploaded to /api/llm/video/analyze",
        "Server stores the file temporarily and routes it to Gemini video models",
        "Long recordings are split into 2-minute chunks by the desktop and processed in parallel",
        "Analysis summary is returned and stored in the job response"
      ]
    },
    "normalization": {
      "heading": "Format Normalization",
      "description": "Recordings are sent mostly as-is. WebM recordings are remuxed to fix container metadata before analysis.",
      "outputs": "No separate transcript or frame artifacts are generated; the output is a text analysis summary."
    },
    "multimodalAnalysis": {
      "heading": "Multimodal Analysis",
      "description": "Recordings are analyzed with {code} video models, which accept video and audio in a single request.",
      "combined": "The default video analysis system prompt adapts the output to your goal rather than forcing a fixed schema."
    },
    "transcription": {
      "heading": "Audio context",
      "description": "Audio is analyzed as part of the video; the app does not generate a standalone transcript.",
      "attribution": "If spoken content is unclear, the model may mark it as partially visible rather than guessing.",
      "featuresHeading": "Audio analysis notes",
      "features": [
        "Narration steers the summary",
        "Spoken intent and errors can be quoted",
        "No diarization or timestamped transcript"
      ]
    },
    "frames": {
      "heading": "Frame rate hint",
      "description": "FPS is a sampling hint sent with the analysis request. For large files the provider may ignore it.",
      "timestamps": "Long recordings can be chunked to keep analysis responsive."
    },
    "structuredExtraction": {
      "heading": "Structured Extraction",
      "description": "The analysis output is freeform and adapts to your prompt. Typical outputs include:",
      "extractedHeading": "Extracted Elements",
      "items": [
        "Bug reproduction steps and observed errors",
        "UI walkthrough notes and navigation paths",
        "Design feedback or UX issues shown on screen",
        "Suggested fixes or follow-up tasks"
      ]
    },
    "artifacts": {
      "heading": "Analysis Artifacts",
      "description": "Video analysis produces artifacts stored with the job:",
      "items": [
        "analysis_summary: Text summary stored in background_jobs.response",
        "job_metadata: durationMs, framerate, videoPath",
        "chunk_info: chunk boundaries for long recordings (when applicable)"
      ]
    },
    "keyFiles": {
      "heading": "Key Source Files",
      "items": [
        "desktop/src/app/components/generate-prompt/_components/video-recording-dialog.tsx",
        "desktop/src/contexts/screen-recording/Provider.tsx",
        "desktop/src-tauri/src/jobs/processors/video_analysis_processor.rs",
        "server/src/handlers/proxy/specialized/video_analysis.rs",
        "server/src/utils/multipart_utils.rs",
        "server/src/clients/google_client.rs"
      ]
    },
    "handoff": {
      "heading": "Planning Handoff",
      "description": "Video analysis summaries can be incorporated into the task description for planning.",
      "pipeline": "The summary can be refined with text_improvement and task_refinement before file discovery."
    },
    "cta": {
      "heading": "Continue to video analysis",
      "description": "Learn more about how video frames are analyzed.",
      "links": {
        "video": "Video analysis",
        "textImprovement": "Text improvement"
      }
    }
  },
  "videoAnalysisDoc": {
    "meta": {
      "title": "Video analysis - PlanToCode",
      "description": "Adaptive analysis of screen recordings with Gemini video models."
    },
    "category": "Inputs",
    "date": "2025-09-25",
    "readTime": "6 min",
    "title": "Video Analysis",
    "description": "Adaptive analysis and prompts for screen recordings.",
    "intro": "Video analysis sends the recording to Gemini video models with a system prompt that adapts to your goal. The output is a text summary, not a frame-by-frame export or separate transcript.",
    "visuals": {
      "frameNotes": {
        "title": "Video analysis pipeline",
        "description": "How recordings flow through the analysis model.",
        "imageSrc": "/assets/images/demo-video-analysis.jpg",
        "imageAlt": "Video analysis interface",
        "caption": "The video analysis interface showing analysis options."
      }
    },
    "apiEndpoint": {
      "heading": "API Endpoint",
      "endpoint": "Video analysis is handled by {code} on the server. The endpoint accepts multipart form data with the video file and analysis parameters.",
      "payloadHeading": "Payload Fields",
      "payloadFields": [
        "video: The video file",
        "model: Model identifier for analysis (google/* required)",
        "prompt: Task description and optional focus prompt (wrapped in <description> and <video_attention_prompt>)",
        "temperature: Sampling temperature from task settings",
        "durationMs: Recording duration in milliseconds",
        "framerate: Sampling hint (0.1-20 from the UI)",
        "systemPrompt: Composed system prompt (server-generated)"
      ]
    },
    "inputs": {
      "heading": "Supported Input Formats",
      "items": [
        "MP4, WebM, MOV, and AVI are common inputs",
        "Large files may be uploaded with the provider File API",
        "Long recordings are chunked by the desktop app before analysis"
      ]
    },
    "sampling": {
      "heading": "Frame rate hint",
      "description": "FPS is a hint for how densely to sample the video. For large files the provider may ignore it; for long recordings the desktop may downsample when chunking.",
      "fps": "Default recorder rate is 5 FPS. Lower rates reduce cost but may miss rapid UI changes.",
      "parametersHeading": "Sampling Parameters",
      "parameters": [
        "framerate: 0.1-20 selection in the UI (provider requests are clamped to 1-20)",
        "chunking: long recordings split into 2-minute segments",
        "audio: include narration when \"Include dictation\" is enabled"
      ]
    },
    "modelRequirements": {
      "heading": "Model Requirements",
      "format": "Video analysis requires Gemini video models. Model identifiers follow {code} format; only {code} models are supported.",
      "reasoning": "The server restricts video analysis to Google Gemini models that accept video inputs."
    },
    "analysis": {
      "heading": "Analysis Process",
      "description": "The model analyzes the full video (and audio if present) and produces a goal-oriented summary.",
      "prompting": "The default system prompt (default_video_analysis) tells the model to adapt to your goal, quote visible text when relevant, and mark unclear content instead of guessing.",
      "promptElementsHeading": "Prompt Elements",
      "promptElements": [
        "Goal alignment: focus on the user's stated intent",
        "Evidence: quote visible errors, logs, or UI text when relevant",
        "Sequence: describe the order of events or steps shown",
        "Next steps: suggest fixes or follow-up tasks"
      ]
    },
    "outputs": {
      "heading": "Analysis Outputs",
      "items": [
        "Analysis summary text tailored to the prompt",
        "Quoted errors or UI text when visible",
        "Workflow notes describing what happened on screen",
        "Suggested fixes or follow-up tasks"
      ]
    },
    "billing": {
      "heading": "Token Usage & Billing",
      "description": "Video analysis usage and cost are tracked per job using provider-reported tokens or duration-based estimates.",
      "tracked": [
        "tokens_sent: Prompt + video tokens",
        "tokens_received: Analysis response tokens",
        "actual_cost: Computed from model pricing"
      ]
    },
    "storage": {
      "heading": "Result Storage",
      "description": "Analysis results are stored in background_jobs.response with task_type \"video_analysis\". Long recordings may include chunk metadata.",
      "reuse": "Results can be incorporated into task descriptions or used directly in the planning workflow."
    },
    "keyFiles": {
      "heading": "Key Source Files",
      "items": [
        "desktop/src/app/components/generate-prompt/_components/video-recording-dialog.tsx",
        "desktop/src/contexts/screen-recording/Provider.tsx",
        "desktop/src-tauri/src/jobs/processors/video_analysis_processor.rs",
        "server/src/handlers/proxy/specialized/video_analysis.rs",
        "server/src/clients/google_client.rs"
      ]
    },
    "integration": {
      "heading": "Integration with Planning",
      "description": "Video analysis summaries can be appended to the task description for context-aware planning.",
      "followup": "Use text_improvement or task_refinement to polish the summary before file discovery."
    },
    "cta": {
      "heading": "See meeting ingestion",
      "description": "Learn more about how video analysis works.",
      "links": {
        "meeting": "Meeting ingestion",
        "runtime": "Runtime walkthrough"
      }
    }
  },
  "mobileIos": {
    "meta": {
      "title": "iOS client architecture - PlanToCode",
      "description": "Swift workflows, Auth0 login flow, and device-link session management for the iOS companion app."
    },
    "category": "Architecture",
    "date": "2025-09-25",
    "readTime": "12 min",
    "title": "iOS Client Architecture",
    "description": "Swift workflows, Auth0 login flow, and device-link session management.",
    "intro": "The PlanToCode iOS app is a companion client that connects to linked desktop sessions. It provides mobile access to terminal output, job status, and voice transcription while maintaining the desktop as the primary planning workspace.",
    "visuals": {
      "app": {
        "title": "iOS app interface",
        "description": "Screenshots of the iOS app showing device linking and terminal view.",
        "imageSrc": "/images/docs/overview/system-map.svg",
        "imageAlt": "PlanToCode iOS app screenshots",
        "caption": "Placeholder for iOS app screenshots."
      }
    },
    "packageStructure": {
      "heading": "Swift Package Structure",
      "description": "The iOS app is organized into Swift packages:",
      "packages": [
        {
          "name": "Core",
          "path": "mobile/ios/Core/",
          "description": "Business logic and API clients",
          "components": [
            "WorkflowManager",
            "APIClient",
            "MobileSessionManager",
            "DeviceLinkClient"
          ]
        },
        {
          "name": "Security",
          "path": "mobile/ios/Security/",
          "description": "Authentication and credential storage",
          "components": [
            "Auth0Manager",
            "KeychainHelper",
            "TokenStore"
          ]
        },
        {
          "name": "VibeUI",
          "path": "mobile/ios/VibeUI/",
          "description": "SwiftUI components and design system",
          "components": [
            "TerminalView",
            "JobListView",
            "SettingsView",
            "DeviceLinkView"
          ]
        }
      ]
    },
    "auth": {
      "heading": "Auth0 PKCE Integration",
      "description": "The iOS app uses Auth0 with PKCE flow for secure authentication:",
      "flow": [
        "User taps Sign In, app generates code verifier and challenge",
        "ASWebAuthenticationSession opens Auth0 login page",
        "User authenticates and Auth0 redirects with authorization code",
        "App exchanges code for tokens using code verifier",
        "Tokens stored securely in iOS Keychain"
      ],
      "tokenManagement": {
        "heading": "Token Management",
        "items": [
          "Access token used for API requests",
          "Refresh token stored for silent renewal",
          "Token refresh triggered before expiry",
          "Logout clears all tokens from Keychain"
        ]
      }
    },
    "deviceLink": {
      "heading": "Device Linking via WebSocket Relay",
      "description": "iOS connects to desktop sessions through the server's WebSocket relay:",
      "protocol": {
        "heading": "Linking Protocol",
        "steps": [
          "Desktop generates link code and displays QR",
          "iOS scans QR or enters code manually",
          "Both connect to /ws/device-link with credentials",
          "Server validates and establishes relay",
          "Bidirectional communication enabled"
        ]
      },
      "messageTypes": {
        "heading": "Message Types",
        "items": [
          "terminal_output: PTY output from desktop terminal",
          "job_status: Background job status updates",
          "session_sync: Session state synchronization",
          "rpc_command: Commands from mobile to desktop"
        ]
      },
      "reconnection": {
        "heading": "Reconnection Handling",
        "description": "The WebSocket connection handles network interruptions with automatic reconnection, exponential backoff, and session state recovery."
      }
    },
    "rpcRouting": {
      "heading": "RPC Command Routing",
      "description": "iOS can send commands to the linked desktop:",
      "commands": {
        "heading": "Supported Commands",
        "items": [
          "send_terminal_input: Send keystrokes to terminal",
          "request_job_status: Get status of specific job",
          "start_voice_transcription: Begin recording on mobile",
          "sync_session: Request full session state"
        ]
      },
      "implementation": {
        "heading": "Implementation",
        "description": "Commands are JSON-RPC messages sent over WebSocket. Desktop validates commands and returns results asynchronously."
      }
    },
    "offlineQueue": {
      "heading": "Offline Action Queue",
      "description": "Actions performed while disconnected are queued for sync:",
      "architecture": {
        "heading": "Queue Architecture",
        "items": [
          "Actions stored in local SQLite database",
          "Queue processed on reconnection",
          "Conflicts resolved with server timestamps",
          "Failed actions reported to user"
        ]
      },
      "supportedActions": {
        "heading": "Supported Offline Actions",
        "items": [
          "Voice transcription recording (stored locally)",
          "Session notes and annotations",
          "Preference changes"
        ]
      }
    },
    "localStorage": {
      "heading": "SQLite Local Storage",
      "description": "iOS uses SQLite for local persistence:",
      "database": {
        "heading": "Database Schema",
        "path": "~/Documents/plantocode.sqlite",
        "tables": [
          "linked_devices: Desktop connections",
          "offline_queue: Pending sync actions",
          "cached_sessions: Recent session data",
          "transcriptions: Local voice recordings"
        ]
      },
      "migrations": {
        "heading": "Migrations",
        "description": "Schema version tracked in user_version pragma. Migrations run on app launch."
      }
    },
    "sessions": {
      "heading": "Mobile Sessions",
      "description": "MobileSessionManager coordinates session state:",
      "lifecycle": [
        "Load last active session on launch",
        "Connect to linked desktop if available",
        "Subscribe to session updates via WebSocket",
        "Cache session data for offline access"
      ]
    },
    "workflows": {
      "heading": "Workflow Entry Points",
      "description": "Key workflows accessible from mobile:",
      "items": [
        "Terminal monitoring: View output, send input",
        "Job status: Track background job progress",
        "Voice capture: Record and transcribe on mobile",
        "Session browsing: Review plans and history"
      ]
    },
    "region": {
      "heading": "Region Settings",
      "description": "iOS respects user region preference for API routing:",
      "implementation": "Region stored in UserDefaults, used to select api-eu.plantocode.com or api-us.plantocode.com for all requests."
    }
  },
  "providerRouting": {
    "meta": {
      "title": "Provider routing and streaming - PlanToCode",
      "description": "How PlanToCode routes LLM requests through a proxy, normalizes responses, and streams tokens to the desktop client."
    },
    "category": "Research & Models",
    "date": "2025-09-24",
    "readTime": "10 min",
    "title": "Provider Routing and Streaming",
    "description": "Routing layer that mediates all external LLM requests with normalization, streaming, and usage tracking.",
    "visuals": {
      "routingMap": {
        "title": "Provider routing map",
        "description": "Diagram of how requests flow from the desktop app to the proxy and out to providers.",
        "imageSrc": "/images/docs/provider-routing/routing-map.svg",
        "imageAlt": "Diagram of provider routing flow from desktop to external providers",
        "caption": "Placeholder for provider routing diagram."
      }
    },
    "cta": {
      "heading": "Continue into model configuration",
      "description": "Model configuration explains how allowed lists and token guardrails are exposed to the UI.",
      "links": {
        "modelConfiguration": "Model configuration",
        "runtimeWalkthrough": "Runtime walkthrough"
      }
    }
  },
  "docs": {
    "meta": {
      "title": "Documentation - PlanToCode",
      "description": "Learn how to plan and ship code changes with PlanToCode: file discovery, implementation plans, terminal sessions, model guardrails, and voice."
    }
  }
}
