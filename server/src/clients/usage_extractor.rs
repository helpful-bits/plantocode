/// Usage extraction and standardization for multi-provider LLM API clients
/// 
/// This module provides a unified interface for extracting usage information from different
/// LLM provider responses. It standardizes token counts and timing information
/// across all supported providers, enabling consistent usage tracking.
/// 
/// The system supports:
/// - Token-based usage tracking (prompt, completion, cache tokens)
/// - Duration tracking for performance analysis
/// 
/// IMPORTANT: prompt_tokens ALWAYS represents TOTAL input tokens (uncached + cache_write + cache_read)
/// 
/// # Usage
/// 
/// Each provider client should implement the `UsageExtractor` trait to extract
/// usage information from their specific response formats:
/// 
/// ```rust
/// use crate::clients::usage_extractor::{UsageExtractor, ProviderUsage};
/// 
/// impl UsageExtractor for MyProviderClient {
///     fn extract_usage(&self, raw_json: &serde_json::Value) -> Option<ProviderUsage> {
///         // Extract usage from provider-specific JSON response
///         let usage = raw_json.get("usage")?;
///         Some(ProviderUsage::new(
///             usage.get("input_tokens")?.as_i64()? as i32,
///             usage.get("output_tokens")?.as_i64()? as i32,
///             0,
///             usage.get("cached_tokens")?.as_i64().unwrap_or(0) as i32,
///             "model-id".to_string()
///         ))
///     }
/// }
/// ```

use serde::{Deserialize, Serialize};
use tracing::debug;
use bigdecimal::{BigDecimal, Signed};
use crate::models::usage_metadata::UsageMetadata;

/// Standardized usage information extracted from provider responses
/// 
/// This struct provides a unified representation of usage metrics across all
/// LLM providers, enabling consistent usage tracking.
/// 
/// # TOKEN SEMANTIC CONTRACT
/// 
/// ## Core Fields
/// 
/// - `prompt_tokens`: **TOTAL INPUT TOKENS** processed by the model
///   - This is the sum of ALL input tokens: uncached + cache_write + cache_read
///   - Represents the complete input token count, regardless of cache status
///   - For providers that report uncached tokens separately, this field contains the total
///   - **NEVER** contains just the uncached portion - it's always the complete input count
///   - **INVARIANT**: prompt_tokens >= cache_write_tokens + cache_read_tokens
/// 
/// - `completion_tokens`: Output tokens generated by the model (the response)
///   - These are the tokens in the assistant's response
///   - Does not include any cached content
/// 
/// ## Cache Token Fields (For Billing Breakdown Only)
/// 
/// - `cache_write_tokens`: Tokens written to provider cache
///   - Used for detailed billing calculations
///   - Part of the total represented in `prompt_tokens`
///   - May have different pricing than regular input tokens
/// 
/// - `cache_read_tokens`: Tokens read from provider cache  
///   - Used for detailed billing calculations
///   - Part of the total represented in `prompt_tokens`
///   - Often has significantly lower pricing than regular input tokens
/// 
/// ## Important Implementation Notes
/// 
/// 1. **Total Input Calculation**: When implementing UsageExtractor for a provider:
///    - If the provider reports total input directly, use that for `prompt_tokens`
///    - If the provider reports components separately, calculate: 
///      `prompt_tokens = uncached_input + cache_write + cache_read`
/// 
/// 2. **Cache Token Semantics**: The cache token fields (`cache_write_tokens`, `cache_read_tokens`)
///    are ONLY for billing breakdown purposes. They help calculate costs with different rates
///    but do NOT affect the total token count which is always in `prompt_tokens`.
/// 
/// 3. **Billing Calculation**: The cost calculation system uses:
///    - `prompt_tokens` to verify total input
///    - Cache token fields to apply different pricing rates
///    - Formula: `uncached = prompt_tokens - cache_write_tokens - cache_read_tokens`
/// 
/// ## Provider Mappings
/// 
/// ### Anthropic
/// - `input_tokens` (total) → `prompt_tokens`
/// - `output_tokens` → `completion_tokens`  
/// - `cache_creation_input_tokens` → `cache_write_tokens`
/// - `cache_read_input_tokens` → `cache_read_tokens`
/// 
/// ### OpenAI
/// - `prompt_tokens` (total) → `prompt_tokens`
/// - `completion_tokens` → `completion_tokens`
/// - `prompt_tokens_details.cached_tokens` → `cache_read_tokens`
/// - Cache writes not explicitly reported
/// 
/// ### Google
/// - `promptTokenCount` (total) → `prompt_tokens`
/// - `candidatesTokenCount` → `completion_tokens`
/// - `cachedContentTokenCount` → `cache_read_tokens`
/// - Cache writes not explicitly reported
/// 
/// ### OpenRouter
/// - `prompt_tokens` (total) → `prompt_tokens`
/// - `completion_tokens` → `completion_tokens`
/// - Cache tokens may be in usage details
/// 
/// ## Database Mappings
/// 
/// - `prompt_tokens` → `tokens_sent` (BackgroundJob), `tokens_input` (ApiUsageEntry)
/// - `completion_tokens` → `tokens_received` (BackgroundJob), `tokens_output` (ApiUsageEntry)
/// - Cache tokens stored separately for billing details
/// 
/// ## Example
/// 
/// ```rust
/// // Provider reports: uncached=500, cache_write=200, cache_read=300
/// let mut usage = ProviderUsage::new(
///     1000,    // Total: 500 + 200 + 300
///     150,     // Output tokens
///     200,     // For billing breakdown
///     300,     // For billing breakdown
///     "claude-3".to_string()
/// );
/// usage.set_duration(1500);
/// // cost is calculated by the server
/// ```
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct ProviderUsage {
    /// Total number of input tokens processed by the model
    /// 
    /// **CRITICAL**: This field represents the TOTAL INPUT TOKENS (uncached + cache_write + cache_read)
    /// It is NOT just the uncached tokens - it's the complete input token count.
    /// For providers that support caching, this is the sum of all input token types.
    /// 
    /// This design ensures consistent usage tracking across all providers, whether they
    /// support caching or not. Billing systems can rely on this field for the total
    /// input token count.
    pub prompt_tokens: i32,
    
    /// Number of output/completion tokens generated by the model
    /// 
    /// These are the tokens in the assistant's response. This count does not include
    /// any cached content - it's purely the newly generated tokens.
    pub completion_tokens: i32,
    
    /// Number of tokens written to the provider's cache system
    /// 
    /// This field is for billing breakdown purposes only. These tokens are already
    /// included in the `prompt_tokens` total. Providers may charge different rates
    /// for cache writes compared to regular input tokens.
    /// 
    /// Not all providers support or report cache writes explicitly.
    pub cache_write_tokens: i32,
    
    /// Number of tokens read from the provider's cache system
    /// 
    /// This field is for billing breakdown purposes only. These tokens are already
    /// included in the `prompt_tokens` total. Cache reads typically have significantly
    /// lower pricing than regular input tokens.
    /// 
    /// Not all providers support or report cache reads explicitly.
    pub cache_read_tokens: i32,
    
    /// The model ID that was used for this request
    /// 
    /// This should match the model ID used in the request, which helps with
    /// cost calculation and usage attribution.
    pub model_id: String,
    
    /// Duration of the request in milliseconds
    /// 
    /// Used for performance monitoring and analysis. This measures the total
    /// time from request initiation to response completion.
    pub duration_ms: Option<i64>,
    
    /// Cost of the request as calculated by the provider
    /// 
    /// This preserves provider-calculated costs when available for auditing purposes.
    /// However, the server ALWAYS calculates its own authoritative costs for billing.
    /// Provider costs are logged but never used for actual billing.
    /// 
    /// Using BigDecimal for precise financial calculations without floating-point errors.
    pub cost: Option<BigDecimal>,
    
    /// Comprehensive metadata for future billing models and auditing
    /// 
    /// This field captures all additional token types and provider-specific data
    /// that isn't part of the core billing calculation but may be needed for:
    /// - Future billing models (reasoning tokens pricing)
    /// - Multimodal usage tracking (audio, image tokens)
    /// - Provider cost reconciliation
    /// - Usage analytics and optimization
    pub metadata: Option<UsageMetadata>,
}

impl ProviderUsage {
    /// Create a new ProviderUsage with explicit total input tokens
    /// 
    /// This constructor makes it clear that prompt_tokens represents the TOTAL
    /// input tokens, not just uncached tokens. Use this for all new implementations.
    /// 
    /// # Arguments
    /// 
    /// * `total_input_tokens` - TOTAL number of input tokens (uncached + cache_write + cache_read)
    /// * `completion_tokens` - Number of output tokens
    /// * `cache_write_tokens` - Number of tokens written to cache (for billing breakdown)
    /// * `cache_read_tokens` - Number of tokens read from cache (for billing breakdown)
    /// * `model_id` - The model ID used for this request
    /// 
    /// # Example
    /// 
    /// ```rust
    /// // If provider reports: uncached=100, cache_write=25, cache_read=75
    /// let usage = ProviderUsage::new(
    ///     200,  // total_input_tokens (100 + 25 + 75)
    ///     50,   // completion_tokens
    ///     25,   // cache_write_tokens
    ///     75,   // cache_read_tokens
    ///     "claude-3-sonnet".to_string()
    /// );
    /// assert_eq!(usage.prompt_tokens, 200);
    /// assert_eq!(usage.total_input_tokens(), 200);
    /// ```
    pub fn new(
        total_input_tokens: i32,
        completion_tokens: i32,
        cache_write_tokens: i32,
        cache_read_tokens: i32,
        model_id: String,
    ) -> Self {
        Self {
            prompt_tokens: total_input_tokens,
            completion_tokens,
            cache_write_tokens,
            cache_read_tokens,
            model_id,
            duration_ms: None,
            cost: None,
            metadata: None,
        }
    }
    
    
    
    
    /// Get the total number of input tokens
    /// 
    /// Since `prompt_tokens` already represents the TOTAL input tokens,
    /// this method simply returns that value. The separate cache token
    /// fields are for detailed billing breakdowns only.
    /// 
    /// # Returns
    /// 
    /// The total number of input tokens (same as prompt_tokens)
    /// 
    /// # Note for Implementers
    /// 
    /// When implementing UsageExtractor, ensure that prompt_tokens contains
    /// the sum of ALL input tokens. If your provider reports components separately,
    /// calculate: prompt_tokens = uncached + cache_write + cache_read
    pub fn total_input_tokens(&self) -> i32 {
        // prompt_tokens already contains the total input tokens
        // It should equal uncached + cache_write + cache_read
        self.prompt_tokens
    }
    
    /// Get the total number of tokens (input + output)
    /// 
    /// This is the sum of all tokens involved in the request, which is useful
    /// for providers that bill based on total token usage.
    pub fn total_tokens(&self) -> i32 {
        self.prompt_tokens + self.completion_tokens
    }
    
    /// Update the duration information
    /// 
    /// This method allows adding timing information after the request completes,
    /// which is useful for performance monitoring.
    /// 
    /// # Arguments
    /// 
    /// * `duration_ms` - Request duration in milliseconds
    pub fn set_duration(&mut self, duration_ms: i64) {
        self.duration_ms = Some(duration_ms);
    }
    
    /// Set the cost information
    /// 
    /// This method allows adding provider-calculated cost information
    /// which is useful for accurate billing and cost tracking.
    /// 
    /// # Arguments
    /// 
    /// * `cost` - Request cost as calculated by the provider
    pub fn set_cost(&mut self, cost: BigDecimal) {
        self.cost = Some(cost);
    }
    
    
    
    /// Create a new ProviderUsage with provider-calculated cost information
    /// 
    /// This constructor creates a ProviderUsage instance with cost information
    /// provided by the LLM provider. Cost information is preserved for auditing
    /// purposes but is not used for billing calculations.
    /// 
    /// # Arguments
    /// 
    /// * `total_input_tokens` - TOTAL number of input tokens (uncached + cache_write + cache_read)
    /// * `completion_tokens` - Number of output tokens
    /// * `cache_write_tokens` - Number of tokens written to cache (for billing breakdown)
    /// * `cache_read_tokens` - Number of tokens read from cache (for billing breakdown)
    /// * `model_id` - The model ID used for this request
    /// * `cost` - Provider-calculated cost
    /// 
    /// # Example
    /// 
    /// ```rust
    /// use bigdecimal::BigDecimal;
    /// use std::str::FromStr;
    /// 
    /// let cost = BigDecimal::from_str("0.0025").unwrap();
    /// let usage = ProviderUsage::with_cost(
    ///     200,  // total_input_tokens
    ///     50,   // completion_tokens
    ///     25,   // cache_write_tokens
    ///     75,   // cache_read_tokens
    ///     "claude-3-sonnet".to_string(),
    ///     cost
    /// );
    /// ```
    pub fn with_cost(
        total_input_tokens: i32,
        completion_tokens: i32,
        cache_write_tokens: i32,
        cache_read_tokens: i32,
        model_id: String,
        cost: BigDecimal,
    ) -> Self {
        Self {
            prompt_tokens: total_input_tokens,
            completion_tokens,
            cache_write_tokens,
            cache_read_tokens,
            model_id,
            duration_ms: None,
            cost: Some(cost),
            metadata: None,
        }
    }
    
    /// Validate that token counts are non-negative and semantically correct
    /// 
    /// This method performs comprehensive validation to ensure usage data integrity:
    /// - All token counts must be non-negative
    /// - Cache write + cache read tokens cannot exceed total prompt tokens
    /// - Cost values must be non-negative and finite if present
    /// - Duration must be non-negative if present
    /// 
    /// # Returns
    /// 
    /// * `Ok(())` - All values are valid
    /// * `Err(String)` - Description of the validation error
    /// 
    /// # Example
    /// 
    /// ```rust
    /// let usage = ProviderUsage::new(100, 50, 20, 30, "test-model".to_string());
    /// assert!(usage.validate().is_ok());
    /// 
    /// let invalid_usage = ProviderUsage::new(-10, 50, 0, 0, "test".to_string());
    /// assert!(invalid_usage.validate().is_err());
    /// ```
    pub fn validate(&self) -> Result<(), String> {
        use tracing::warn;
        
        // Check all token counts are non-negative
        if self.prompt_tokens < 0 {
            let msg = format!("Invalid prompt_tokens: {}", self.prompt_tokens);
            warn!("{}", msg);
            return Err(msg);
        }
        if self.completion_tokens < 0 {
            let msg = format!("Invalid completion_tokens: {}", self.completion_tokens);
            warn!("{}", msg);
            return Err(msg);
        }
        if self.cache_write_tokens < 0 {
            let msg = format!("Invalid cache_write_tokens: {}", self.cache_write_tokens);
            warn!("{}", msg);
            return Err(msg);
        }
        if self.cache_read_tokens < 0 {
            let msg = format!("Invalid cache_read_tokens: {}", self.cache_read_tokens);
            warn!("{}", msg);
            return Err(msg);
        }
        
        // Validate that cache write + cache read <= prompt tokens
        if self.cache_write_tokens + self.cache_read_tokens > self.prompt_tokens {
            let msg = format!(
                "Cache tokens ({} write + {} read = {}) exceed total prompt tokens ({})",
                self.cache_write_tokens, self.cache_read_tokens, 
                self.cache_write_tokens + self.cache_read_tokens, self.prompt_tokens
            );
            warn!("{}", msg);
            return Err(msg);
        }
        
        // Validate duration if present
        if let Some(duration) = self.duration_ms {
            if duration < 0 {
                let msg = format!("Invalid duration_ms: {}", duration);
                warn!("{}", msg);
                return Err(msg);
            }
        }
        
        // Validate cost if present - must be non-negative and finite
        if let Some(cost) = &self.cost {
            if cost.is_negative() {
                let msg = "Invalid cost: cost cannot be negative".to_string();
                warn!("{}", msg);
                return Err(msg);
            }
            // Check for invalid numeric values (NaN equivalent for BigDecimal)
            // BigDecimal doesn't have NaN, but we can check for invalid states
            if cost.as_bigint_and_exponent().0.to_string().is_empty() {
                let msg = "Invalid cost: cost cannot be NaN or infinite".to_string();
                warn!("{}", msg);
                return Err(msg);
            }
        }
        
        // Validate metadata if present
        if let Some(metadata) = &self.metadata {
            // Validate that all metadata token counts are non-negative
            if let Some(reasoning_tokens) = metadata.reasoning_tokens {
                if reasoning_tokens < 0 {
                    let msg = format!("Invalid metadata reasoning_tokens: {}", reasoning_tokens);
                    warn!("{}", msg);
                    return Err(msg);
                }
            }
            if let Some(thoughts_tokens) = metadata.thoughts_tokens {
                if thoughts_tokens < 0 {
                    let msg = format!("Invalid metadata thoughts_tokens: {}", thoughts_tokens);
                    warn!("{}", msg);
                    return Err(msg);
                }
            }
            if let Some(audio_tokens_input) = metadata.audio_tokens_input {
                if audio_tokens_input < 0 {
                    let msg = format!("Invalid metadata audio_tokens_input: {}", audio_tokens_input);
                    warn!("{}", msg);
                    return Err(msg);
                }
            }
            if let Some(audio_tokens_output) = metadata.audio_tokens_output {
                if audio_tokens_output < 0 {
                    let msg = format!("Invalid metadata audio_tokens_output: {}", audio_tokens_output);
                    warn!("{}", msg);
                    return Err(msg);
                }
            }
            if let Some(image_tokens) = metadata.image_tokens {
                if image_tokens < 0 {
                    let msg = format!("Invalid metadata image_tokens: {}", image_tokens);
                    warn!("{}", msg);
                    return Err(msg);
                }
            }
            if let Some(text_tokens) = metadata.text_tokens {
                if text_tokens < 0 {
                    let msg = format!("Invalid metadata text_tokens: {}", text_tokens);
                    warn!("{}", msg);
                    return Err(msg);
                }
            }
            if let Some(accepted_prediction_tokens) = metadata.accepted_prediction_tokens {
                if accepted_prediction_tokens < 0 {
                    let msg = format!("Invalid metadata accepted_prediction_tokens: {}", accepted_prediction_tokens);
                    warn!("{}", msg);
                    return Err(msg);
                }
            }
            if let Some(rejected_prediction_tokens) = metadata.rejected_prediction_tokens {
                if rejected_prediction_tokens < 0 {
                    let msg = format!("Invalid metadata rejected_prediction_tokens: {}", rejected_prediction_tokens);
                    warn!("{}", msg);
                    return Err(msg);
                }
            }
            if let Some(upstream_inference_cost) = metadata.upstream_inference_cost {
                if upstream_inference_cost < 0.0 {
                    let msg = format!("Invalid metadata upstream_inference_cost: {}", upstream_inference_cost);
                    warn!("{}", msg);
                    return Err(msg);
                }
            }
            if let Some(prompt_tokens_details) = &metadata.prompt_tokens_details {
                for detail in prompt_tokens_details {
                    if detail.token_count < 0 {
                        let msg = format!("Invalid metadata prompt_tokens_details token_count: {}", detail.token_count);
                        warn!("{}", msg);
                        return Err(msg);
                    }
                }
            }
        }
        
        Ok(())
    }
}


/// Trait for extracting standardized usage information from provider responses
/// 
/// This trait must be implemented by each LLM provider client to enable
/// consistent usage tracking across the system. The implementation should
/// parse the provider's response format and extract token counts and timing 
/// information into the standardized `ProviderUsage` struct.
/// 
/// # Implementation Guidelines
/// 
/// 1. **Token Mapping**: Map provider-specific token fields to standard fields:
///    - TOTAL input tokens → `prompt_tokens` (must be sum of all input tokens)
///    - Output/completion tokens → `completion_tokens`
///    - Cached tokens → `cache_read_tokens` (for billing breakdown)
///    - Cache writes → `cache_write_tokens` (for billing breakdown)
/// 
/// 2. **Total Input Calculation**: Ensure `prompt_tokens` contains the TOTAL input tokens:
///    - If provider reports total directly, use that value
///    - If provider reports components separately, sum them: uncached + cache_write + cache_read
/// 
/// 3. **Error Handling**: Return appropriate errors for unparseable responses
/// 
/// 4. **Logging**: Use debug logging to aid in troubleshooting usage extraction
/// 
/// # Example Implementation
/// 
/// ```rust
/// impl UsageExtractor for AnthropicClient {
///     async fn extract_from_response_body(&self, body: &[u8], model_id: &str) -> Result<ProviderUsage, crate::error::AppError> {
///         let body_str = std::str::from_utf8(body)
///             .map_err(|e| crate::error::AppError::InvalidArgument(format!("Invalid UTF-8: {}", e)))?;
///         
///         let json: serde_json::Value = serde_json::from_str(body_str)
///             .map_err(|e| crate::error::AppError::External(format!("Failed to parse JSON: {}", e)))?;
///         
///         let usage = json.get("usage").ok_or_else(|| crate::error::AppError::External("Missing usage field".to_string()))?;
///         
///         // Anthropic reports total input_tokens
///         let total_input = usage.get("input_tokens").and_then(|v| v.as_i64()).ok_or_else(|| crate::error::AppError::External("Missing input_tokens".to_string()))? as i32;
///         let completion_tokens = usage.get("output_tokens").and_then(|v| v.as_i64()).ok_or_else(|| crate::error::AppError::External("Missing output_tokens".to_string()))? as i32;
///         let cache_read = usage.get("cache_read_input_tokens").and_then(|v| v.as_i64()).unwrap_or(0) as i32;
///         let cache_write = usage.get("cache_creation_input_tokens").and_then(|v| v.as_i64()).unwrap_or(0) as i32;
///         
///         let provider_usage = ProviderUsage::new(
///             total_input,      // TOTAL input tokens
///             completion_tokens,
///             cache_write,
///             cache_read,
///             model_id.to_string()
///         );
///         
///         // Validate the extracted usage
///         provider_usage.validate().map_err(|e| crate::error::AppError::InvalidArgument(format!("Validation failed: {}", e)))?;
///         
///         Ok(provider_usage)
///     }
/// }
/// ```
pub trait UsageExtractor {
    /// Extract usage information from provider's HTTP response body
    /// 
    /// This is the primary method for extracting usage from provider responses.
    /// It handles the complete HTTP response body for non-streaming responses
    /// with comprehensive error handling.
    /// 
    /// # Arguments
    /// 
    /// * `body` - The complete HTTP response body as bytes
    /// * `model_id` - The model identifier for this request
    /// 
    /// # Returns
    /// 
    /// * `Ok(ProviderUsage)` - Successfully extracted usage information
    /// * `Err(AppError)` - Unable to extract usage (parse errors, missing fields, etc.)
    /// 
    /// # Implementation Requirements
    /// 
    /// - Parse provider-specific payload formats (JSON, etc.)
    /// - Handle malformed responses gracefully (return error, don't panic)
    /// - Parse complete response bodies only (not streaming chunks)
    /// - Map provider token fields to standard ProviderUsage structure
    /// - Include model_id in the returned ProviderUsage
    /// - **MUST** validate extracted data using `ProviderUsage::validate()` before returning
    /// - Return validation errors as `AppError::InvalidArgument` with descriptive messages
    async fn extract_from_response_body(&self, body: &[u8], model_id: &str) -> Result<ProviderUsage, crate::error::AppError>;

    /// Extract usage information from a parsed JSON value
    /// 
    /// This is a convenience method for extracting usage from already-parsed JSON.
    /// It's commonly used in testing and for processing streaming responses.
    /// 
    /// # Arguments
    /// 
    /// * `raw_json` - The parsed JSON response from the provider
    /// 
    /// # Returns
    /// 
    /// * `Some(ProviderUsage)` - Successfully extracted usage information
    /// * `None` - Unable to extract usage (missing fields, invalid format, etc.)
    /// 
    /// # Implementation Requirements
    /// 
    /// - Parse provider-specific JSON formats
    /// - Handle missing fields gracefully (return None, don't panic)
    /// - Map provider token fields to standard ProviderUsage structure
    /// - **MUST** validate extracted data using `ProviderUsage::validate()`
    /// - Return None if validation fails
    fn extract_usage(&self, raw_json: &serde_json::Value) -> Option<ProviderUsage>;
}

#[cfg(test)]
mod tests {
    use super::*;
    use serde_json::json;
    use std::str::FromStr;

    #[test]
    fn test_provider_usage_new() {
        let usage = ProviderUsage::new(100, 50, 20, 30, "test-model".to_string());
        assert_eq!(usage.prompt_tokens, 100);
        assert_eq!(usage.completion_tokens, 50);
        assert_eq!(usage.cache_write_tokens, 20);
        assert_eq!(usage.cache_read_tokens, 30);
        assert_eq!(usage.model_id, "test-model");
        assert_eq!(usage.total_tokens(), 150);
    }



    #[test]
    fn test_set_duration() {
        let mut usage = ProviderUsage::new(100, 50, 0, 0, "test-model".to_string());
        usage.set_duration(1500);
        assert_eq!(usage.duration_ms, Some(1500));
    }

    #[test]
    fn test_validate_positive_tokens() {
        let usage = ProviderUsage::new(100, 50, 0, 0, "test-model".to_string());
        assert!(usage.validate().is_ok());
    }

    #[test]
    fn test_validate_negative_tokens() {
        let usage = ProviderUsage::new(-1, 50, 0, 0, "test-model".to_string());
        assert!(usage.validate().is_err());
        assert!(usage.validate().unwrap_err().contains("prompt_tokens"));
    }

    #[test]
    fn test_validate_negative_duration() {
        let mut usage = ProviderUsage::new(100, 50, 0, 0, "test-model".to_string());
        usage.duration_ms = Some(-100);
        assert!(usage.validate().is_err());
        assert!(usage.validate().unwrap_err().contains("duration_ms"));
    }

    #[test]
    fn test_validate_cache_tokens_exceed_total() {
        let usage = ProviderUsage::new(100, 50, 60, 60, "test-model".to_string());
        assert!(usage.validate().is_err());
        let error = usage.validate().unwrap_err();
        assert!(error.contains("exceed total prompt tokens"));
        assert!(error.contains("120")); // 60 + 60
    }

    #[test]
    fn test_validate_negative_cost() {
        let usage = ProviderUsage::with_cost(100, 50, 0, 0, "test-model".to_string(), BigDecimal::from_str("-0.01").unwrap());
        assert!(usage.validate().is_err());
        assert!(usage.validate().unwrap_err().contains("cost cannot be negative"));
    }


    #[test]
    fn test_set_cost() {
        let mut usage = ProviderUsage::new(100, 50, 0, 0, "test-model".to_string());
        assert_eq!(usage.cost, None);
        
        let cost = BigDecimal::from_str("0.0025").unwrap();
        usage.set_cost(cost.clone());
        assert_eq!(usage.cost, Some(cost));
    }

    
    
    #[test]
    fn test_with_total_input() {
        let usage = ProviderUsage::new(
            200,  // total_input_tokens (e.g., 100 uncached + 25 write + 75 read)
            50,   // completion_tokens
            25,   // cache_write_tokens
            75,   // cache_read_tokens
            "test-model".to_string()
        );
        assert_eq!(usage.prompt_tokens, 200);
        assert_eq!(usage.completion_tokens, 50);
        assert_eq!(usage.cache_write_tokens, 25);
        assert_eq!(usage.cache_read_tokens, 75);
        assert_eq!(usage.total_input_tokens(), 200);
        assert_eq!(usage.total_tokens(), 250);
    }
    
    #[test]
    fn test_with_cost() {
        let cost = BigDecimal::from_str("0.0045").unwrap();
        let usage = ProviderUsage::with_cost(
            300,  // total_input_tokens
            100,  // completion_tokens
            50,   // cache_write_tokens
            150,  // cache_read_tokens
            "gpt-4".to_string(),
            cost.clone()
        );
        assert_eq!(usage.prompt_tokens, 300);
        assert_eq!(usage.completion_tokens, 100);
        assert_eq!(usage.cache_write_tokens, 50);
        assert_eq!(usage.cache_read_tokens, 150);
        assert_eq!(usage.cost, Some(cost));
        assert_eq!(usage.total_input_tokens(), 300);
        assert_eq!(usage.total_tokens(), 400);
    }

    // Mock implementation for testing the trait
    struct MockUsageExtractor;

    impl UsageExtractor for MockUsageExtractor {
        async fn extract_from_response_body(&self, body: &[u8], model_id: &str) -> Result<ProviderUsage, crate::error::AppError> {
            let body_str = std::str::from_utf8(body)
                .map_err(|e| crate::error::AppError::InvalidArgument(format!("Invalid UTF-8: {}", e)))?;
            
            let json: serde_json::Value = serde_json::from_str(body_str)
                .map_err(|e| crate::error::AppError::External(format!("Failed to parse JSON: {}", e)))?;
            
            let usage = json.get("usage")
                .ok_or_else(|| crate::error::AppError::External("Missing usage field".to_string()))?;
            let prompt_tokens = usage.get("prompt_tokens")
                .and_then(|v| v.as_i64())
                .ok_or_else(|| crate::error::AppError::External("Missing prompt_tokens".to_string()))? as i32;
            let completion_tokens = usage.get("completion_tokens")
                .and_then(|v| v.as_i64())
                .ok_or_else(|| crate::error::AppError::External("Missing completion_tokens".to_string()))? as i32;
            
            let usage = ProviderUsage::new(
                prompt_tokens,
                completion_tokens,
                0,
                0,
                model_id.to_string()
            );
            
            // Validate the extracted usage
            usage.validate().map_err(|e| crate::error::AppError::InvalidArgument(format!("Validation failed: {}", e)))?;
            
            Ok(usage)
        }

        fn extract_usage(&self, raw_json: &serde_json::Value) -> Option<ProviderUsage> {
            let usage = raw_json.get("usage")?;
            let prompt_tokens = usage.get("prompt_tokens")?.as_i64()? as i32;
            let completion_tokens = usage.get("completion_tokens")?.as_i64()? as i32;
            let usage = ProviderUsage::new(prompt_tokens, completion_tokens, 0, 0, "test-model".to_string());
            
            // Validate the extracted usage
            usage.validate().ok()?;
            
            Some(usage)
        }

    }

    #[test]
    fn test_extract_usage_trait() {
        let extractor = MockUsageExtractor;
        let response = json!({
            "usage": {
                "prompt_tokens": 150,
                "completion_tokens": 75
            }
        });
        
        let usage = extractor.extract_usage(&response).unwrap();
        assert_eq!(usage.prompt_tokens, 150);
        assert_eq!(usage.completion_tokens, 75);
    }


    #[test] 
    fn test_extract_usage_missing_fields() {
        let extractor = MockUsageExtractor;
        let response = json!({
            "other_field": "value"
        });
        
        let usage = extractor.extract_usage(&response);
        assert!(usage.is_none());
    }
    
    #[tokio::test]
    async fn test_extract_from_response_body() {
        let extractor = MockUsageExtractor;
        let response_body = r#"{
            "usage": {
                "prompt_tokens": 150,
                "completion_tokens": 75
            }
        }"#;
        
        let result = extractor.extract_from_response_body(
            response_body.as_bytes(), 
            "test-model"
        ).await;
        
        assert!(result.is_ok());
        let usage = result.unwrap();
        assert_eq!(usage.prompt_tokens, 150);
        assert_eq!(usage.completion_tokens, 75);
        assert_eq!(usage.model_id, "test-model");
    }
    
    #[tokio::test]
    async fn test_extract_from_response_body_invalid_json() {
        let extractor = MockUsageExtractor;
        let invalid_body = "invalid json";
        
        let result = extractor.extract_from_response_body(
            invalid_body.as_bytes(),
            "test-model"
        ).await;
        
        assert!(result.is_err());
        let error_msg = result.unwrap_err().to_string();
        assert!(error_msg.contains("Failed to extract usage from provider response"));
    }
    
    #[tokio::test] 
    async fn test_extract_from_response_body_missing_usage() {
        let extractor = MockUsageExtractor;
        let response_body = r#"{
            "other_field": "value"
        }"#;
        
        let result = extractor.extract_from_response_body(
            response_body.as_bytes(),
            "test-model"
        ).await;
        
        assert!(result.is_err());
        let error_msg = result.unwrap_err().to_string();
        assert!(error_msg.contains("Missing usage field"));
    }
}