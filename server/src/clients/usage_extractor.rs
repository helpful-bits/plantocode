/// Usage extraction and standardization for multi-provider LLM API clients
/// 
/// This module provides a unified interface for extracting usage information from different
/// LLM provider responses. It standardizes token counts and timing information
/// across all supported providers, enabling consistent usage tracking.
/// 
/// The system supports:
/// - Token-based usage tracking (prompt, completion, cache tokens)
/// - Duration tracking for performance analysis
/// 
/// # Usage
/// 
/// Each provider client should implement the `UsageExtractor` trait to extract
/// usage information from their specific response formats:
/// 
/// ```rust
/// use crate::clients::usage_extractor::{UsageExtractor, ProviderUsage};
/// 
/// impl UsageExtractor for MyProviderClient {
///     fn extract_usage(&self, raw_json: &serde_json::Value) -> Option<ProviderUsage> {
///         // Extract usage from provider-specific JSON response
///         let usage = raw_json.get("usage")?;
///         Some(ProviderUsage {
///             prompt_tokens: usage.get("input_tokens")?.as_i64()? as i32,
///             completion_tokens: usage.get("output_tokens")?.as_i64()? as i32,
///             cache_write_tokens: 0,
///             cache_read_tokens: usage.get("cached_tokens")?.as_i64().unwrap_or(0) as i32,
///             duration_ms: None,
///         })
///     }
/// }
/// ```

use serde::{Deserialize, Serialize};
use tracing::debug;
use bigdecimal::{BigDecimal, Signed};

/// Standardized usage information extracted from provider responses
/// 
/// This struct provides a unified representation of usage metrics across all
/// LLM providers, enabling consistent usage tracking.
/// 
/// # TOKEN SEMANTIC CONTRACT
/// 
/// ## Core Fields
/// 
/// - `prompt_tokens`: **TOTAL INPUT TOKENS** processed by the model
///   - This is the sum of ALL input tokens: uncached + cache_write + cache_read
///   - Represents the complete input token count, regardless of cache status
///   - For providers that report uncached tokens separately, this field contains the total
///   - **NEVER** contains just the uncached portion - it's always the complete input count
/// 
/// - `completion_tokens`: Output tokens generated by the model (the response)
///   - These are the tokens in the assistant's response
///   - Does not include any cached content
/// 
/// ## Cache Token Fields (For Billing Breakdown Only)
/// 
/// - `cache_write_tokens`: Tokens written to provider cache
///   - Used for detailed billing calculations
///   - Part of the total represented in `prompt_tokens`
///   - May have different pricing than regular input tokens
/// 
/// - `cache_read_tokens`: Tokens read from provider cache  
///   - Used for detailed billing calculations
///   - Part of the total represented in `prompt_tokens`
///   - Often has significantly lower pricing than regular input tokens
/// 
/// ## Important Implementation Notes
/// 
/// 1. **Total Input Calculation**: When implementing UsageExtractor for a provider:
///    - If the provider reports total input directly, use that for `prompt_tokens`
///    - If the provider reports components separately, calculate: 
///      `prompt_tokens = uncached_input + cache_write + cache_read`
/// 
/// 2. **Cache Token Semantics**: The cache token fields (`cache_write_tokens`, `cache_read_tokens`)
///    are ONLY for billing breakdown purposes. They help calculate costs with different rates
///    but do NOT affect the total token count which is always in `prompt_tokens`.
/// 
/// 3. **Billing Calculation**: The cost calculation system uses:
///    - `prompt_tokens` to verify total input
///    - Cache token fields to apply different pricing rates
///    - Formula: `uncached = prompt_tokens - cache_write_tokens - cache_read_tokens`
/// 
/// ## Provider Mappings
/// 
/// ### Anthropic
/// - `input_tokens` (total) → `prompt_tokens`
/// - `output_tokens` → `completion_tokens`  
/// - `cache_creation_input_tokens` → `cache_write_tokens`
/// - `cache_read_input_tokens` → `cache_read_tokens`
/// 
/// ### OpenAI
/// - `prompt_tokens` (total) → `prompt_tokens`
/// - `completion_tokens` → `completion_tokens`
/// - `prompt_tokens_details.cached_tokens` → `cache_read_tokens`
/// - Cache writes not explicitly reported
/// 
/// ### Google
/// - `promptTokenCount` (total) → `prompt_tokens`
/// - `candidatesTokenCount` → `completion_tokens`
/// - `cachedContentTokenCount` → `cache_read_tokens`
/// - Cache writes not explicitly reported
/// 
/// ### OpenRouter
/// - `prompt_tokens` (total) → `prompt_tokens`
/// - `completion_tokens` → `completion_tokens`
/// - Cache tokens may be in usage details
/// 
/// ## Database Mappings
/// 
/// - `prompt_tokens` → `tokens_sent` (BackgroundJob), `tokens_input` (ApiUsageEntry)
/// - `completion_tokens` → `tokens_received` (BackgroundJob), `tokens_output` (ApiUsageEntry)
/// - Cache tokens stored separately for billing details
/// 
/// ## Example
/// 
/// ```rust
/// // Provider reports: uncached=500, cache_write=200, cache_read=300
/// let usage = ProviderUsage {
///     prompt_tokens: 1000,      // Total: 500 + 200 + 300
///     completion_tokens: 150,   // Output tokens
///     cache_write_tokens: 200,  // For billing breakdown
///     cache_read_tokens: 300,   // For billing breakdown
///     model_id: "claude-3".to_string(),
///     duration_ms: Some(1500),
///     cost: None,              // Server calculates its own costs
/// };
/// ```
#[derive(Debug, Clone, Serialize, Deserialize, PartialEq)]
pub struct ProviderUsage {
    /// Total number of input tokens processed by the model
    /// 
    /// **CRITICAL**: This field represents the TOTAL INPUT TOKENS (uncached + cache_write + cache_read)
    /// It is NOT just the uncached tokens - it's the complete input token count.
    /// For providers that support caching, this is the sum of all input token types.
    /// 
    /// This design ensures consistent usage tracking across all providers, whether they
    /// support caching or not. Billing systems can rely on this field for the total
    /// input token count.
    pub prompt_tokens: i32,
    
    /// Number of output/completion tokens generated by the model
    /// 
    /// These are the tokens in the assistant's response. This count does not include
    /// any cached content - it's purely the newly generated tokens.
    pub completion_tokens: i32,
    
    /// Number of tokens written to the provider's cache system
    /// 
    /// This field is for billing breakdown purposes only. These tokens are already
    /// included in the `prompt_tokens` total. Providers may charge different rates
    /// for cache writes compared to regular input tokens.
    /// 
    /// Not all providers support or report cache writes explicitly.
    pub cache_write_tokens: i32,
    
    /// Number of tokens read from the provider's cache system
    /// 
    /// This field is for billing breakdown purposes only. These tokens are already
    /// included in the `prompt_tokens` total. Cache reads typically have significantly
    /// lower pricing than regular input tokens.
    /// 
    /// Not all providers support or report cache reads explicitly.
    pub cache_read_tokens: i32,
    
    /// The model ID that was used for this request
    /// 
    /// This should match the model ID used in the request, which helps with
    /// cost calculation and usage attribution.
    pub model_id: String,
    
    /// Duration of the request in milliseconds
    /// 
    /// Used for performance monitoring and analysis. This measures the total
    /// time from request initiation to response completion.
    pub duration_ms: Option<i64>,
    
    /// Cost of the request as calculated by the provider
    /// 
    /// This preserves provider-calculated costs when available for auditing purposes.
    /// However, the server ALWAYS calculates its own authoritative costs for billing.
    /// Provider costs are logged but never used for actual billing.
    /// 
    /// Using BigDecimal for precise financial calculations without floating-point errors.
    pub cost: Option<BigDecimal>,
}

impl ProviderUsage {
    /// Create a new ProviderUsage with basic token counts
    /// 
    /// This is a convenience constructor for the most common use case where
    /// only total input and completion tokens are available.
    /// 
    /// # Arguments
    /// 
    /// * `prompt_tokens` - TOTAL number of input tokens (includes all input: uncached + cached)
    /// * `completion_tokens` - Number of output tokens
    /// * `model_id` - The model ID used for this request
    /// 
    /// # Example
    /// 
    /// ```rust
    /// let usage = ProviderUsage::new(150, 75, "model-id".to_string());
    /// assert_eq!(usage.prompt_tokens, 150); // Total input tokens
    /// assert_eq!(usage.completion_tokens, 75);
    /// assert_eq!(usage.total_tokens(), 225);
    /// ```
    pub fn new(prompt_tokens: i32, completion_tokens: i32, model_id: String) -> Self {
        Self {
            prompt_tokens,
            completion_tokens,
            cache_write_tokens: 0,
            cache_read_tokens: 0,
            model_id,
            duration_ms: None,
            cost: None,
        }
    }
    
    /// Derive cache write tokens from uncached tokens and cache read tokens
    /// 
    /// This helper is for providers that don't explicitly report cache write tokens.
    /// It calculates cache writes by assuming uncached tokens are written to cache.
    /// 
    /// # Arguments
    /// 
    /// * `uncached_tokens` - Tokens that were NOT read from cache (not total input!)
    /// * `cache_read_tokens` - Tokens read from cache
    /// 
    /// # Returns
    /// 
    /// The estimated number of tokens written to cache
    /// 
    /// # Important
    /// 
    /// This method expects UNCACHED tokens, not total input tokens. If you have
    /// total input tokens, first subtract cache_read_tokens to get uncached tokens.
    /// 
    /// # Example
    /// 
    /// ```rust
    /// // Provider reports: total_input=200, cache_read=75
    /// let uncached = 200 - 75; // 125
    /// let cache_write = ProviderUsage::derive_cache_write(uncached, 75); // Returns 125
    /// ```
    pub fn derive_cache_write(uncached_tokens: i32, cache_read_tokens: i32) -> i32 {
        // In most cases, uncached tokens are written to cache for future use
        // This is a heuristic - some providers may have different caching strategies
        uncached_tokens.max(0)
    }
    
    /// Create a ProviderUsage from legacy token counts without cache information
    /// 
    /// This constructor is provided for compatibility with older systems
    /// that don't support cache token tracking.
    /// 
    /// # Arguments
    /// 
    /// * `prompt_tokens` - TOTAL number of input tokens (all input tokens processed)
    /// * `completion_tokens` - Number of output tokens
    /// * `model_id` - The model ID used for this request
    pub fn from_legacy(prompt_tokens: i32, completion_tokens: i32, model_id: String) -> Self {
        Self {
            prompt_tokens,
            completion_tokens,
            cache_write_tokens: 0,
            cache_read_tokens: 0,
            model_id,
            duration_ms: None,
            cost: None,
        }
    }
    
    /// Create a new ProviderUsage with cache token information
    /// 
    /// DEPRECATED: Use `with_total_input()` instead for clearer semantics.
    /// 
    /// This constructor supports providers that offer caching capabilities
    /// with separate billing for cache reads and writes.
    #[deprecated(since = "0.1.0", note = "Use `with_total_input()` instead for clearer semantics")]
    /// 
    /// # Arguments
    /// 
    /// * `prompt_tokens` - TOTAL number of input tokens (should be uncached + cache_write + cache_read)
    /// * `completion_tokens` - Number of output tokens
    /// * `cache_write_tokens` - Number of tokens written to cache
    /// * `cache_read_tokens` - Number of tokens read from cache
    /// * `model_id` - The model ID used for this request
    /// 
    /// # Example
    /// 
    /// ```rust
    /// let usage = ProviderUsage::with_cache(200, 50, 25, 75, "claude-3-sonnet".to_string());
    /// assert_eq!(usage.prompt_tokens, 200); // Total input tokens
    /// assert_eq!(usage.total_input_tokens(), 200); // Same as prompt_tokens when properly set
    /// ```
    pub fn with_cache(
        prompt_tokens: i32, 
        completion_tokens: i32, 
        cache_write_tokens: i32, 
        cache_read_tokens: i32,
        model_id: String
    ) -> Self {
        Self {
            prompt_tokens,
            completion_tokens,
            cache_write_tokens,
            cache_read_tokens,
            model_id,
            duration_ms: None,
            cost: None,
        }
    }
    
    /// Get the total number of input tokens
    /// 
    /// Since `prompt_tokens` already represents the TOTAL input tokens,
    /// this method simply returns that value. The separate cache token
    /// fields are for detailed billing breakdowns only.
    /// 
    /// # Returns
    /// 
    /// The total number of input tokens (same as prompt_tokens)
    /// 
    /// # Note for Implementers
    /// 
    /// When implementing UsageExtractor, ensure that prompt_tokens contains
    /// the sum of ALL input tokens. If your provider reports components separately,
    /// calculate: prompt_tokens = uncached + cache_write + cache_read
    pub fn total_input_tokens(&self) -> i32 {
        // prompt_tokens already contains the total input tokens
        // It should equal uncached + cache_write + cache_read
        self.prompt_tokens
    }
    
    /// Get the total number of tokens (input + output)
    /// 
    /// This is the sum of all tokens involved in the request, which is useful
    /// for providers that bill based on total token usage.
    pub fn total_tokens(&self) -> i32 {
        self.prompt_tokens + self.completion_tokens
    }
    
    /// Update the duration information
    /// 
    /// This method allows adding timing information after the request completes,
    /// which is useful for performance monitoring.
    /// 
    /// # Arguments
    /// 
    /// * `duration_ms` - Request duration in milliseconds
    pub fn set_duration(&mut self, duration_ms: i64) {
        self.duration_ms = Some(duration_ms);
    }
    
    /// Set the cost information
    /// 
    /// This method allows adding provider-calculated cost information
    /// which is useful for accurate billing and cost tracking.
    /// 
    /// # Arguments
    /// 
    /// * `cost` - Request cost as calculated by the provider
    pub fn set_cost(&mut self, cost: BigDecimal) {
        self.cost = Some(cost);
    }
    
    /// Create a new ProviderUsage with cost information
    /// 
    /// DEPRECATED: Use `with_total_input_and_cost()` instead for clearer semantics.
    /// 
    /// This constructor supports providers that return cost information
    /// in their responses for accurate billing tracking.
    #[deprecated(since = "0.1.0", note = "Use `with_total_input_and_cost()` instead for clearer semantics")]
    /// 
    /// # Arguments
    /// 
    /// * `prompt_tokens` - TOTAL number of input tokens (should be uncached + cache_write + cache_read)
    /// * `completion_tokens` - Number of output tokens
    /// * `cache_write_tokens` - Number of tokens written to cache
    /// * `cache_read_tokens` - Number of tokens read from cache
    /// * `model_id` - The model ID used for this request
    /// * `cost` - Provider-calculated cost
    pub fn with_cost(
        prompt_tokens: i32,
        completion_tokens: i32,
        cache_write_tokens: i32,
        cache_read_tokens: i32,
        model_id: String,
        cost: Option<BigDecimal>
    ) -> Self {
        Self {
            prompt_tokens,
            completion_tokens,
            cache_write_tokens,
            cache_read_tokens,
            model_id,
            duration_ms: None,
            cost,
        }
    }
    
    /// Create a new ProviderUsage with explicit total input tokens
    /// 
    /// This constructor makes it clear that prompt_tokens represents the TOTAL
    /// input tokens, not just uncached tokens. Use this for all new implementations.
    /// 
    /// # Arguments
    /// 
    /// * `total_input_tokens` - TOTAL number of input tokens (uncached + cache_write + cache_read)
    /// * `completion_tokens` - Number of output tokens
    /// * `cache_write_tokens` - Number of tokens written to cache (for billing breakdown)
    /// * `cache_read_tokens` - Number of tokens read from cache (for billing breakdown)
    /// * `model_id` - The model ID used for this request
    /// 
    /// # Example
    /// 
    /// ```rust
    /// // If provider reports: uncached=100, cache_write=25, cache_read=75
    /// let usage = ProviderUsage::with_total_input(
    ///     200,  // total_input_tokens (100 + 25 + 75)
    ///     50,   // completion_tokens
    ///     25,   // cache_write_tokens
    ///     75,   // cache_read_tokens
    ///     "claude-3-sonnet".to_string()
    /// );
    /// assert_eq!(usage.prompt_tokens, 200);
    /// assert_eq!(usage.total_input_tokens(), 200);
    /// ```
    pub fn with_total_input(
        total_input_tokens: i32,
        completion_tokens: i32,
        cache_write_tokens: i32,
        cache_read_tokens: i32,
        model_id: String,
    ) -> Self {
        Self {
            prompt_tokens: total_input_tokens,
            completion_tokens,
            cache_write_tokens,
            cache_read_tokens,
            model_id,
            duration_ms: None,
            cost: None,
        }
    }
    
    /// Create a new ProviderUsage with total input tokens and cost information
    /// 
    /// This constructor combines explicit total input semantics with cost tracking
    /// for providers that return billing information.
    /// 
    /// # Arguments
    /// 
    /// * `total_input_tokens` - TOTAL number of input tokens (uncached + cache_write + cache_read)
    /// * `completion_tokens` - Number of output tokens
    /// * `cache_write_tokens` - Number of tokens written to cache (for billing breakdown)
    /// * `cache_read_tokens` - Number of tokens read from cache (for billing breakdown)
    /// * `model_id` - The model ID used for this request
    /// * `cost` - Provider-calculated cost
    /// 
    /// # Example
    /// 
    /// ```rust
    /// use bigdecimal::BigDecimal;
    /// use std::str::FromStr;
    /// 
    /// let cost = BigDecimal::from_str("0.0025").unwrap();
    /// let usage = ProviderUsage::with_total_input_and_cost(
    ///     200,  // total_input_tokens
    ///     50,   // completion_tokens
    ///     25,   // cache_write_tokens
    ///     75,   // cache_read_tokens
    ///     "claude-3-sonnet".to_string(),
    ///     Some(cost)
    /// );
    /// ```
    pub fn with_total_input_and_cost(
        total_input_tokens: i32,
        completion_tokens: i32,
        cache_write_tokens: i32,
        cache_read_tokens: i32,
        model_id: String,
        cost: Option<BigDecimal>,
    ) -> Self {
        Self {
            prompt_tokens: total_input_tokens,
            completion_tokens,
            cache_write_tokens,
            cache_read_tokens,
            model_id,
            duration_ms: None,
            cost,
        }
    }
    
    /// Validate that token counts are non-negative and semantically correct
    /// 
    /// This method performs validation to ensure usage data integrity:
    /// - All token counts must be non-negative
    /// - If cache tokens are reported, prompt_tokens should equal the sum of all input components
    /// - Cost values must be non-negative if present
    /// - Duration must be non-negative if present
    /// 
    /// # Returns
    /// 
    /// * `Ok(())` - All token counts are valid
    /// * `Err(String)` - Description of the validation error
    /// 
    /// # Example
    /// 
    /// ```rust
    /// let usage = ProviderUsage::new(100, 50, "test-model".to_string());
    /// assert!(usage.validate().is_ok());
    /// 
    /// let invalid_usage = ProviderUsage {
    ///     prompt_tokens: -10,
    ///     completion_tokens: 50,
    ///     cache_write_tokens: 0,
    ///     cache_read_tokens: 0,
    ///     model_id: "test".to_string(),
    ///     duration_ms: None,
    ///     cost: None,
    /// };
    /// assert!(invalid_usage.validate().is_err());
    /// ```
    pub fn validate(&self) -> Result<(), String> {
        if self.prompt_tokens < 0 {
            return Err(format!("Invalid prompt_tokens: {}", self.prompt_tokens));
        }
        if self.completion_tokens < 0 {
            return Err(format!("Invalid completion_tokens: {}", self.completion_tokens));
        }
        if self.cache_write_tokens < 0 {
            return Err(format!("Invalid cache_write_tokens: {}", self.cache_write_tokens));
        }
        if self.cache_read_tokens < 0 {
            return Err(format!("Invalid cache_read_tokens: {}", self.cache_read_tokens));
        }
        if let Some(duration) = self.duration_ms {
            if duration < 0 {
                return Err(format!("Invalid duration_ms: {}", duration));
            }
        }
        if let Some(cost) = &self.cost {
            if cost.is_negative() {
                return Err(format!("Invalid cost: cost cannot be negative"));
            }
        }
        Ok(())
    }
}

impl Default for ProviderUsage {
    fn default() -> Self {
        Self::new(0, 0, "unknown".to_string())
    }
}

/// Trait for extracting standardized usage information from provider responses
/// 
/// This trait must be implemented by each LLM provider client to enable
/// consistent usage tracking across the system. The implementation should
/// parse the provider's response format and extract token counts and timing 
/// information into the standardized `ProviderUsage` struct.
/// 
/// # Implementation Guidelines
/// 
/// 1. **Token Mapping**: Map provider-specific token fields to standard fields:
///    - TOTAL input tokens → `prompt_tokens` (must be sum of all input tokens)
///    - Output/completion tokens → `completion_tokens`
///    - Cached tokens → `cache_read_tokens` (for billing breakdown)
///    - Cache writes → `cache_write_tokens` (for billing breakdown)
/// 
/// 2. **Total Input Calculation**: Ensure `prompt_tokens` contains the TOTAL input tokens:
///    - If provider reports total directly, use that value
///    - If provider reports components separately, sum them: uncached + cache_write + cache_read
/// 
/// 3. **Error Handling**: Return None for unparseable responses rather than panicking
/// 
/// 4. **Logging**: Use debug logging to aid in troubleshooting usage extraction
/// 
/// # Example Implementation
/// 
/// ```rust
/// impl UsageExtractor for AnthropicClient {
///     fn extract_usage(&self, raw_json: &serde_json::Value) -> Option<ProviderUsage> {
///         let usage = raw_json.get("usage")?;
///         
///         // Anthropic reports total input_tokens
///         let total_input = usage.get("input_tokens")?.as_i64()? as i32;
///         let completion_tokens = usage.get("output_tokens")?.as_i64()? as i32;
///         let cache_read = usage.get("cache_read_input_tokens")?.as_i64().unwrap_or(0) as i32;
///         let cache_write = usage.get("cache_creation_input_tokens")?.as_i64().unwrap_or(0) as i32;
///         
///         Some(ProviderUsage::with_total_input(
///             total_input,      // TOTAL input tokens
///             completion_tokens,
///             cache_write,
///             cache_read,
///             "claude-3-sonnet".to_string()
///         ))
///     }
/// }
/// ```
pub trait UsageExtractor {
    /// Extract usage information from provider's HTTP response body (2025-07 format)
    /// 
    /// This is the primary method for extracting usage from provider responses.
    /// It handles the complete HTTP response body and supports both streaming 
    /// and non-streaming responses with comprehensive error handling.
    /// 
    /// # Arguments
    /// 
    /// * `body` - The complete HTTP response body as bytes
    /// * `model_id` - The model identifier for this request
    /// * `is_streaming` - Whether this is a streaming response
    /// 
    /// # Returns
    /// 
    /// * `Ok(ProviderUsage)` - Successfully extracted usage information
    /// * `Err(AppError)` - Unable to extract usage (parse errors, missing fields, etc.)
    /// 
    /// # Implementation Requirements
    /// 
    /// - Parse provider-specific payload formats (JSON, SSE, etc.)
    /// - Handle malformed responses gracefully (return error, don't panic)
    /// - Support both streaming final chunks and complete responses
    /// - Map provider token fields to standard ProviderUsage structure
    /// - Include model_id in the returned ProviderUsage
    async fn extract_from_http_body(&self, body: &[u8], model_id: &str, is_streaming: bool) -> Result<ProviderUsage, crate::error::AppError> {
        let body_str = std::str::from_utf8(body)
            .map_err(|e| crate::error::AppError::InvalidArgument(format!("Invalid UTF-8 in response body: {}", e)))?;
        
        self.extract_usage_from_string(body_str)
            .map(|mut usage| {
                usage.model_id = model_id.to_string();
                usage
            })
            .ok_or_else(|| crate::error::AppError::External("Failed to extract usage from provider response".to_string()))
    }

    /// Extract usage information from a provider's raw JSON response
    /// 
    /// This method should parse the provider-specific response format and
    /// return standardized usage information. The implementation should be
    /// robust to variations in response format and return None rather than
    /// panicking on parse errors.
    /// 
    /// # Arguments
    /// 
    /// * `raw_json` - The complete JSON response from the provider
    /// 
    /// # Returns
    /// 
    /// * `Some(ProviderUsage)` - Successfully extracted usage information
    /// * `None` - Unable to extract usage (missing fields, parse errors, etc.)
    /// 
    /// # Implementation Notes
    /// 
    /// - Use debug logging to trace extraction process
    /// - Handle missing optional fields gracefully
    /// - Validate extracted data using `ProviderUsage::validate()`
    /// - Consider provider-specific edge cases (e.g., streaming vs non-streaming)
    fn extract_usage(&self, raw_json: &serde_json::Value) -> Option<ProviderUsage> {
        debug!("Extracting usage from provider response");
        
        // Default implementation returns None - providers must override
        // This ensures compilation but prevents silent failures
        debug!("No usage extraction implementation available for this provider");
        None
    }
    
    /// Extract usage information from a streaming response chunk
    /// 
    /// Some providers send usage information in streaming chunks rather than
    /// a final response. This method handles extraction from individual chunks.
    /// 
    /// The default implementation tries to extract from the chunk as if it
    /// were a complete response, but providers can override for chunk-specific logic.
    /// 
    /// # Arguments
    /// 
    /// * `chunk_json` - A single streaming chunk as JSON
    /// 
    /// # Returns
    /// 
    /// * `Some(ProviderUsage)` - Usage information found in this chunk
    /// * `None` - No usage information in this chunk (common for content chunks)
    fn extract_usage_from_stream_chunk(&self, chunk_json: &serde_json::Value) -> Option<ProviderUsage> {
        debug!("Extracting usage from stream chunk");
        
        // Default implementation: try normal extraction
        // Most providers send usage in final chunks with same format
        self.extract_usage(chunk_json)
    }
    
    /// Extract usage information from a raw string response
    /// 
    /// This is a convenience method that parses JSON and delegates to extract_usage.
    /// It handles JSON parsing errors gracefully and provides debug logging.
    /// 
    /// # Arguments
    /// 
    /// * `raw_response` - The raw string response from the provider
    /// 
    /// # Returns
    /// 
    /// * `Some(ProviderUsage)` - Successfully parsed and extracted usage
    /// * `None` - JSON parsing failed or usage extraction failed
    fn extract_usage_from_string(&self, raw_response: &str) -> Option<ProviderUsage> {
        debug!("Parsing JSON response for usage extraction");
        
        match serde_json::from_str::<serde_json::Value>(raw_response) {
            Ok(json) => {
                debug!("Successfully parsed JSON, extracting usage");
                self.extract_usage(&json)
            }
            Err(e) => {
                debug!("Failed to parse JSON response: {}", e);
                None
            }
        }
    }
}

#[cfg(test)]
mod tests {
    use super::*;
    use serde_json::json;
    use std::str::FromStr;

    #[test]
    fn test_provider_usage_new() {
        let usage = ProviderUsage::new(100, 50, "test-model".to_string());
        assert_eq!(usage.prompt_tokens, 100);
        assert_eq!(usage.completion_tokens, 50);
        assert_eq!(usage.cache_write_tokens, 0);
        assert_eq!(usage.cache_read_tokens, 0);
        assert_eq!(usage.model_id, "test-model");
        assert_eq!(usage.total_tokens(), 150);
    }

    #[test]
    fn test_provider_usage_with_cache() {
        // with_cache is deprecated but still works - prompt_tokens should be total
        let usage = ProviderUsage::with_cache(200, 50, 25, 75, "test-model".to_string());
        assert_eq!(usage.prompt_tokens, 200); // Total input tokens
        assert_eq!(usage.completion_tokens, 50);
        assert_eq!(usage.cache_write_tokens, 25);
        assert_eq!(usage.cache_read_tokens, 75);
        assert_eq!(usage.model_id, "test-model");
        assert_eq!(usage.total_input_tokens(), 200); // Same as prompt_tokens
        assert_eq!(usage.total_tokens(), 250);
    }


    #[test]
    fn test_set_duration() {
        let mut usage = ProviderUsage::new(100, 50, "test-model".to_string());
        usage.set_duration(1500);
        assert_eq!(usage.duration_ms, Some(1500));
    }

    #[test]
    fn test_validate_positive_tokens() {
        let usage = ProviderUsage::new(100, 50, "test-model".to_string());
        assert!(usage.validate().is_ok());
    }

    #[test]
    fn test_validate_negative_tokens() {
        let usage = ProviderUsage {
            prompt_tokens: -1,
            completion_tokens: 50,
            cache_write_tokens: 0,
            cache_read_tokens: 0,
            model_id: "test-model".to_string(),
            duration_ms: None,
            cost: None,
        };
        assert!(usage.validate().is_err());
        assert!(usage.validate().unwrap_err().contains("prompt_tokens"));
    }

    #[test]
    fn test_validate_negative_duration() {
        let usage = ProviderUsage {
            prompt_tokens: 100,
            completion_tokens: 50,
            cache_write_tokens: 0,
            cache_read_tokens: 0,
            model_id: "test-model".to_string(),
            duration_ms: Some(-100),
            cost: None,
        };
        assert!(usage.validate().is_err());
        assert!(usage.validate().unwrap_err().contains("duration_ms"));
    }

    #[test]
    fn test_default() {
        let usage = ProviderUsage::default();
        assert_eq!(usage.prompt_tokens, 0);
        assert_eq!(usage.completion_tokens, 0);
        assert_eq!(usage.total_tokens(), 0);
        assert_eq!(usage.cost, None);
    }

    #[test]
    fn test_set_cost() {
        let mut usage = ProviderUsage::new(100, 50, "test-model".to_string());
        assert_eq!(usage.cost, None);
        
        let cost = BigDecimal::from_str("0.0025").unwrap();
        usage.set_cost(cost.clone());
        assert_eq!(usage.cost, Some(cost));
    }

    #[test]
    fn test_with_cost() {
        let cost = BigDecimal::from_str("0.0035").unwrap();
        // with_cost is deprecated but still works - prompt_tokens should be total
        let usage = ProviderUsage::with_cost(200, 50, 25, 75, "test-model".to_string(), Some(cost.clone()));
        assert_eq!(usage.prompt_tokens, 200); // Total input tokens
        assert_eq!(usage.completion_tokens, 50);
        assert_eq!(usage.cache_write_tokens, 25);
        assert_eq!(usage.cache_read_tokens, 75);
        assert_eq!(usage.cost, Some(cost));
        assert_eq!(usage.total_tokens(), 250);
    }
    
    #[test]
    fn test_derive_cache_write() {
        // Normal case: uncached tokens are written to cache
        assert_eq!(ProviderUsage::derive_cache_write(70, 30), 70);
        
        // Edge case: no uncached tokens (all from cache)
        assert_eq!(ProviderUsage::derive_cache_write(0, 100), 0);
        
        // Edge case: no cache read, all tokens are uncached
        assert_eq!(ProviderUsage::derive_cache_write(100, 0), 100);
        
        // Edge case: negative uncached tokens (should not happen but handle gracefully)
        assert_eq!(ProviderUsage::derive_cache_write(-50, 100), 0);
    }
    
    #[test]
    fn test_with_total_input() {
        let usage = ProviderUsage::with_total_input(
            200,  // total_input_tokens (e.g., 100 uncached + 25 write + 75 read)
            50,   // completion_tokens
            25,   // cache_write_tokens
            75,   // cache_read_tokens
            "test-model".to_string()
        );
        assert_eq!(usage.prompt_tokens, 200);
        assert_eq!(usage.completion_tokens, 50);
        assert_eq!(usage.cache_write_tokens, 25);
        assert_eq!(usage.cache_read_tokens, 75);
        assert_eq!(usage.total_input_tokens(), 200);
        assert_eq!(usage.total_tokens(), 250);
    }
    
    #[test]
    fn test_with_total_input_and_cost() {
        let cost = BigDecimal::from_str("0.0045").unwrap();
        let usage = ProviderUsage::with_total_input_and_cost(
            300,  // total_input_tokens
            100,  // completion_tokens
            50,   // cache_write_tokens
            150,  // cache_read_tokens
            "gpt-4".to_string(),
            Some(cost.clone())
        );
        assert_eq!(usage.prompt_tokens, 300);
        assert_eq!(usage.completion_tokens, 100);
        assert_eq!(usage.cache_write_tokens, 50);
        assert_eq!(usage.cache_read_tokens, 150);
        assert_eq!(usage.cost, Some(cost));
        assert_eq!(usage.total_input_tokens(), 300);
        assert_eq!(usage.total_tokens(), 400);
    }

    // Mock implementation for testing the trait
    struct MockUsageExtractor;

    impl UsageExtractor for MockUsageExtractor {
        async fn extract_from_http_body(&self, body: &[u8], model_id: &str, is_streaming: bool) -> Result<ProviderUsage, crate::error::AppError> {
            let body_str = std::str::from_utf8(body)
                .map_err(|e| crate::error::AppError::InvalidArgument(format!("Invalid UTF-8: {}", e)))?;
            
            let json: serde_json::Value = serde_json::from_str(body_str)
                .map_err(|e| crate::error::AppError::External(format!("Failed to parse JSON: {}", e)))?;
            
            let usage = json.get("usage")
                .ok_or_else(|| crate::error::AppError::External("Missing usage field".to_string()))?;
            let prompt_tokens = usage.get("prompt_tokens")
                .and_then(|v| v.as_i64())
                .ok_or_else(|| crate::error::AppError::External("Missing prompt_tokens".to_string()))? as i32;
            let completion_tokens = usage.get("completion_tokens")
                .and_then(|v| v.as_i64())
                .ok_or_else(|| crate::error::AppError::External("Missing completion_tokens".to_string()))? as i32;
            
            Ok(ProviderUsage {
                prompt_tokens,
                completion_tokens,
                cache_write_tokens: 0,
                cache_read_tokens: 0,
                model_id: model_id.to_string(),
                duration_ms: None,
                cost: None,
            })
        }

        fn extract_usage(&self, raw_json: &serde_json::Value) -> Option<ProviderUsage> {
            let usage = raw_json.get("usage")?;
            let prompt_tokens = usage.get("prompt_tokens")?.as_i64()? as i32;
            let completion_tokens = usage.get("completion_tokens")?.as_i64()? as i32;
            Some(ProviderUsage::new(prompt_tokens, completion_tokens, "test-model".to_string()))
        }
    }

    #[test]
    fn test_extract_usage_trait() {
        let extractor = MockUsageExtractor;
        let response = json!({
            "usage": {
                "prompt_tokens": 150,
                "completion_tokens": 75
            }
        });
        
        let usage = extractor.extract_usage(&response).unwrap();
        assert_eq!(usage.prompt_tokens, 150);
        assert_eq!(usage.completion_tokens, 75);
    }

    #[test]
    fn test_extract_usage_from_string() {
        let extractor = MockUsageExtractor;
        let response_str = r#"{"usage": {"prompt_tokens": 200, "completion_tokens": 100}}"#;
        
        let usage = extractor.extract_usage_from_string(response_str).unwrap();
        assert_eq!(usage.prompt_tokens, 200);
        assert_eq!(usage.completion_tokens, 100);
    }

    #[test]
    fn test_extract_usage_from_invalid_json() {
        let extractor = MockUsageExtractor;
        let invalid_json = "invalid json";
        
        let usage = extractor.extract_usage_from_string(invalid_json);
        assert!(usage.is_none());
    }

    #[test] 
    fn test_extract_usage_missing_fields() {
        let extractor = MockUsageExtractor;
        let response = json!({
            "other_field": "value"
        });
        
        let usage = extractor.extract_usage(&response);
        assert!(usage.is_none());
    }
    
    #[tokio::test]
    async fn test_extract_from_http_body() {
        let extractor = MockUsageExtractor;
        let response_body = r#"{
            "usage": {
                "prompt_tokens": 150,
                "completion_tokens": 75
            }
        }"#;
        
        let result = extractor.extract_from_http_body(
            response_body.as_bytes(), 
            "test-model", 
            false
        ).await;
        
        assert!(result.is_ok());
        let usage = result.unwrap();
        assert_eq!(usage.prompt_tokens, 150);
        assert_eq!(usage.completion_tokens, 75);
        assert_eq!(usage.model_id, "test-model");
    }
    
    #[tokio::test]
    async fn test_extract_from_http_body_invalid_json() {
        let extractor = MockUsageExtractor;
        let invalid_body = "invalid json";
        
        let result = extractor.extract_from_http_body(
            invalid_body.as_bytes(),
            "test-model",
            false
        ).await;
        
        assert!(result.is_err());
        let error_msg = result.unwrap_err().to_string();
        assert!(error_msg.contains("Failed to extract usage from provider response"));
    }
    
    #[tokio::test] 
    async fn test_extract_from_http_body_missing_usage() {
        let extractor = MockUsageExtractor;
        let response_body = r#"{
            "other_field": "value"
        }"#;
        
        let result = extractor.extract_from_http_body(
            response_body.as_bytes(),
            "test-model",
            false
        ).await;
        
        assert!(result.is_err());
        let error_msg = result.unwrap_err().to_string();
        assert!(error_msg.contains("Missing usage field"));
    }
}